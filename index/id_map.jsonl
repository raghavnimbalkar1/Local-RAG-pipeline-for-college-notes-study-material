{"id": "CSDS/linear regression (3).pdf#p1#c1", "source": "CSDS/linear regression (3).pdf", "page": 1, "snippet": "consider the variate x “ which tates n values ~ ag then the arithmetic mean js al io, - denoted by and given. by — — — $ _ — — qe uther - - - + % ey a"}
{"id": "CSDS/linear regression (3).pdf#p2#c1", "source": "CSDS/linear regression (3).pdf", "page": 2, "snippet": "= sind mil valu es 2, dis hs bucton cme — ae - educe the calculahon we use deviahion > | tae d = 4 — a a ig assummed. mean. then we can wnte fd = fu —"}
{"id": "CSDS/linear regression (3).pdf#p3#c1", "source": "CSDS/linear regression (3).pdf", "page": 3, "snippet": "_ variate clivides it into two equal part. “ tmealian of. a distnbufiion ts the value _ of the. her discrete type ' — oo mr ’ : number of obsewationg "}
{"id": "CSDS/linear regression (3).pdf#p4#c1", "source": "CSDS/linear regression (3).pdf", "page": 4, "snippet": "“ e tek “ obteuin the median of pe dishbuben _ total freq n = a ) nw cas. e7 et 2 4 - just qreater than + 5 s — hs zt. do the corres pond value d - va"}
{"id": "CSDS/linear regression (3).pdf#p5#c1", "source": "CSDS/linear regression (3).pdf", "page": 5, "snippet": "_ _ pccurs most pepveetey na set al obsewodtons mode. oo jt ts the, velue. ot. the. “ vediede. “ hick | gt dower limit 54 modal class | = _ freg of mo"}
{"id": "CSDS/linear regression (3).pdf#p6#c1", "source": "CSDS/linear regression (3).pdf", "page": 6, "snippet": "— fjnd mean i median and ads fer the. following. freq ueney aistobudion | oo d = 80, 4213 fore fp = 10, “ tae u8 _ mode = je + sea ) — ze = § - 10"}
{"id": "CSDS/linear regression (3).pdf#p7#c1", "source": "CSDS/linear regression (3).pdf", "page": 7, "snippet": "_ nole among oll averages, arith mefic mean mected by pluchsations ts jeast a | ‘ dispersion. $ $ $ _ _ _ — _ — — — whether the average gives correct "}
{"id": "CSDS/linear regression (3).pdf#p8#c1", "source": "CSDS/linear regression (3).pdf", "page": 8, "snippet": "{ | _ — a dew even,. _. nauance. =. _ here the step t sq yuacing he. deyiedion fa — ¥ xx } _ evercomes the drawbeuk? — of mean deviation. oe — amena, "}
{"id": "CSDS/linear regression (3).pdf#p9#c1", "source": "CSDS/linear regression (3).pdf", "page": 9, "snippet": "| dewiatien method saree co let dean a : tt aac me ) ( ox here a ya ett sf a ~ where u = ah fe yap eb ) ( fat ) ~ ( a ) ~ — here fe a4 _ h = no. a abs"}
{"id": "CSDS/linear regression (3).pdf#p10#c1", "source": "CSDS/linear regression (3).pdf", "page": 10, "snippet": "a ey goals scered by two. team a and b in la football season were as follows. — | a b en nd. out hich team is _ more. consistent 2 hp, = 10 66 : a = '"}
{"id": "CSDS/linear regression (3).pdf#p11#c1", "source": "CSDS/linear regression (3).pdf", "page": 11, "snippet": "| oh moments skewness & kurtosts + the at ) maim ere oboud abby meen at. a esl teal l cafe tar | 1s ales by ty, d is ge by ( i. a >. ais my, ne sf. av"}
{"id": "CSDS/linear regression (3).pdf#p12#c1", "source": "CSDS/linear regression (3).pdf", "page": 12, "snippet": "ayles ge s flacal che cl el de bby! ae ot eo “ fot ( 2a ae = 5h st ta ) ae : = see adl = r rt ea | aa bx a oa. aud | cae | uh his dee sta i ) ike ; : "}
{"id": "CSDS/linear regression (3).pdf#p13#c1", "source": "CSDS/linear regression (3).pdf", "page": 13, "snippet": "ae abe ea — 2c, zp p26 asdf - _ es seca bvi op lyfe aa = aaa | at = wisest eee “ ~ ate hy! — 2, byly! go ly ) ” 7 ‘ : ay = ty ) ~ 4s 4 “ 6 al yl 27 ic"}
{"id": "CSDS/linear regression (3).pdf#p14#c1", "source": "CSDS/linear regression (3).pdf", "page": 14, "snippet": "| kurds as! ae to er lorwp | lefe jdec of - fla 2 bist hebn fi bebb. eae ucortedge af lon ), ais pers / our keline ge tern : have. oben. of the - lafn"}
{"id": "CSDS/linear regression (3).pdf#p15#c1", "source": "CSDS/linear regression (3).pdf", "page": 15, "snippet": "nie fest ey, ewes ate sy ~ hashrlyac ree as boovt he volute 5 © adhe 2, 2b, gor dis te vi sen information otter — first fom ected soe foie aes, miso )"}
{"id": "CSDS/linear regression (3).pdf#p16#c1", "source": "CSDS/linear regression (3).pdf", "page": 16, "snippet": ": find the first four moments about the working | mean 44. 5 ofa distribution are — 0. 4, 2. 99, : 0. 08 and 27. 63. calculate the n joments e bout th"}
{"id": "CSDS/linear regression (3).pdf#p17#c1", "source": "CSDS/linear regression (3).pdf", "page": 17, "snippet": "ex. 5 : calculate the first four moments about the mean of the given distribution, » fet onl bo |. feo 5. 0 = f ae 60 90 10 x - 3. 5 sol. : taking a ="}
{"id": "CSDS/linear regression (3).pdf#p18#c1", "source": "CSDS/linear regression (3).pdf", "page": 18, "snippet": "crwth inst ai btintven nn sotthn ponrnibriven tiated — » arcdiordnasistandidedl tes y relations a, b, c of section 5. 5 central moments mi = 0 h2 = ( "}
{"id": "CSDS/linear regression (3).pdf#p19#c1", "source": "CSDS/linear regression (3).pdf", "page": 19, "snippet": "ws = h° z fur = ( 10 ) ° ( = o ) = = ( 1000 ) ( - 0. 11 ) = — 110 ee lt wi, = h te = ( 10 ) gee \\ = ( 10 ) * ( 2. 33 ) = 23300 using relations a, b, c"}
{"id": "CSDS/linear regression (3).pdf#p20#c1", "source": "CSDS/linear regression (3).pdf", "page": 20, "snippet": "i ’ 59. 1 introduction a relationship may be obtained in two series. for ramps ad cael relating to bs and weights of a group of persons are given. it "}
{"id": "CSDS/linear regression (3).pdf#p21#c1", "source": "CSDS/linear regression (3).pdf", "page": 21, "snippet": "59. 4 correlation whenever two variables x and y are so related that an increase in the one is accompanied by an increase or decrease in the other, th"}
{"id": "CSDS/linear regression (3).pdf#p22#c1", "source": "CSDS/linear regression (3).pdf", "page": 22, "snippet": "ee 59. 6 een odom aha ( 2 ) negative correlation oe if the increase in the values of one variable x results in a corresponding decrease in the values "}
{"id": "CSDS/linear regression (3).pdf#p23#c1", "source": "CSDS/linear regression (3).pdf", "page": 23, "snippet": "f7karl pi to measure the intensity or degree of linear relationship between two variables, karl pearson developed a formula called correlation coeffic"}
{"id": "CSDS/linear regression (3).pdf#p24#c1", "source": "CSDS/linear regression (3).pdf", "page": 24, "snippet": "x1 : following are the values of import of raw material and export of finished oduct in suitable units. iy solution : let x : quantity exported, y : q"}
{"id": "CSDS/linear regression (3).pdf#p25#c1", "source": "CSDS/linear regression (3).pdf", "page": 25, "snippet": "43 regression ans. u. l24 scatter diagram indi, bes aizgterh will neosat ee relationship between two variabl ifthe ntrated round a curve. this curve i"}
{"id": "CSDS/linear regression (3).pdf#p26#c1", "source": "CSDS/linear regression (3).pdf", "page": 26, "snippet": "tuvyg pry! smyisciny nmathhnomaice where x and ) are the means of x series and y series. this shows that ( x, ) ) lie on the line of regression ( 1 ),"}
{"id": "CSDS/linear regression (3).pdf#p27#c1", "source": "CSDS/linear regression (3).pdf", "page": 27, "snippet": "ex. 9 : obtain regression lines for the following data ( dec. 2012 ) sol. : to find regression lines we require to calculate regr ession coefficient b"}
{"id": "CSDS/linear regression (3).pdf#p28#c1", "source": "CSDS/linear regression (3).pdf", "page": 28, "snippet": "cov ( x, y ) = 42. 8 — 48 = — 5. 2 4, seon ( 8 yh te yx o, = a oe = ~ 0. 65 cov ( x, £59 dyy be ) at ger lae oy 6. regression line of y on x is > ad y"}
{"id": "CSDS/linear regression (3).pdf#p29#c1", "source": "CSDS/linear regression (3).pdf", "page": 29, "snippet": "dee ee ne oe en nnn at ee ene pomme me re mo ge ne hl a ex. 14 : the regression equations are 8x — 10y + 66 = oand 40x — 1 — = 214. the value of varia"}
{"id": "CSDS/linear regression (3).pdf#p30#c1", "source": "CSDS/linear regression (3).pdf", "page": 30, "snippet": "i. e. y = 0. 8x + 6. 6 and x = 0. 45 y + 5. 35. byx = regression coefficient of y on x = 0. 8 and byy = regression coefficient of x on y = 0. 45 corre"}
{"id": "CSDS/linear regression (3).pdf#p31#c1", "source": "CSDS/linear regression (3).pdf", "page": 31, "snippet": "wee all ee ye ae as example 19. the two regression equations of the variables x and y are x = 19. 13 - 0. 87y and y = 11. 64 — 0. 50x. find ( i ) mean"}
{"id": "CSDS/linear regression (3).pdf#p32#c1", "source": "CSDS/linear regression (3).pdf", "page": 32, "snippet": "example 24. / n a study between the amount of rainfall and the quantity of air pollution removed the following data were collected. daily rainfall in "}
{"id": "CSDS/Assignment-2.pdf#p1#c1", "source": "CSDS/Assignment-2.pdf", "page": 1, "snippet": "- — vv © ) aa > ted atlas [ 33 ] x assignment - 2 1. if the probability that an individual suffers a bad reaction from a certain injection is 0. 001, "}
{"id": "CSDS/least square method (3).pdf#p1#c1", "source": "CSDS/least square method (3).pdf", "page": 1, "snippet": "( chaptery 58 meron of least squares 58. 1 principle of least square : the method of least squares is probably the most systematic procedure to fit a "}
{"id": "CSDS/least square method (3).pdf#p2#c1", "source": "CSDS/least square method (3).pdf", "page": 2, "snippet": "gf veast squares a ss 2 - a ~ br, ) for s t0 be minimum - os n a 2 2 ( y ; ~ a - by, ) ( - 1 ) = 0 or e ( ya bx ) = 0 be ), [ to generalise y, y is wr"}
{"id": "CSDS/least square method (3).pdf#p3#c1", "source": "CSDS/least square method (3).pdf", "page": 3, "snippet": "on solving ( 4 ) and ( 5 ), we get on *. cae 1, b = 1. 9 substituting the values of a and 4 in ( 1 ), we get ans, ye 1 + 1. 9x : i 3 example 2. by the"}
{"id": "CSDS/least square method (3).pdf#p4#c1", "source": "CSDS/least square method (3).pdf", "page": 4, "snippet": "of least squares - 1881 ee ormal equations are = ly = nct + pyy ( 4 ) zxy = clux + b dx : 5 on putting the values of, ex, sy, ¥ xy andy 2? in equation"}
{"id": "CSDS/least square method (3).pdf#p5#c1", "source": "CSDS/least square method (3).pdf", "page": 5, "snippet": "1582 higher engineering mathematics - putting the values of a and 5 in ( 1 ), we get v = 1. 743 u ww ( 4 ) ; x ~ 12. 5 a putting u = and v = y — 20 in"}
{"id": "CSDS/least square method (3).pdf#p6#c1", "source": "CSDS/least square method (3).pdf", "page": 6, "snippet": "of least squares : 1 yo exercise 56, ea find the linear least square polynomials based on d ta is given. find the least square straight line approxima"}
{"id": "CSDS/least square method (3).pdf#p7#c1", "source": "CSDS/least square method (3).pdf", "page": 7, "snippet": "higher engineering mathematics i i la. ae tting the values of a, b and c in ( 1 ), we get the required eae of parabola. on puttin ; fl : anes to sauia"}
{"id": "CSDS/least square method (3).pdf#p8#c1", "source": "CSDS/least square method (3).pdf", "page": 8, "snippet": "normal equations are = yy = na + bexter ya xy = g2xt + beveory. q ) exy = aletbesacy yl ; on putting the values of £ ye xy, z x? yet, in ( 2 ), ( 3 ) "}
{"id": "CSDS/least square method (3).pdf#p9#c1", "source": "CSDS/least square method (3).pdf", "page": 9, "snippet": "higher engineering mathematics 566 on putting the values of £ x, ly, 2 xy, 23 \", lexy, bx, ex, in equations ( 2 ), ( 3 ), ( 4 ), we get 129 = 10a + 10"}
{"id": "CSDS/least square method (3).pdf#p10#c1", "source": "CSDS/least square method (3).pdf", "page": 10, "snippet": "poo ven squares a 10. fit a second degree parabola to the following data : to ts [ we [ se ] o ] se | a ) eat ae cinco solution. let the equation of t"}
{"id": "CSDS/least square method (3).pdf#p11#c1", "source": "CSDS/least square method (3).pdf", "page": 11, "snippet": "exercise 58. 2 1. find the values of a, b, c so that y = a + bx + cx ’ is the best fit to the data : ans. a = 1, b = - 3, c = 2 a - | - 2. fit a secon"}
{"id": "CSDS/Assignment-2(1).pdf#p1#c1", "source": "CSDS/Assignment-2(1).pdf", "page": 1, "snippet": "- — vv © ) aa > ted atlas [ 33 ] x assignment - 2 1. if the probability that an individual suffers a bad reaction from a certain injection is 0. 001, "}
{"id": "CSDS/Assignment-3 (2)(1).pdf#p1#c1", "source": "CSDS/Assignment-3 (2)(1).pdf", "page": 1, "snippet": "mtech csds assignment - 3 1. for the following data, fit a straight - line y = a x + b using least square approximation method. x 2 5 7 11 12 y 5. 2 1"}
{"id": "CSDS/Assignment-3 (2)(1).pdf#p2#c1", "source": "CSDS/Assignment-3 (2)(1).pdf", "page": 2, "snippet": "9. for the stochastic matrix = [ 0 1 / 2 1 / 2 1 / 2 1 / 2 0 0 1 0 ], find the unique fixed vector u. 10. if = 0. 70, = 0. 61, = 0. 40 then find the c"}
{"id": "CSDS/Computational statistics for data science (1).pdf#p1#c1", "source": "CSDS/Computational statistics for data science (1).pdf", "page": 1, "snippet": "course structure course code course category professional core course title computational statistics for data science teaching scheme and credits week"}
{"id": "CSDS/Computational statistics for data science (1).pdf#p2#c1", "source": "CSDS/Computational statistics for data science (1).pdf", "page": 2, "snippet": "distribution, central limit theorem, hypergeometric distribution, uniform distribution, gamma distribution and normal distribution. tests of hypothesi"}
{"id": "CSDS/Computational statistics for data science (1).pdf#p3#c1", "source": "CSDS/Computational statistics for data science (1).pdf", "page": 3, "snippet": "https : / / nptel. ac. in / courses / 111 / 105 / 111105041 / # https : / / nptel. ac. in / courses / 111 / 102 / 111102098 / moocs : https : / / www."}
{"id": "CSDS/Computational statistics for data science (1).pdf#p4#c1", "source": "CSDS/Computational statistics for data science (1).pdf", "page": 4, "snippet": "and variance, sampling distribution of mean and variance, confidence interval estimates of population parameter. 3 random variables and probability di"}
{"id": "CSDS/moments (1).pdf#p1#c1", "source": "CSDS/moments (1).pdf", "page": 1, "snippet": "ee multip relation, 7 | mulhple co ; in + his dependent var eted ore other dep vor - ee ae weigle inkbs = 4, heighd wed f | ayn yeaa = % coch f multip"}
{"id": "CSDS/moments (1).pdf#p2#c1", "source": "CSDS/moments (1).pdf", "page": 2, "snippet": "maeer ' s mit - coe multiple tea re3s jor ) sis be ine muthple nression w desc bes be welotorit - > “ et these voriables & hhica tris relahonseep is u"}
{"id": "CSDS/moments (1).pdf#p3#c1", "source": "CSDS/moments (1).pdf", "page": 3, "snippet": "sm maeer ' s mit - coe regression oe bis eae xt bes. i pe = ag. 49 + bs ), 2 * ) + b39,, ; % 2 - nicrmal d = p as boo | ben, 2 sees koa a 2 rs = sanh "}
{"id": "CSDS/moments (1).pdf#p4#c1", "source": "CSDS/moments (1).pdf", "page": 4, "snippet": "ee cae abuse. adv ) a = 95 zx, = 48, dx » = 102 — 2x = 221, oe f 2 xox, = 10 % i are t — ca. 4t 48 byy at 102 bys 2 = 5f 484,... + 494 b2. 3 + 103 43."}
{"id": "CSDS/multiple correlation from book (2).pdf#p1#c1", "source": "CSDS/multiple correlation from book (2).pdf", "page": 1, "snippet": "084 = u - 950, advantages of multiple cortelati correlation serves the following siltbbsea era the coefficient of multiple 1. it serves as a measure o"}
{"id": "CSDS/multiple correlation from book (2).pdf#p2#c1", "source": "CSDS/multiple correlation from book (2).pdf", "page": 2, "snippet": "statistical methops \\ the second purpose is achieveg of estimate. the third purpose is fficient of determination. d of least aioe i dard error calcula"}
{"id": "CSDS/multiple correlation from book (2).pdf#p3#c1", "source": "CSDS/multiple correlation from book (2).pdf", "page": 3, "snippet": "pa ee bc nsinextvon a = assumptions of linear multipje iti 19 ression ; 7 | analysi ae point estimation, the principle es for point assumptions * of l"}
{"id": "CSDS/multiple correlation from book (2).pdf#p4#c1", "source": "CSDS/multiple correlation from book (2).pdf", "page": 4, "snippet": "o henna i a statistical method 1150 at a7 x. xi x2 3 ss ) se s3 r ag n3 23 other equations of multiple linear regression in the case of two variables,"}
{"id": "CSDS/multiple correlation from book (2).pdf#p5#c1", "source": "CSDS/multiple correlation from book (2).pdf", "page": 5, "snippet": "extensiv in creases, ion and corre ] y multiple regress relation ave on mi f € or are available to eenrlin in fact, it is. bea in the programme libs e"}
{"id": "CSDS/multiple correlation from book (2).pdf#p6#c1", "source": "CSDS/multiple correlation from book (2).pdf", "page": 6, "snippet": "a ta statistical methops 1152 multiplying ea. ( ) by 8, 92 of + 984 bras + 816 bis = 432 iw ) subtracting eqn. ( ii ) from ( iv ), we an bing # 218 br"}
{"id": "CSDS/multiple correlation from book (2).pdf#p7#c1", "source": "CSDS/multiple correlation from book (2).pdf", "page": 7, "snippet": "partial bina nema dd dns ne aininte a lion = 1153 6 : ies bag. 4 = 2 y 23 = h2 hg 1 - pig,. 95 ~ ( 08 ) ( 06 ) a - ( 0 - 6 ) x 064 = 0 - 05 o thus, x2"}
{"id": "CSDS/multiple correlation from book (2).pdf#p8#c1", "source": "CSDS/multiple correlation from book (2).pdf", "page": 8, "snippet": "! | : 3 1154 statistical methods similar to the case of * % and rin two - variable analysis, r2 is easier to e r is a percentage figure. whereas ris n"}
{"id": "CSDS/multiple correlation from book (2).pdf#p9#c1", "source": "CSDS/multiple correlation from book (2).pdf", "page": 9, "snippet": "partial and multiple correlation. fiza = \\ ria + rg — 2 n23 reg 1 - pog - { 0. 6 + ( 0. 87 - 2 ( 0. 6 ) ( 0. 8 ) ( 0. 8 ) 1 - ( 0. 8 )? 0. 36 = y0. 64"}
{"id": "CSDS/multiple correlation from book (2).pdf#p10#c1", "source": "CSDS/multiple correlation from book (2).pdf", "page": 10, "snippet": "i i ause we must h 2 s gre 1 the given data are if cor sistent os 3 | g ate than 1, < 1, for all ( i # j? k = 1, 2, 3 ing i ion : stration 15. given t"}
{"id": "CSDS/multiple correlation from book (2).pdf#p11#c1", "source": "CSDS/multiple correlation from book (2).pdf", "page": 11, "snippet": "grab and multiple correlation pa ( xg - 74 ) = 4 - 04 ( x2 - 7 ) + 4. 36 ( xx, ( x3 - 74 ) = 4 - 04 xo - 28. 28 + 458 % ; oh 65 sn a wg + 4 : 36 x1 + "}
{"id": "CSDS/multiple correlation from book (2).pdf#p12#c1", "source": "CSDS/multiple correlation from book (2).pdf", "page": 12, "snippet": "— _ — _ — 2 sialisiical methopg, 1158 _ — ngi2 $ 3 ) ( xx — xa ) r - % - [ inte | = “ ( 185 3 m2 & ( 4 - 1 ) 1 - 13 ral substituting the values 0 : 57"}
{"id": "CSDS/multiple correlation from book (2).pdf#p13#c1", "source": "CSDS/multiple correlation from book (2).pdf", "page": 13, "snippet": "giial and multiple correlation pa $ = 2 % - % /? n v8 5 - 367 gp = ve hoe 2 ( xo - xe )? ay nia “ g = 123 - 33 = 4. 83 _ ve ux xa? 655 nad hay vre = v"}
{"id": "CSDS/multiple correlation from book (2).pdf#p14#c1", "source": "CSDS/multiple correlation from book (2).pdf", "page": 14, "snippet": "ee statistical method 1160 ng — 131 [ 23 ot, ag = bi23 = gy ™ * 4 - rea substituting the given values 2 0. 7 x0. 6x0. 4 = 4 % \" 4 - 04 ) 0. 46 _ 9. 41"}
{"id": "CSDS/multiple correlation from book (2).pdf#p15#c1", "source": "CSDS/multiple correlation from book (2).pdf", "page": 15, "snippet": "rtial correlation coefficients pa multiple correlation coefficient multiple regression of x1 on x2 and x3 partial regression coefficient list of formu"}
{"id": "CSDS/Stats notes (2).pdf#p1#c1", "source": "CSDS/Stats notes (2).pdf", "page": 1, "snippet": "consider the variate x “ which tates n values ~ ag then the arithmetic mean js al io, - denoted by and given. by — — — $ _ — — qe uther - - - + % ey a"}
{"id": "CSDS/Stats notes (2).pdf#p2#c1", "source": "CSDS/Stats notes (2).pdf", "page": 2, "snippet": "= sind mil valu es 2, dis hs bucton cme — ae - educe the calculahon we use deviahion > | tae d = 4 — a a ig assummed. mean. then we can wnte fd = fu —"}
{"id": "CSDS/Stats notes (2).pdf#p3#c1", "source": "CSDS/Stats notes (2).pdf", "page": 3, "snippet": "_ variate clivides it into two equal part. “ tmealian of. a distnbufiion ts the value _ of the. her discrete type ' — oo mr ’ : number of obsewationg "}
{"id": "CSDS/Stats notes (2).pdf#p4#c1", "source": "CSDS/Stats notes (2).pdf", "page": 4, "snippet": "“ e tek “ obteuin the median of pe dishbuben _ total freq n = a ) nw cas. e7 et 2 4 - just qreater than + 5 s — hs zt. do the corres pond value d - va"}
{"id": "CSDS/Stats notes (2).pdf#p5#c1", "source": "CSDS/Stats notes (2).pdf", "page": 5, "snippet": "_ _ pccurs most pepveetey na set al obsewodtons mode. oo jt ts the, velue. ot. the. “ vediede. “ hick | gt dower limit 54 modal class | = _ freg of mo"}
{"id": "CSDS/Stats notes (2).pdf#p6#c1", "source": "CSDS/Stats notes (2).pdf", "page": 6, "snippet": "— fjnd mean i median and ads fer the. following. freq ueney aistobudion | oo d = 80, 4213 fore fp = 10, “ tae u8 _ mode = je + sea ) — ze = § - 10"}
{"id": "CSDS/Stats notes (2).pdf#p7#c1", "source": "CSDS/Stats notes (2).pdf", "page": 7, "snippet": "_ nole among oll averages, arith mefic mean mected by pluchsations ts jeast a | ‘ dispersion. $ $ $ _ _ _ — _ — — — whether the average gives correct "}
{"id": "CSDS/Stats notes (2).pdf#p8#c1", "source": "CSDS/Stats notes (2).pdf", "page": 8, "snippet": "{ | _ — a dew even,. _. nauance. =. _ here the step t sq yuacing he. deyiedion fa — ¥ xx } _ evercomes the drawbeuk? — of mean deviation. oe — amena, "}
{"id": "CSDS/Stats notes (2).pdf#p9#c1", "source": "CSDS/Stats notes (2).pdf", "page": 9, "snippet": "| dewiatien method saree co let dean a : tt aac me ) ( ox here a ya ett sf a ~ where u = ah fe yap eb ) ( fat ) ~ ( a ) ~ — here fe a4 _ h = no. a abs"}
{"id": "CSDS/Stats notes (2).pdf#p10#c1", "source": "CSDS/Stats notes (2).pdf", "page": 10, "snippet": "a ey goals scered by two. team a and b in la football season were as follows. — | a b en nd. out hich team is _ more. consistent 2 hp, = 10 66 : a = '"}
{"id": "CSDS/Stats notes (2).pdf#p11#c1", "source": "CSDS/Stats notes (2).pdf", "page": 11, "snippet": "| oh moments skewness & kurtosts + the at ) maim ere oboud abby meen at. a esl teal l cafe tar | 1s ales by ty, d is ge by ( i. a >. ais my, ne sf. av"}
{"id": "CSDS/Stats notes (2).pdf#p12#c1", "source": "CSDS/Stats notes (2).pdf", "page": 12, "snippet": "ayles ge s flacal che cl el de bby! ae ot eo “ fot ( 2a ae = 5h st ta ) ae : = see adl = r rt ea | aa bx a oa. aud | cae | uh his dee sta i ) ike ; : "}
{"id": "CSDS/Stats notes (2).pdf#p13#c1", "source": "CSDS/Stats notes (2).pdf", "page": 13, "snippet": "ae abe ea — 2c, zp p26 asdf - _ es seca bvi op lyfe aa = aaa | at = wisest eee “ ~ ate hy! — 2, byly! go ly ) ” 7 ‘ : ay = ty ) ~ 4s 4 “ 6 al yl 27 ic"}
{"id": "CSDS/Stats notes (2).pdf#p14#c1", "source": "CSDS/Stats notes (2).pdf", "page": 14, "snippet": "| kurds as! ae to er lorwp | lefe jdec of - fla 2 bist hebn fi bebb. eae ucortedge af lon ), ais pers / our keline ge tern : have. oben. of the - lafn"}
{"id": "CSDS/Stats notes (2).pdf#p15#c1", "source": "CSDS/Stats notes (2).pdf", "page": 15, "snippet": "nie fest ey, ewes ate sy ~ hashrlyac ree as boovt he volute 5 © adhe 2, 2b, gor dis te vi sen information otter — first fom ected soe foie aes, miso )"}
{"id": "CSDS/Stats notes (2).pdf#p16#c1", "source": "CSDS/Stats notes (2).pdf", "page": 16, "snippet": ": find the first four moments about the working | mean 44. 5 ofa distribution are — 0. 4, 2. 99, : 0. 08 and 27. 63. calculate the n joments e bout th"}
{"id": "CSDS/Stats notes (2).pdf#p17#c1", "source": "CSDS/Stats notes (2).pdf", "page": 17, "snippet": "ex. 5 : calculate the first four moments about the mean of the given distribution, » fet onl bo |. feo 5. 0 = f ae 60 90 10 x - 3. 5 sol. : taking a ="}
{"id": "CSDS/Stats notes (2).pdf#p18#c1", "source": "CSDS/Stats notes (2).pdf", "page": 18, "snippet": "crwth inst ai btintven nn sotthn ponrnibriven tiated — » arcdiordnasistandidedl tes y relations a, b, c of section 5. 5 central moments mi = 0 h2 = ( "}
{"id": "CSDS/Stats notes (2).pdf#p19#c1", "source": "CSDS/Stats notes (2).pdf", "page": 19, "snippet": "ws = h° z fur = ( 10 ) ° ( = o ) = = ( 1000 ) ( - 0. 11 ) = — 110 ee lt wi, = h te = ( 10 ) gee \\ = ( 10 ) * ( 2. 33 ) = 23300 using relations a, b, c"}
{"id": "CSDS/Stats notes (2).pdf#p20#c1", "source": "CSDS/Stats notes (2).pdf", "page": 20, "snippet": "i ’ 59. 1 introduction a relationship may be obtained in two series. for ramps ad cael relating to bs and weights of a group of persons are given. it "}
{"id": "CSDS/Stats notes (2).pdf#p21#c1", "source": "CSDS/Stats notes (2).pdf", "page": 21, "snippet": "59. 4 correlation whenever two variables x and y are so related that an increase in the one is accompanied by an increase or decrease in the other, th"}
{"id": "CSDS/Stats notes (2).pdf#p22#c1", "source": "CSDS/Stats notes (2).pdf", "page": 22, "snippet": "ee 59. 6 een odom aha ( 2 ) negative correlation oe if the increase in the values of one variable x results in a corresponding decrease in the values "}
{"id": "CSDS/Stats notes (2).pdf#p23#c1", "source": "CSDS/Stats notes (2).pdf", "page": 23, "snippet": "f7karl pi to measure the intensity or degree of linear relationship between two variables, karl pearson developed a formula called correlation coeffic"}
{"id": "CSDS/Stats notes (2).pdf#p24#c1", "source": "CSDS/Stats notes (2).pdf", "page": 24, "snippet": "x1 : following are the values of import of raw material and export of finished oduct in suitable units. iy solution : let x : quantity exported, y : q"}
{"id": "CSDS/Stats notes (2).pdf#p25#c1", "source": "CSDS/Stats notes (2).pdf", "page": 25, "snippet": "43 regression ans. u. l24 scatter diagram indi, bes aizgterh will neosat ee relationship between two variabl ifthe ntrated round a curve. this curve i"}
{"id": "CSDS/Stats notes (2).pdf#p26#c1", "source": "CSDS/Stats notes (2).pdf", "page": 26, "snippet": "tuvyg pry! smyisciny nmathhnomaice where x and ) are the means of x series and y series. this shows that ( x, ) ) lie on the line of regression ( 1 ),"}
{"id": "CSDS/Stats notes (2).pdf#p27#c1", "source": "CSDS/Stats notes (2).pdf", "page": 27, "snippet": "ex. 9 : obtain regression lines for the following data ( dec. 2012 ) sol. : to find regression lines we require to calculate regr ession coefficient b"}
{"id": "CSDS/Stats notes (2).pdf#p28#c1", "source": "CSDS/Stats notes (2).pdf", "page": 28, "snippet": "cov ( x, y ) = 42. 8 — 48 = — 5. 2 4, seon ( 8 yh te yx o, = a oe = ~ 0. 65 cov ( x, £59 dyy be ) at ger lae oy 6. regression line of y on x is > ad y"}
{"id": "CSDS/Stats notes (2).pdf#p29#c1", "source": "CSDS/Stats notes (2).pdf", "page": 29, "snippet": "dee ee ne oe en nnn at ee ene pomme me re mo ge ne hl a ex. 14 : the regression equations are 8x — 10y + 66 = oand 40x — 1 — = 214. the value of varia"}
{"id": "CSDS/Stats notes (2).pdf#p30#c1", "source": "CSDS/Stats notes (2).pdf", "page": 30, "snippet": "i. e. y = 0. 8x + 6. 6 and x = 0. 45 y + 5. 35. byx = regression coefficient of y on x = 0. 8 and byy = regression coefficient of x on y = 0. 45 corre"}
{"id": "CSDS/Stats notes (2).pdf#p31#c1", "source": "CSDS/Stats notes (2).pdf", "page": 31, "snippet": "wee all ee ye ae as example 19. the two regression equations of the variables x and y are x = 19. 13 - 0. 87y and y = 11. 64 — 0. 50x. find ( i ) mean"}
{"id": "CSDS/Stats notes (2).pdf#p32#c1", "source": "CSDS/Stats notes (2).pdf", "page": 32, "snippet": "example 24. / n a study between the amount of rainfall and the quantity of air pollution removed the following data were collected. daily rainfall in "}
{"id": "CSDS/normal distribution 27-Sep-2021 13-42-39 (1) (1)(1).pdf#p1#c1", "source": "CSDS/normal distribution 27-Sep-2021 13-42-39 (1) (1)(1).pdf", "page": 1, "snippet": "puke ~ ~ cos case, tte ee a by the yystration 35 - the following table gives the distribution of height of first, 7 irsi e - e at sd * 63 64 65 66 67 "}
{"id": "CSDS/normal distribution 27-Sep-2021 13-42-39 (1) (1)(1).pdf#p2#c1", "source": "CSDS/normal distribution 27-Sep-2021 13-42-39 (1) (1)(1).pdf", "page": 2, "snippet": "se ratcmriag ls 882 ds xt 10 = 67. 9 + 2. 36 = ( 66. 54 and 70. 26 ) f students having height in the range x + a re artic he 93 + 106 + 126 + 109 + 87"}
{"id": "CSDS/normal distribution 27-Sep-2021 13-42-39 (1) (1).pdf#p1#c1", "source": "CSDS/normal distribution 27-Sep-2021 13-42-39 (1) (1).pdf", "page": 1, "snippet": "puke ~ ~ cos case, tte ee a by the yystration 35 - the following table gives the distribution of height of first, 7 irsi e - e at sd * 63 64 65 66 67 "}
{"id": "CSDS/normal distribution 27-Sep-2021 13-42-39 (1) (1).pdf#p2#c1", "source": "CSDS/normal distribution 27-Sep-2021 13-42-39 (1) (1).pdf", "page": 2, "snippet": "se ratcmriag ls 882 ds xt 10 = 67. 9 + 2. 36 = ( 66. 54 and 70. 26 ) f students having height in the range x + a re artic he 93 + 106 + 126 + 109 + 87"}
{"id": "CSDS/assignment-1 (2).pdf#p1#c1", "source": "CSDS/assignment-1 (2).pdf", "page": 1, "snippet": "f. y. m. tech computational statistics for data science assignment - 1 1. the first four moments of a distribution about the value 30. 2 are 0. 255, 6"}
{"id": "CSDS/assignment-1 (2).pdf#p2#c1", "source": "CSDS/assignment-1 (2).pdf", "page": 2, "snippet": "b 97 12 40 96 13 which batsman is more consistent? 8. the following are some of the particulars of the distribution of weights of boys and girls in a "}
{"id": "CSDS/combined average (2).pdf#p1#c1", "source": "CSDS/combined average (2).pdf", "page": 1, "snippet": "i lee 2h 2o _ _ ee ~ she = = 2 \" + o _ £ ee | y combined ¢ts 3 — e — i pte apt int a = = _ speke. n97? ny 05 + n, = neda j — a | 7 wn, fv 7 = 4 where."}
{"id": "CSDS/combined average (2).pdf#p2#c1", "source": "CSDS/combined average (2).pdf", "page": 2, "snippet": "toon ] nt nss tags en a et he “ nyt net nay d tid = fay = xpo5, | : iat :! [ a = tes ] da = lo x 2 pe se | ajyoa = n, 4 % + no % + jo % s, is n, + n. "}
{"id": "CSDS/combined average (2).pdf#p3#c1", "source": "CSDS/combined average (2).pdf", "page": 3, "snippet": "ene eee the hedp a normal cus rey wei cate a — _ measure _ even wish, reader reds ion fw no a ta ems that fat ‘ ois specific ranges : ee a sy nimebica"}
{"id": "CSDS/combined average (2).pdf#p4#c1", "source": "CSDS/combined average (2).pdf", "page": 4, "snippet": "ele ere ] 5 another comparison. es he made of the _ set age hat are f in clu oled | 7 one qn ; md & sd wae : above & helovw the mean st normal cists b"}
{"id": "CSDS/combined average (2).pdf#p5#c1", "source": "CSDS/combined average (2).pdf", "page": 5, "snippet": "se ee _ _ aso al i the tote ] into several _ parts, cathe — — buctebse to \" oven ef the pastors cau vouoaton ly yet | inde 2 of sees. vahionte = ga a "}
{"id": "CSDS/combined average (2).pdf#p6#c1", "source": "CSDS/combined average (2).pdf", "page": 6, "snippet": "| = fes : of. ses dj = jg — aiea l = [ 1130 - ingoj = 30 d = te = fo d = [ a — dos l = ilo _ ex - - the following tuble. gives “ the length sl ufe of "}
{"id": "CSDS/combined average (2).pdf#p7#c1", "source": "CSDS/combined average (2).pdf", "page": 7, "snippet": "a, 4 & t miol point no fe ( hes ) ev oe lovo —! 14 logqrs ” | 12 \\ qo0 - 1999 | 1299, 6 = < gop 1547 | ( 499. 6 | 652 bon - 17449 begs 78 | ian = 19. "}
{"id": "CSDS/combined average (2).pdf#p8#c1", "source": "CSDS/combined average (2).pdf", "page": 8, "snippet": "t 4 wp bed & 2577 : | ha to eee ayjing bed ” [ er as wwe make om weed xe shhect | hems are equally! fe the class icon — 119 - 9 hw she ake [ 2 prep [ "}
{"id": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf#p1#c1", "source": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf", "page": 1, "snippet": "let x,, x ),..., x, ben independent n ( 0, 1 ) variables, then n y = } x ; follows chi - square distribution with n degrees of freedom ( d. f. ). i = "}
{"id": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf#p2#c1", "source": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf", "page": 2, "snippet": "a fsovabiily and rropabiiity visitiduiions hypothesis of the type hi : > yoy : p ; bs wea oa tices oral 1 p < 0. 5, hy : p < h,, hy : 0, > 0 ; etc. ar"}
{"id": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf#p3#c1", "source": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf", "page": 3, "snippet": "p21 test of hypothes sinisintaiieseneeetesenenncnenacaata appr a tule which leads to the decision of acceptance of h, or rejection of hy on me dasis o"}
{"id": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf#p4#c1", "source": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf", "page": 4, "snippet": "of to minimize lev eis ; e ty el of significance, the probability of type ii error increases.. if ignifican 50 jevel op ) of 0. 01 ( ie erty he made z"}
{"id": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf#p5#c1", "source": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf", "page": 5, "snippet": "1. we can apply this test if expected frequencies are greater ani ears to § ( i. e. e ; > 5 ) and total of cell frequencies is sufficiently large ( gr"}
{"id": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf#p6#c1", "source": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf", "page": 6, "snippet": "- — = — _ probability and rrovalne = ) — 2 2 i - - { calculated value ] 3 = 34 > yx3. 005 = reject hy at 5 % l. o. s xoo = 7. 815 [ table value ] e ; "}
{"id": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf#p7#c1", "source": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf", "page": 7, "snippet": "\" a gs i4 were red i, ; between guinea pigs » 10 ‘ springs of a certain cross d numbers should be ; were ie vate ron tue lcanine to a genetic model, t"}
{"id": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf#p8#c1", "source": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf", "page": 8, "snippet": "i ip 2 550 xs 120 = 4. 5833 x5, oos = 11, 07 ' ws < % 5, 003 \\ accept ho. a ie. issuing of book is independent of day. px. 5? in experiment on pea br "}
{"id": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf#p9#c1", "source": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf", "page": 9, "snippet": "a an ee™ succes ci hence the theoretical frequencies of getting vu, 1, 4, 2s %, 1 mnahin ah ssive ‘ 5 terms of the binomial a mops ere var ey permis :"}
{"id": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf#p10#c1", "source": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf", "page": 10, "snippet": "| : a qhe degree of freedom for this experiment is 6 bo x6, oos = 12. 592 s e have, f now, x. 2 2 : xe < x6, 005 _ accept ho. { conclusion : the fit i"}
{"id": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf#p11#c1", "source": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf", "page": 11, "snippet": ", ility that nd the aie a will appear. y : dg ( ii ) three or less ans. ( i ) 0. 1172 ; ( ii ) 0. 171 ea, ars of age is 0. 65. find the years will ive"}
{"id": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf#p12#c1", "source": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf", "page": 12, "snippet": "g os tion of normal sno distribution. ~ opiain the equa curve that may be fitted to the following distribu p | 50 | 6 [ 70 | 80 | 90 | 100 | | _ 5 _ ["}
{"id": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf#p13#c1", "source": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf", "page": 13, "snippet": "es os — it ( computer 11 ) ave average life of 2000 electric lamps are insta ie oe lamps | 7. whoo burn eas with standard deviation of 00 howe. 400 bu"}
{"id": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf#p14#c1", "source": "CSDS/chi square distribution(1) 16-Dec-2023 10-29-07(1) (2).pdf", "page": 14, "snippet": "jocality, 100 persons were randoml s! ir educational achievements. the results are given as es lad and asked for their education test whether eens dep"}
{"id": "CSDS/moments skewness kurtosis (2).pdf#p1#c1", "source": "CSDS/moments skewness kurtosis (2).pdf", "page": 1, "snippet": "aii moments skewness § kusrtosis “ the concept of skewness will clear from the followin three cliag rams showing a sym mets cal astabudion. i ] symmeb"}
{"id": "CSDS/moments skewness kurtosis (2).pdf#p2#c1", "source": "CSDS/moments skewness kurtosis (2).pdf", "page": 2, "snippet": "or ss snl st al st os nn 5 ne se nas a difverenre bepn dispersion & skewns — & ) sk ewnerss dispersion ht js concerned with hie | skewness is an imp c"}
{"id": "CSDS/moments skewness kurtosis (2).pdf#p3#c1", "source": "CSDS/moments skewness kurtosis (2).pdf", "page": 3, "snippet": "absolule mensuua of skewness absotide skewness = a — mode. ff mean = x > mocle. then skewness is + ve. th wemode then it is — vely skewed. distrlution"}
{"id": "CSDS/moments skewness kurtosis (2).pdf#p4#c1", "source": "CSDS/moments skewness kurtosis (2).pdf", "page": 4, "snippet": "2 ) bowley ' s coed? of skewners ( prot bowley ) st hts boseol or quash leg : to symm ebical dismbudion q, § oe eg uidlistewct trom median. ee nl q me"}
{"id": "CSDS/moments skewness kurtosis (2).pdf#p5#c1", "source": "CSDS/moments skewness kurtosis (2).pdf", "page": 5, "snippet": ": 3 k es coeff fd chegna ‘ bo wley ’ s measure neglects dhe dive extreme quarters of the dak. th wowd be better fe - a measur fo cover fhe _ entre cla"}
{"id": "CSDS/moments skewness kurtosis (2).pdf#p6#c1", "source": "CSDS/moments skewness kurtosis (2).pdf", "page": 6, "snippet": "put r = o ie hay = dl a™ = oie fy ab bt ) = tem a = wx - x = 0 ny = + df ( 1 - ¥ ) 2 ge variomte. my, = ly f4 ( xx ) gives pe 314 monsict & so on. act"}
{"id": "CSDS/moments skewness kurtosis (2).pdf#p7#c1", "source": "CSDS/moments skewness kurtosis (2).pdf", "page": 7, "snippet": ": ee ce ee lene ne ees ee tee © retation bebveete ly § al ar = oe zf oa ) ” ee | ae a + a — x ) \" = £ die - ey let jo a = aj ze afd _ sf ( x - “ 4 ) n"}
{"id": "CSDS/moments skewness kurtosis (2).pdf#p8#c1", "source": "CSDS/moments skewness kurtosis (2).pdf", "page": 8, "snippet": "ee eo i. a tes ss ee i es ee te ge a = aa @ ) hew j = x - a f s [ t = 4 ) ] substihing in above setobnga my = a, — re raj / ho + 1 ) \" ( a ) = 30n ) w"}
{"id": "CSDS/moments skewness kurtosis (2).pdf#p9#c1", "source": "CSDS/moments skewness kurtosis (2).pdf", "page": 9, "snippet": "on remark ds change f evigin froperty — the centol momerts are invaduout + o he of origin df u = x - a then ly of u ) = ( tr # * ) d change fovigin f "}
{"id": "CSDS/moments skewness kurtosis (2).pdf#p10#c1", "source": "CSDS/moments skewness kurtosis (2).pdf", "page": 10, "snippet": "erut measure ff skewn cag aik. the dif b skewness = 3 ( mean — median ) oe ah a ober py = ae ] 4 * _ kudosis — te gt jdeg about platness or peaknexs o"}
{"id": "CSDS/moments skewness kurtosis (2).pdf#p11#c1", "source": "CSDS/moments skewness kurtosis (2).pdf", "page": 11, "snippet": "ex — calertlate the first the. mean of tw geo clisti bution hos fr4 f & p, _ comment ov ) tu nature xt cue, am | e ii — — > dias hy nny sfuse 2480 204"}
{"id": "CSDS/moments skewness kurtosis (2).pdf#p12#c1", "source": "CSDS/moments skewness kurtosis (2).pdf", "page": 12, "snippet": "ay = o hy 22 4. ) a = 0464924 ao, = 0 : 00329824 | | | a, = 047999 p, aa fe 2 i 76849 4° b = 44 = 2 : 4587 §. a € ry f ex - find he qirst four central"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p1#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 1, "snippet": "( covers all course fundamentats use this book with any text or as a sett study guide teaches effective problerrsoting 0d problems and worked sehution"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p2#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 2, "snippet": "contents chapter i set theory...................................................... page 1 introduction. sets, elements. set operations. finite and co"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p3#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 3, "snippet": "chapter i set theory introduction this chapter treats some of the elementary ideas and concepts of set theory which are necessary for a modern introdu"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p4#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 4, "snippet": "2 set theory [ chap. 1 example 1. 2 : we use the following special symbols : n = the set of positive integers : 1, 2, 3,... z = the set of integers :."}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p5#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 5, "snippet": "chap. 11 set theory 3 example 1. 6 : the following diagrams, called venn diagrams, illustrate the above set operations. here sets are represented by s"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p6#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 6, "snippet": "4 set theory [ chap. 1 remark : each of the above laws follows from an analogous logical law. for example, anb = { x : xea and xeb } = { x : xeb and x"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p7#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 7, "snippet": "5 chap. 11 set theory the concept of product set is extended to any finite number of sets in a natural way. the product set of the sets ai, a2,..., a,"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p8#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 8, "snippet": "6 set theory [ chap. 1 solved problems sets, elements, subsets 1. 1. let a = { x : 3x = 6 }. does a = 2? a is the set which consists of the single ele"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p9#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 9, "snippet": "7 chap. 13 set theory 1. 9. consider the following sets of figures in the euclidean plane : a = { x : x is a quadrilateral ) c = { x : x is a rhombus "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p10#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 10, "snippet": "8 set theory [ chap. 1 1. 13. in the venn diagram below, shade : ( i ) bc, ( ii ) ( a ub ) c, ( iii ) ( b \\ a ) c, ( iv ) acnbc. bc consists of the el"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p11#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 11, "snippet": "9 chap. 11 set theory 1. 14. prove : b \\ a = bnac. thus the set operation of difference can be written in terms of the operations of intersection and "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p12#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 12, "snippet": "10 set theory [ chap. 1 then compute the union of the two sets : ( a x b ) u ( a x c ) = { ( a, 2 ), ( a, 3 ), ( 6, 2 ), ( b, 3 ), ( a, 4 ), ( b, 4 ) "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p13#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 13, "snippet": "11 chap. 13 set theory 1. 25. let x = ( a, b, c, d, e, f, g }, and let : ( i ) ai = { a, c, e }, a2 = { b }, a3 = { d, g } ; ( ii ) b1 = { a, e, g }, "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p14#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 14, "snippet": "12 set theory [ chap. 1 supplementary problems sets, elements, subsets 1. 31. write in set notation : ( a ) r is a subset of t. ( d ) m is not a subse"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p15#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 15, "snippet": "13 chap. 11 set theory 1. 43. prove : a c b implies a u ( b \\ a ) = b. 1. 44. ( i ) prove : a n ( b \\ c ) = ( a nb ) \\ ( a nc ). ( ii ) give an exampl"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p16#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 16, "snippet": "14 set theory [ chap. 1 answers to supplementary problems 1. 31. ( a ) r c t, ( b ) x e y, ( c ) e ), ( d ) m q! s, ( e ) z 4a, ( f ) r e ca. 1. 32. ("}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p17#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 17, "snippet": "16 chap. 11 set theory 1. 47. the elements of s x t x w are the ordered triplets listed to the right of the tree diagram. 1. 48. each has 60 elements."}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p18#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 18, "snippet": "chapter 2 techniques of counting introduction in this chapter we develop some techniques for determining without direct enumeration the number of poss"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p19#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 19, "snippet": "17 chap. 21 techniques of counting the number of permutations of n objects taken r at a time will be denoted by p ( n, r ) before we derive the genera"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p20#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 20, "snippet": "18 techniques of counting [ chap. 2 we indicate the proof of the above theorem by a particular example. suppose we want to form all possible 5 letter "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p21#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 21, "snippet": "- - chap. 21 techniques of counting 19 on the other hand if there is no replacement, then the first card can be chosen in 52 different ways, the secon"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p22#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 22, "snippet": "20 techniques of counting [ chap. 2 the following properties of the expansion of ( a + b ) \" should be observed : ( i ) there are n + 1terms. ( ii ) t"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p23#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 23, "snippet": "21 chap. 21 techniques of counting combinations suppose we have a collection of n objects. a combination of these n objects taken r at a time, or an r"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p24#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 24, "snippet": "~ 22 techniques of counting [ chap. 2 example 2. 14 : how many committees of 3 can be formed from 8 people? each committee is essentially a combinatio"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p25#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 25, "snippet": "23 chap. 21 techniques of counting tree diagrams a tree diagram is a device used to enumerate all the passible outcomes of a sequence of experiments w"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p26#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 26, "snippet": "24 techniques of counting [ chap. 2 solved problems factorial 2. 1. compute 4!, 5!, 6!, 7! and 8!. 4! = 1. 20304 = 24 7! = 7 * 6! = 7. 720 = 5040 5! ="}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p27#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 27, "snippet": "25 chap. 21 techniques of counting ( v ) the box on the right can be filled in only 1 way, by 5, since the numbers must be multiples of 5 ; the box on"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p28#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 28, "snippet": "26 techniques of counting [ chap. 2 2. 10. suppose an urn contains 8 balls. find the number of ordered samples of size 3 ( i ) with replacement, ( ii "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p29#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 29, "snippet": "27 chap. 21 techniques of counting 2. 16. prove : 24 = 16 = ( t ) + ( ; ) + ( i ) + ( t ) + ( : ). expand ( 1 + l ) 4 using the binomial theorem : + ("}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p30#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 30, "snippet": "28 techniques of counting [ chap. 2 but, by theorem 2. 6, note that ( a3 - b ) ( a + b ) n is a polynomial of degree n + 1 in b. consequently, ( a + b"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p31#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 31, "snippet": "chap. 21 techniques of counting 29 2. 22. a student is to answer 8 out of 10 questions on an exam. ( i ) how many choices has he? ( ii ) how many if h"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p32#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 32, "snippet": "30 techniques of counting [ chap. 2 2. 26. there are 12 students in a class. in how many ways can the 12 students take 3 different tests if 4 students"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p33#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 33, "snippet": "31 chap. 21 techniques of counting tree diagrams 2. 29. construct the tree diagram for the number of permutations of ( a, b, c }. c bac a bca b cab a "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p34#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 34, "snippet": "32 techniques of counting [ chap. 2 2. 36. find the number of ways in which 6 people can ride a toboggan if one of three must drive. 2. 37. ( i ) find"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p35#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 35, "snippet": "33 chap. 21 techniques of counting combinations 2. 55. a class contains 9 boys and 3 girls. ( i ) in how many ways can the teacher choose a committee "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p36#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 36, "snippet": "34 techniques of counting [ chap. 2 2. 70. teams a and b play in a basketball tournament. the first team that wins two games in a row or a total of fo"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p37#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 37, "snippet": "35 9 chap. 21 techniques of counting 2. 42. ( i ) 2 * 4! 4! = i 1152 ( ii ) 2 7 * 3! - 3! = 504 ( iii ) 1152 - 504 = 648 2. 43. ( i ) 3! * 4! = 144 ( "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p38#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 38, "snippet": "36 techniques of counting [ chap. 2 - - 9! 2. 61. 3! 3! 3! - 1680 2. 62. 1680 / 3! = 280 or c ) ( : ) = 280 ( 140 ) ( ; ) 2. 63. - 0 10! - 1 - 2100 or"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p39#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 39, "snippet": "37 chap. 21 techniques of counting 2. 72. hint. the tree is essentially the same as the tree of the preceding problem. 2. 73. the appropriate tree dia"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p40#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 40, "snippet": "chapter 3 introduction to probability introduction probability is the study of random or nondeterministic experiments. if a die is tossed in the air, "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p41#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 41, "snippet": "39 chap. 31 introduction to probability two events a and b are called mutuubby exceusive if they are disjoint, i. e. if a n b = q >. in other words, a"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p42#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 42, "snippet": "40 introduction to probability [ chap. 3 axioms of probability let s be a sample space, let & be the class of events, and let p be a real - valued fun"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p43#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 43, "snippet": "41 chap. 31 introduction to probability theorem 3. 5 : if a and b are any two events, then p ( a ub ) = p ( a ) + p ( b ) - p ( a nb ) proof. note tha"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p44#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 44, "snippet": "42 introduction to probability [ chap. 3 finite equiprobable spaces frequently, the physical characteristics of an experiment suggest that the various"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p45#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 45, "snippet": "chap. 31 introduction to probability 43 example 3. 9 : ( classical birthday problem. ) we seek the probability p that n people have dis - tinct birthd"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p46#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 46, "snippet": "44 introduction to probability [ chap. 3 solved problems sample spaces and events 3. 1. let a and b be events. find an expression and exhibit the venn"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p47#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 47, "snippet": "chap. 31 introduction to probability 45 ( i ) to obtain a, choose those elements of s consisting of an h and an even number : a = { h2, h4, h6 ). to o"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p48#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 48, "snippet": "46 introduction to probability [ chap. 3 two men, ml and m2, and three women, wi, wzand w3, are in a chess tournament. those of the same sex have equa"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p49#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 49, "snippet": "chap. 31 introduction to probability 47 3. 10. two cards are drawn at random from an ordinary deck of 52 cards. find the proba - bility p that ( i ) b"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p50#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 50, "snippet": "48 introduction to probability [ chap. 3 ( i ) there are ( y ) = 66 ways to choose 2 people from the 12 people. ( a ) there are 6 married couples ; he"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p51#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 51, "snippet": "49 chap. 31 introduction to probability 3. 17. three points a, b and c are selected at random from the circumference of a circle. find the probability"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p52#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 52, "snippet": "50 introduction to probability [ chap. 3 3. 22. a die is tossed 100 times. the following table lists the six numbers and frequency with which each num"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p53#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 53, "snippet": "51 chap. 31 introduction to probability supplementary problems sample spaces and events 3. 25. let a and b be events. find an expression and exhibit t"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p54#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 54, "snippet": "52 introduction to probability [ chap. 3 3. 39. ten students, a, b,..., are in a class. if a committee of 3 is chosen at random from the class, find t"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p55#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 55, "snippet": "53 chap. 31 introduction to probability answers to supplementary problems 3. 25. ( i ) aubc, ( ii ) ( aub ) ~ 3. 26. ( i ) ( anbcncc ) u ( bnacncc ) u"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p56#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 56, "snippet": "chapter 4 conditional probability and independence conditional probability let e be an arbitrary event in a sample space s with p ( e ) > 0. the proba"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p57#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 57, "snippet": "55 chap. 41 conditional probability and independence multiplication theorem for conditional : probability if we cross multiply the above equation defi"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p58#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 58, "snippet": "56 conditional probability and independence [ chap. 4 the probability that any particular path of the tree occurs is, by the multiplica - tion theorem"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p59#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 59, "snippet": "57 chap. 41 conditional probability and independence example 4. 6 : three machines a, b and c produce respectively 50 %, 30 % and 20 % of the total nu"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p60#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 60, "snippet": "58 conditional probability and independence [ chap. 4 accordingly, p ( a ) p ( b ) = 211 5 = - 1 = p ( a nb ), and so a and b are independent ; 4 4 = "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p61#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 61, "snippet": "chap. 41 conditional probability and independence 59 example 4. 11 : whenever three horses a, b and c race together, their respective probabilities of"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p62#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 62, "snippet": "60 conditional probability and independence [ chap. 4 4. 2. three fair coins are tossed. find the probability p that they are all heads if ( i ) the f"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p63#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 63, "snippet": "( ; : ) there are chap. 41 conditional probability and independence 61 ( ii ) there are 26 cards, including 4 hearts, divided among east and west. way"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p64#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 64, "snippet": "- - - - - - 62 conditional probability and independence [ chap. 4 ( ii ) there are two mutually exclusive cases : the first pupil is a boy, and the fi"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p65#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 65, "snippet": "63 chap. 41 conditional probability and independence 4. 14. find p ( b ( a ) if ( i ) a is a subset of b, ( ii ) a and b are mutually exclusive. ( i )"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p66#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 66, "snippet": "64 conditional probability and independence [ chap. 4 p ( sne ) - ( 5 ) we have sne = e ; hence p ( s1 e ) = - - - = 1. thus [ p, ] holds. p ( e ) p ("}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p67#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 67, "snippet": "65 chap. 41 conditional probability and independence alternately, by bayes ’ theorem, 4. 20. box a contains nine cards numbered 1 through 9, and box b"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p68#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 68, "snippet": "66 conditional probability and independence [ chap. 4 note that if urn a is selected and a red marble drawn and put into urn b, then urn b has 3 red m"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p69#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 69, "snippet": "67 chap. 41 conditional probability and independence ( ii ) we seek p ( a ub ). p ( a ub ) = p ( a ) + p ( b ) - p ( a nb ) = 5 + q - = + ( iii ) we s"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p70#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 70, "snippet": "68 conditional probability and independence [ chap. 4 independent trials 4 - 28, a certain type of missile hits its target with probability. 3. how ma"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p71#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 71, "snippet": "69 chap. 41 conditional probability and independence supplementary problems conditional probability 4. 31. a die is tossed. if the number is odd, what"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p72#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 72, "snippet": "70 conditional probability and independence [ chap. 4 finite stochastic processes 4. 44. we are given two urns as follows : urn a contains 5 red marbl"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p73#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 73, "snippet": "chap. 41 conditional probability and independence 71 4. 54. a box contains 5 radio tubes of which 2 are defective. the tubes are tested one after the "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p74#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 74, "snippet": "72 conditional probability and independence [ chap. 4 answers to supplementary problems 4. 31. 2 4. 40. ( i ) 8, ( ii ) g, ( iii ) q 4. 32. 4 4. 41. ("}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p75#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 75, "snippet": "73 chap. 41 conditional probability and independence 4. 52. ( i ) & + + q = a, ( ii ) 4, ( iii ) & \\ c - h - i ‘ h ~ n - j - d - d 1 tree diagram for "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p76#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 76, "snippet": "chapter 5 random variables introduction we recall the concept of a function. let s and t be arbitrary sets. suppose to each s e s there is assigned a "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p77#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 77, "snippet": "chap. 51 random variables 75 distribution and expectation of a finite random variable let x be a random variable on a sample space s with a finite ima"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p78#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 78, "snippet": "76 random variables [ chap. 5 the distribution g of y follows : c yi 2 3 4 5 6 7 8 9101112 g ( yi ) ll34sssl3l. l 36 36 36 36 36 36 36 36 36 36 36 we "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p79#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 79, "snippet": "77 chap. 51 random variables this information is put in the form of a table as follows : the mean of x is computed as follows : e ( x ) = cxif ( xi ) "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p80#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 80, "snippet": "78 random variables [ chap. 5 a simple induction argument yields corollary 5. 3 : let xi, x2,..., xn be random variables on s. then e ( x1 + *. * + xn"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p81#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 81, "snippet": "79 chap. 61 random variables yi 2 3 4 5 6 7 8 9101112 4 3 - 2. 4 3 z ig ( ll { ) r 2 336 36 36 36 36 36 36 36 36 36 36 and its mean is by = 7. we comp"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p82#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 82, "snippet": "80 random variables [ chap. 6 the above functions f and g are defined by m n f ( xi ) = c h ( xi, yj ) and g ( yi ) = c h ( xi, ~ i ) j = l t = 1 i. e"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p83#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 83, "snippet": "chap. 51 random variables 81 the above entry h ( 3, 5 ) = 6 comes from the fact that ( 3, 2 ) and ( 2, 3 ) are the only points in s whose maximum numb"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p84#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 84, "snippet": "82 random variables [ chap. 5 for any values xi, yj,..., zk. in particular, x and y are independent if now if x and y have respective distributions f "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p85#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 85, "snippet": "chap. 51 random variables 83 theorem 5. 8 : let x and y be random variables on the same sample space s with y = @ ( x ). then e ( y ) = 2 @ ( xi ) f ("}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p86#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 86, "snippet": "84 random variables [ chap. 6 converges absolutely and the relation cov ( x, y ) pxpy = - pxpy holds just as in the finite case. remark : to avoid tec"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p87#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 87, "snippet": "85 chap. 61 random variables the standard deviation ox is defined by ux = dmq when var ( x ) exists. we have already remarked that we will establish m"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p88#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 88, "snippet": "86 random variables [ chap. 6 example 5. 10 : let x be a discrete random variable with the following distribution : the graph of the cumulative distri"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p89#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 89, "snippet": "chap. 61 random variables 87 we delete all the terms in the above series for which izi - pl < e. this does not increase the value of the series, since"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p90#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 90, "snippet": "88 random variables [ chap. 5 solved problems random variables and expectation 5. 1. find the expectation p, variance 02 and standard deviation u of e"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p91#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 91, "snippet": "89 1 chap. 61 random variables ( ii ) y ( l ) = 1, y ( 2 ) = 3, y ( 3 ) = 1, y ( 4 ) = 3, y ( 6 ) = 1, y ( 6 ) = 3. hence y ( s ) = ( 1, 3 } and g ( 1"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p92#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 92, "snippet": "90 random variables [ chap. 6 5. 3. a coin weighted so that p ( h ) = 3 and p ( t ) = $ is tossed three times. let x be the random variable which deno"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p93#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 93, "snippet": "- - 91 chap. 51 random variables 5. 5. concentric circles of radius 1 and 3 inches are drawn on a circular target of radius 5 inches. a man re - ceive"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p94#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 94, "snippet": "random variables [ chap. 692 ( i ) the marginal distribution on the right is the distribution of x, and the marginal distribution on the bottom is the"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p95#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 95, "snippet": "93 chap. 61 random variables 5. 10. a fair coin is tossed three times. let x denote 0 or 1according as a head or a tail occurs on the first toss, and "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p96#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 96, "snippet": "94 random variables [ chap. 6 ( i ) since y = x2, the random variable y can only take on the values 4 and 1. furthermore, ' g ( 4 ) = p ( y = 4 ) = p "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p97#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 97, "snippet": "95 chap. 51 random variables suppose that x takes on the values sl,..., s, and that + ( s $ takes on the values yl,..., y,,, as i runs from 1 to n. th"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p98#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 98, "snippet": "96 random variables [ chap. 6 5. 18. show that ( proof is given for the case when x and y are discrete and finite. ) since 5. 19. prove theorem 5. 6 :"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p99#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 99, "snippet": "97 chap. 51 random variables 5. 20. prove theorem 5. 7 : let x1, x2,..., xn be independent random variables. then var ( xl + - - + xn ) = var ( xl ) +"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p100#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 100, "snippet": "98 random variables [ chap. 5 the graph of f appears on the right. the region a must have area 1 ; hence 1 k ( b - a ) = 1 or k = - b - a if we view p"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p101#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 101, "snippet": "99 chap. 61 random variables 5. 25. let h be the joint distribution of the random variables x and y. ( i ) show that the distribution f of the sum 2 ="}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p102#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 102, "snippet": "100 random variables [ chap. 6 supplementary problems random variables 5. 26. find the mean p, variance us and standard deviation u of each distributi"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p103#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 103, "snippet": "101 chap. 51 random variables 5. 40. a player tosses three fair coins. he wins $ 10 if 3 heads occur, $ 5 if 2 heads occur, $ 3 if 1 head occurs and $"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p104#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 104, "snippet": "102 random variables [ chap. 6 5. 48. plot the graph of the cumulative distribution function f of the random variable x with distribution 5. 49. show "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p105#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 105, "snippet": "103 chap. 61 random variables 5. 32. 5. 33. 5. 34. 5. 35. 5. 36. 5. 37. 5. 38. 5. 39. 5. 40. 5. 41. 5. 42. 5. 43. 5. 44. ( iii ) g l e ( x + y ) = 6. "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p106#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 106, "snippet": "104 random variables [ chap. 6 5. 45. ( i ) ( ii ) cov ( x, y ) = 52, p ( x, y ) =. 9 5. 46. ( i ) p ( 2 fx f6 ) = 8, p ( 3 sxf7 ) = 9, p ( x5 - 6 ) ="}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p107#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 107, "snippet": "chapter 6 binomial, normal and poisson distributions binomial distribution we consider repeated and independent trials of an experiment with two outco"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p108#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 108, "snippet": "106 i binomial, normal and poisson distributions [ chap. 6 if we regard n and p as constant, then the above function p ( k ) = b ( k ; n, p ) is a dis"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p109#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 109, "snippet": "107 chap. 61 binomial, normal and poisson distributions properties of the normal distribution follow : theorem 6. 3 : mean p variance u2 standard devi"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p110#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 110, "snippet": "108 binomial, normal and poisson distributions [ chap. 6 normal approximation to the binomial distribution. central limit theorem the binomial distrib"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p111#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 111, "snippet": "109 0. 1 chap. 61 binomial, normal and poisson distributions il ill,, 0 4 6 0 h = l h = 2 h = 5 h = 10 poisson distribution for selected values of a p"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p112#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 112, "snippet": "110 binomial, normal and poisson distributions [ chap. 6 standard normal curve ordinates this table gives values + ( t ) of the standard normal distri"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p113#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 113, "snippet": "111 chap. 61 binomial, normal and poisson distributions standard normal curve areas this table gives areas under the stand - ard normal distribution +"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p114#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 114, "snippet": "112 binomial, normal and poisson distributions [ chap. 6 x 0. 0 0. 1 0. 2 0. 3 0. 4 0. 5 0. 6 0. 7 0. 8 0. 9 e - - h 1. 000. 905. 819. 741. 670, 607. "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p115#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 115, "snippet": "chap. 61 binomial, normal and poisson distributions 113 ( ii ) here q4 = ( & ) 4 = $ is the probability that a loses all four games. then 1 - q4 = is "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p116#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 116, "snippet": "114 binomial, normal and poisson distributions [ chap. 6 6. 8. prove theorem 6. 2 : let x be a random variable with the binomial distribution b ( k ; "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p117#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 117, "snippet": "chap. 61 binomial, normal and poisson distributions 115 normal distribution 6. 11. the mean and standard deviation on an examination are 74 and 12 res"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p118#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 118, "snippet": "116 binomial, normal and poisson distributions [ chap. 6 ( iv ) p ( 0. 65 6 x 6 1. 26 ) = p ( 0 5 x 6 1. 26 ) - p ( 0 fx f0. 65 ) =. 3962 - 2422 = s54"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p119#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 119, "snippet": "117 chap. 61 binomial, normal and poisson distributions 6. 16. suppose the temperature t during june is normally distributed with mean 68 \" and standa"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p120#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 120, "snippet": "118 binomial, normal and poisson distributions [ chap. 6 ( ii ) here p = np = 12 - & = 6 and u = = df & = 1. 73. let x denote the number of heads occu"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p121#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 121, "snippet": "119 chap. 61 binomial, normal and poisson distributions 950 in standard units = ( 950 - 1000 ) / 30 = - 1. 67 thus p x p ( x5 950 ) = p ( x * f - 1. 6"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p122#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 122, "snippet": "120 binomial, normal and poisson distributions [ chap. 6 6. 25. show that the poisson distribution p ( k ; a ) is a probability distribution, i. e. k "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p123#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 123, "snippet": "121 chap. 61 binomial, normal and poisson distributions therefore if n is large, x2 x3 in b ( 0 ; n, p ) = n in - a2n 3n2 and hence b ( 0 ; n, p ) = e"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p124#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 124, "snippet": "122 binomial, normal and poisson distributions [ chap. 6 e ( x ) = - ( at + p ) edt2l2 dt = 2 - s - m sw & - - m te - - ' l2 dt +, u - & - - m e - t2 "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p125#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 125, "snippet": "123 chap. 61 binomial, normal and poisson distributions 6. 37. the probability of a man hitting a target is q. ( i ) if he fires 5 times, what is the "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p126#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 126, "snippet": "124 binomial, normal and poisson distributions [ chap. 6 poisson distribution 6. 53. find ( i ) e - - le6, ( ii ) e - - 2. 3. 6. 54. for the poisson d"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p127#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 127, "snippet": "125 chap. 61 binomial, normal and poisson distributions 6. 43. ( i ) + ( & ) =. 3867, + ( * ) =. 3521, # ( - $ ) =. 3011. ( ii ) ( a ) t = k1. 66, ( b"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p128#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 128, "snippet": "chapter 7 markov chains introduction we review the definitions and elementary properties of vectors and matrices which are required for this chapter. "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p129#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 129, "snippet": "127 chap. 71 markov chains where if the number of columns of a is not equal to the number of rows of b, say a is m x p and b is q x n where p # q, the"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p130#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 130, "snippet": "128 markov chains [ chap. 7 example 7. 5 : consider the following vectors : u = ( 2, 0, - &, q ), v = ( 2, q, 0, $ 1 and w = < &, 4, 0, 6 ) then : u i"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p131#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 131, "snippet": "- - 129 chap. 71 markov chains fixed points and regular stochastic matrices the fundamental property of regular stochastic matrices is contained in th"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p132#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 132, "snippet": "130 markov chains [ chap. 7 multiplying the left side of the above matrix equation and then setting correspond - ing components equal to each other, w"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p133#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 133, "snippet": "131 chap. 71 markov chains the first row of the matrix corresponds to the fact that he never takes the train two days in a row and so he definitely wi"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p134#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 134, "snippet": "132 markov chains [ chap. 7 the next theorem answers this question ; here the pg ) are arranged in a matrix pen ) called the n - step transition matri"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p135#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 135, "snippet": "133 chap. 71 markov chains suppose c was the first person with the ball, i. e. suppose p ( 0 ) = ( 0, 0, 1 ) is the initial probability distribution. "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p136#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 136, "snippet": "134 markov chains [ chap. 7 by example 7. 10, the unique fixed probability vector of the above matrix is ( *, # ). thus, in the long run, the man will"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p137#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 137, "snippet": "135 chap. ' 71 markov chains example 7. 24 : a man tosses a fair coin until 3 heads occur in a row. let x,, = k if, at the nth trial, the last tail oc"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p138#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 138, "snippet": "136 markov chains [ chap. 7 7. 2. let a = ( i find ( i ) ab, ( ii ) ba. ’ ) and b = ( 2 - “ ). 2 - 1 3 - 2 6 ( i ) since a is 2 x 2 and b is 2 x 3, th"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p139#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 139, "snippet": "chap. 71 markov chains 137 ( ii ) the sum of the components is 4 + 0 + 1 + 2 - i - 0 + 6 = 12 ; hence multiply the vector, i. e. each component, by to"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p140#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 140, "snippet": "138 markov chains [ chap. 7 a11 a12.. * a21 a22. -. ua = ( ~ 1, up,..., u, )................. - - ( u, all + u2a21 + - - * + unanl, ula12 + u2a22 + - "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p141#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 141, "snippet": "139 chap. 71 markov chains 7. 13. ( i ) show that the vector u = ( b, a ) is a fixed point of the general 2 x 2 stochastic ( ii ) use the result of ( "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p142#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 142, "snippet": "- - - 140 markov chains [ chap. 7 we know that the system has a nonzero solution ; hence we can arbitrarily assign a value to one of the unknowns. set"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p143#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 143, "snippet": "141 chap. 71 markov chains ( iii ) c is not regular since it has a 1 on the main diagonal. since all the entries of 03 are positive, d is regular. mar"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p144#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 144, "snippet": "142 markov chains [ chap. 7 7. 20. given the transition matrix with initial probability distribution p ( 0 ) = ( &, * ). define and find : ( i ) p ', "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p145#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 145, "snippet": "143 chap. 71 markov chains * x + * y + z = y ( 2, ii, 2 ) * * 0 = ( x, y, 4 or * v = x 82 = zi : f d find any nonzero solution of the above system of "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p146#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 146, "snippet": "144 markov chains [ chap. 7 suppose the system is in state al. it can move to state a. if and only if a red marble is selected from urn a and a white "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p147#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 147, "snippet": "chap. 71 markov chains 145 ( i ) we seek p (, 5 ), the probability that the system is in state a. after five steps. compute the 6th step probability d"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p148#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 148, "snippet": "146 markov chains [ chap. 7 we seek a fixed vector u = ( x, y, x, w ) = ( x, g, z, w ). of p : ( x, y, z, w ) p set the corresponding components of up"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p149#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 149, "snippet": "chap. 71 markov chains 147 ( i ) note first that the state space is { al, as, a3 } and so the transition matrix is of the form a1 a2 a3 the ith row of"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p150#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 150, "snippet": "148 markov chains [ chap. 7 supplementary problems matrix multiplication 7. 32. given a =. find ua if ( i ) u = ( 1, - 3, 2 ), ( ii ) u = ( 3, 0, - 2 "}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p151#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 151, "snippet": "- - 149 chap. 71 markov chains 7. 43. ( i ) given that t = ( a, o, &, & ) is a fixed point of a stochastic matrix p, is p regular? ( ii ) given that t"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p152#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 152, "snippet": "150 markov chains [ chap. 7 7. 53. a fair coin is tossed until 3 heads occur in a row. let x, be the length of the sequence of heads ending at the nth"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p153#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 153, "snippet": "151 chap. 71 markov chains 7. 37. ( i ) ( 3 / 13, 0, 2 / 13, 5 / 13, 3 / 13 ) ( ii ) ( 8 / 18, 2 / 18, 0, 1 / 18, 3 / 18, 0, 4 / 18 ) ( iii ) ( 4 / 45"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p154#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 154, "snippet": "index absorbing state, 134 algebra of sets, 5 bayes ’ theorem, 56 bernoulli distribution, 106 binomial coefficients, 19 theorem, 19, 27 binomial distr"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p155#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 155, "snippet": "index 153 mutually exclusive events, 39 n ( positive integers ), 2 n ( cc, u2 ), 107 normal distribution, 106 null set, 1 odds, 42 ordered partitions,"}
{"id": "CSDS/schaums series_Stochastic processes-probability (1).pdf#p156#c1", "source": "CSDS/schaums series_Stochastic processes-probability (1).pdf", "page": 156, "snippet": "schaum ' s outlines and the power of computers... the ultimate solution! now available! an electronic, interactive version of theory and problems of e"}
{"id": "CSDS/Computational statistics for data science (1)(1).pdf#p1#c1", "source": "CSDS/Computational statistics for data science (1)(1).pdf", "page": 1, "snippet": "course structure course code course category professional core course title computational statistics for data science teaching scheme and credits week"}
{"id": "CSDS/Computational statistics for data science (1)(1).pdf#p2#c1", "source": "CSDS/Computational statistics for data science (1)(1).pdf", "page": 2, "snippet": "distribution, central limit theorem, hypergeometric distribution, uniform distribution, gamma distribution and normal distribution. tests of hypothesi"}
{"id": "CSDS/Computational statistics for data science (1)(1).pdf#p3#c1", "source": "CSDS/Computational statistics for data science (1)(1).pdf", "page": 3, "snippet": "https : / / nptel. ac. in / courses / 111 / 105 / 111105041 / # https : / / nptel. ac. in / courses / 111 / 102 / 111102098 / moocs : https : / / www."}
{"id": "CSDS/Computational statistics for data science (1)(1).pdf#p4#c1", "source": "CSDS/Computational statistics for data science (1)(1).pdf", "page": 4, "snippet": "and variance, sampling distribution of mean and variance, confidence interval estimates of population parameter. 3 random variables and probability di"}
{"id": "CSDS/assignment-1 (2)(1).pdf#p1#c1", "source": "CSDS/assignment-1 (2)(1).pdf", "page": 1, "snippet": "f. y. m. tech computational statistics for data science assignment - 1 1. the first four moments of a distribution about the value 30. 2 are 0. 255, 6"}
{"id": "CSDS/assignment-1 (2)(1).pdf#p2#c1", "source": "CSDS/assignment-1 (2)(1).pdf", "page": 2, "snippet": "b 97 12 40 96 13 which batsman is more consistent? 8. the following are some of the particulars of the distribution of weights of boys and girls in a "}
{"id": "CSDS/Assignment-3 (2).pdf#p1#c1", "source": "CSDS/Assignment-3 (2).pdf", "page": 1, "snippet": "mtech csds assignment - 3 1. for the following data, fit a straight - line y = a x + b using least square approximation method. x 2 5 7 11 12 y 5. 2 1"}
{"id": "CSDS/Assignment-3 (2).pdf#p2#c1", "source": "CSDS/Assignment-3 (2).pdf", "page": 2, "snippet": "9. for the stochastic matrix = [ 0 1 / 2 1 / 2 1 / 2 1 / 2 0 0 1 0 ], find the unique fixed vector u. 10. if = 0. 70, = 0. 61, = 0. 40 then find the c"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p1#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 1, "snippet": "ml - unit ii school of computer engineering & technology"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p2#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 2, "snippet": "supervised learning techniques : classification"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p3#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 3, "snippet": "# › supervised vs. unsupervised learning supervised learning ( classification ) supervision : the training data ( observations, measurements, etc. ) a"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p4#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 4, "snippet": "# › classification predicts categorical class labels ( discrete or nominal ) classifies data ( constructs a model ) based on the training set and the "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p5#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 5, "snippet": "# › classification — a two - step process model construction : describing a set of predetermined classes each tuple / sample is assumed to belong to a"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p6#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 6, "snippet": "# › process ( 1 ) : model construction classification algorithms if rank = ‘ professor ’ or years > 6 then tenured = ‘ yes ’"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p7#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 7, "snippet": "# › process ( 2 ) : using the model in prediction ( jeff, professor, 4 ) tenured?"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p8#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 8, "snippet": "# › classification : basic concepts classification : basic concepts decision tree induction"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p9#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 9, "snippet": "# › decision tree induction : an example training data set : buys _ computer the data set follows an example of quinlan ’ s id3 ( playing tennis ) res"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p10#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 10, "snippet": "# › algorithm for decision tree induction basic algorithm ( a greedy algorithm ) tree is constructed in a top - down recursive divide - and - conquer "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p11#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 11, "snippet": "brief review of entropy # › m = 2"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p12#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 12, "snippet": "# › attribute selection measure : information gain ( id3 / c4. 5 ) select the attribute with the highest information gain let pi be the probability th"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p13#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 13, "snippet": "attribute selection : information gain class p : buys _ computer = “ yes ” class n : buys _ computer = “ no ” gain ( age ) > any other gain. so age is"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p14#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 14, "snippet": "# › attribute selection : information gain class p : buys _ computer = “ yes ” class n : buys _ computer = “ no ” means “ age < = 30 ” has 5 out of 14"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p15#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 15, "snippet": "intermediate dt of the buys _ computer dataset age is the splitting attribute at the root node of dt. repeat the procedure to determine the splitting "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p16#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 16, "snippet": "final dt of the buys _ computer dataset"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p17#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 17, "snippet": "example : dt creation"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p18#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 18, "snippet": "example : dt creation"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p19#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 19, "snippet": "example : dt creation"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p20#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 20, "snippet": "example : usage of information gain and entropy in dt creation"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p21#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 21, "snippet": "# › computing information - gain for continuous - valued attributes let attribute a be a continuous - valued attribute must determine the best split p"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p22#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 22, "snippet": "decision tree a classifier ( tree structure ) : used in classification and regression classification mostly uses decision tree decision tree model act"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p23#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 23, "snippet": "classification & regression trees ( cart ) dt creates a model that predicts the value of a target ( or dependent variable ) based on the values of sev"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p24#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 24, "snippet": "# › gain ratio for attribute selection ( c4. 5 ) information gain measure is biased towards attributes with a large number of values c4. 5 ( a success"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p25#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 25, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p26#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 26, "snippet": "# › gini index ( cart, ibm intelligentminer ) if a data set d contains examples from n classes, gini index, gini ( d ) is defined as where pj is the r"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p27#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 27, "snippet": "# › computation of gini index ex. d has 9 tuples in buys _ computer = “ yes ” and 5 in “ no ” suppose the attribute income partitions d into 10 in d1 "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p28#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 28, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p29#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 29, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p30#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 30, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p31#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 31, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p32#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 32, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p33#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 33, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p34#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 34, "snippet": "# › comparing attribute selection measures the three measures, in general, return good results but information gain : biased towards multivalued attri"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p35#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 35, "snippet": "# › overfitting and tree pruning overfitting : an induced tree may overfit the training data too many branches, some may reflect anomalies due to nois"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p36#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 36, "snippet": "# › enhancements to basic decision tree induction allow for continuous - valued attributes dynamically define new discrete - valued attributes that pa"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p37#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 37, "snippet": "# › classification in large databases classification — a classical problem extensively studied by statisticians and machine learning researchers scala"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p38#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 38, "snippet": "support vector machines ( svm ) unit ii"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p39#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 39, "snippet": "# › svm — support vector machines a relatively new classification method for both linear and nonlinear data it uses a nonlinear mapping to transform t"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p40#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 40, "snippet": "svm “ support vector machine ” ( svm ) is a supervised machine learning algorithm which can be used for both classification or regression challenges. "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p41#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 41, "snippet": "how does it work? a thumb rule to identify the right hyper - plane : “ select the hyper - plane which segregates the two classes better ”. in this sce"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p42#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 42, "snippet": "how does it work? maximizing the distances between nearest data point ( either class ) and hyper - plane will help us to decide the right hyper - plan"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p43#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 43, "snippet": "margin in svm margin for hyper - plane c is high as compared to both a and b. hence, we name the right hyper - plane as c. if we select a hyper - plan"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p44#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 44, "snippet": "how does it work? hyper - plane b as it has higher margin compared to a. but, here is the catch, svm selects the hyper - plane which classifies the cl"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p45#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 45, "snippet": "how does it work? the svm algorithm has a feature to ignore outliers and find the hyper - plane that has the maximum margin. hence, we can say, svm cl"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p46#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 46, "snippet": "how does it work? svm can solve this problem. easily! it solves this problem by introducing additional feature. here, we will add a new feature z = x "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p47#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 47, "snippet": "kernel trick the svm kernel is a function that takes low dimensional input space and transforms it to a higher dimensional space i. e. it converts not"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p48#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 48, "snippet": "different kernel functions"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p49#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 49, "snippet": "# › svm — linearly separable a separating hyperplane can be written as w ● x + b = 0 where w = { w1, w2, …, wn } is a weight vector and b a scalar ( b"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p50#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 50, "snippet": "# › why is svm effective on high dimensional data? the complexity of trained classifier is characterized by the # of support vectors rather than the d"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p51#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 51, "snippet": "# › svm — linearly inseparable transform the original input data into a higher dimensional space search for a linear separating hyperplane in the new "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p52#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 52, "snippet": "# › svm : different kernel functions instead of computing the dot product on the transformed data, it is math. equivalent to applying a kernel functio"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p54#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 54, "snippet": "svm in python"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p55#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 55, "snippet": "exercise https : / / www. youtube. com / watch? v = lxgayvxkgtg & t = 189s"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p56#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 56, "snippet": "svm"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p57#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 57, "snippet": "svm"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p61#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 61, "snippet": "bayesian classification a statistical classifier : performs probabilistic prediction, i. e., predicts class membership probabilities foundation : base"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p62#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 62, "snippet": "bayesian classification - example"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p63#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 63, "snippet": "probability : how likely something is to happen probability of an event happening = number of times it can happen _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p64#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 64, "snippet": "let x be a data sample ( “ evidence ” ) : class label is unknown let h be a hypothesis that x belongs to class c classification is to determine p ( h "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p65#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 65, "snippet": "given training data x, posteriori probability of a hypothesis h, p ( h | x ), follows the bayes theorem predicts x belongs to ci iff the probability p"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p66#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 66, "snippet": "let d be a training set of tuples and their associated class labels, and each tuple is represented by an n - d attribute vector x = ( x1, x2, …, xn ) "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p67#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 67, "snippet": "class : c1 : buys _ computer = ‘ yes ’ c2 : buys _ computer = ‘ no ’ data sample x = ( age = youth, income = medium, student = yes credit _ rating = f"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p68#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 68, "snippet": "test for x = ( age = youth, income = medium, student = yes, credit _ rating = fair ) prior probability p ( ci ) : p ( buys _ computer = yes ) = 9 / 14"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p69#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 69, "snippet": "comment on naive bayes classification advantages easy to implement good results obtained in most of the cases disadvantages assumption : class conditi"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p70#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 70, "snippet": "# › lazy vs. eager learning lazy vs. eager learning lazy learning ( e. g., instance - based learning ) : simply stores training data ( or only minor p"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p71#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 71, "snippet": "# › lazy learner : instance - based methods instance - based learning : store training examples and delay the processing ( “ lazy evaluation ” ) until"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p72#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 72, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p73#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 73, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p74#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 74, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p75#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 75, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p76#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 76, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p77#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 77, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p78#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 78, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p79#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 79, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p80#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 80, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p81#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 81, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p82#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 82, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p83#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 83, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p84#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 84, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p85#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 85, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p86#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 86, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p87#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 87, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p88#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 88, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p89#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 89, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p90#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 90, "snippet": "# ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p91#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 91, "snippet": "# › the k - nearest neighbor algorithm all instances correspond to points in the n - d space the nearest neighbor are defined in terms of euclidean di"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p92#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 92, "snippet": "# › discussion on the k - nn algorithm k - nn for real - valued prediction for a given unknown tuple returns the mean values of the k nearest neighbor"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p93#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 93, "snippet": "# › https : / / www. kaggle. com / code / skalskip / iris - data - visualization - and - knn - classification"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p94#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 94, "snippet": "feature scaling feature scaling is the method to limit the range of variables so that they can be compared on common grounds."}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p97#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 97, "snippet": "model evaluation and selection evaluation metrics : how can we measure accuracy? other metrics to consider? use validation test set of class - labeled"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p98#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 98, "snippet": "evaluating classifier accuracy : holdout & cross - validation methods holdout method given data is randomly partitioned into two independent sets trai"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p99#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 99, "snippet": "evaluating classifier accuracy : bootstrap bootstrap works well with small data sets samples the given training tuples uniformly with replacement i. e"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p100#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 100, "snippet": "estimating confidence intervals : classifier models m1 vs. m2 suppose we have 2 classifiers, m1 and m2, which one is better? use 10 - fold cross - val"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p101#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 101, "snippet": "estimating confidence intervals : null hypothesis perform 10 - fold cross - validation assume samples follow a t distribution with k – 1 degrees of fr"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p102#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 102, "snippet": "estimating confidence intervals : t - test if only 1 test set available : pairwise comparison for ith round of 10 - fold cross - validation, the same "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p103#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 103, "snippet": "estimating confidence intervals : table for t - distribution symmetric significance level, e. g., sig = 0. 05 or 5 % means m1 & m2 are significantly d"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p104#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 104, "snippet": "estimating confidence intervals : statistical significance are m1 & m2 significantly different? compute t. select significance level ( e. g. sig = 5 %"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p105#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 105, "snippet": "model selection : roc curves roc ( receiver operating characteristics ) curves : for visual comparison of classification models originated from signal"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p106#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 106, "snippet": "issues affecting model selection accuracy classifier accuracy : predicting class label speed time to construct the model ( training time ) time to use"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p107#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 107, "snippet": "bias and variance 2 / 8 / 2024 # › errors in machine learning irreducible errors are errors which will always be present in a machine learning model, "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p108#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 108, "snippet": "bias and variance 2 / 8 / 2024 # › what is bias? bias refers to the error that is introduced by approximating a real - world problem with a simplified"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p109#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 109, "snippet": "bias and variance 2 / 8 / 2024 # › when the bias is high, assumptions made by our model are too basic, the model can ’ t capture the important feature"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p110#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 110, "snippet": "bias and variance 2 / 8 / 2024 # › what is variance? variance, on the other hand, refers to the amount by which the predictions of a model would chang"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p111#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 111, "snippet": "bias and variance 2 / 8 / 2024 # › what is variance? we can see that our model has learned extremely well for our training data, which has taught it t"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p112#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 112, "snippet": "bias and variance 2 / 8 / 2024 # › hence, model will perform really well on testing data and get high accuracy but will fail to perform on new, unseen"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p113#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 113, "snippet": "bias - variance tradeoff 2 / 8 / 2024 # › in general, the goal of model training is to achieve a balance between bias and variance. this is often refe"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p114#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 114, "snippet": "bias - variance tradeoff 2 / 8 / 2024 # › the above bull ’ s eye graph helps explain bias and variance tradeoff better. the best fit is when the data "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p115#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 115, "snippet": "bias - variance tradeoff 2 / 8 / 2024 # › a model with high bias and low variance may be too simple and fail to capture the complexity of the underlyi"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p116#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 116, "snippet": "bias - variance tradeoff 2 / 8 / 2024 # › in machine learning, bias & variance are two sources of errors that can affect the performance of a model. b"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p117#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 117, "snippet": "bias - variance tradeoff 2 / 8 / 2024 # › cross - validation : cross - validation can be used to evaluate the performance of the model and identify th"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p118#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 118, "snippet": "ensemble method"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p119#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 119, "snippet": "2 / 8 / 2024 data science # ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p120#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 120, "snippet": "2 / 8 / 2024 data science # ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p121#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 121, "snippet": "2 / 8 / 2024 data science # ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p122#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 122, "snippet": "ensemble method training data l1 l2 l3 l4 l * ensemble learning : multiple machine learning models ( classifiers ) are combined to solve a particular "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p123#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 123, "snippet": "“ a group of item viewed as a whole rather than individually ” ensemble method : not depend on just one model or algorithm for your output. rather con"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p124#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 124, "snippet": "d d1 d2 d3 dn c1 c2 c3 cn c * x1 ensemble method 2 / 8 / 2024 # › csp42b : aml original training data randomly chosen sample data classifier with diff"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p125#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 125, "snippet": "2 / 8 / 2024 data science # ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p126#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 126, "snippet": "2 / 8 / 2024 data science # ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p127#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 127, "snippet": "2 / 8 / 2024 data science # ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p128#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 128, "snippet": "2 / 8 / 2024 data science # ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p129#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 129, "snippet": "2 / 8 / 2024 data science # ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p130#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 130, "snippet": "2 / 8 / 2024 data science # ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p131#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 131, "snippet": "bootstrap aggregation ( bagging )"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p132#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 132, "snippet": "bootstrap aggregation ( bagging ) d : original training data ( contains many sample, records ) d1 : randomly chosen sample data record to make new dat"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p133#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 133, "snippet": "bagging reduces overfitting ( variance ) normally uses one type of classifier decision trees are popular easy to parallelize 2 / 8 / 2024 # › csp42b :"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p134#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 134, "snippet": "bagging ( random forest )"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p135#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 135, "snippet": "random forest algorithm ensemble method specifically designed for decision tree classifiers random forests grows many trees ensemble of unpruned decis"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p136#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 136, "snippet": "rf 2 / 8 / 2024 # › csp42b : aml"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p137#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 137, "snippet": "2 / 8 / 2024 data science # ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p138#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 138, "snippet": "boosting boosting is an ensemble learning method that combines a set of weak learners into strong learners to minimize training errors. in boosting, a"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p139#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 139, "snippet": "boosting in boosting, the weak learners are typically decision trees, but they can also be other types of classifiers, such as linear models or neural"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p140#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 140, "snippet": "boosting here ' s how the algorithm works : step 1 : the base algorithm reads the data and assigns equal weight to each sample observation. step 2 : f"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p141#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 141, "snippet": "boosting some popular boosting algorithms include adaboost ( adaptive boosting ), gradient boosting, xgboost. these algorithms differ in the way the w"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p142#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 142, "snippet": "adaboost ( adaptive boosting ) adaboost, short for adaptive boosting, is a machine learning algorithm used for classification and regression tasks. it"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p143#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 143, "snippet": "adaboost ( adaptive boosting ) here are the steps involved in adaboost : initialize the weights of the training examples to be equal. train a weak cla"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p144#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 144, "snippet": "adaboost ( adaptive boosting ) the most important parameters are base _ estimator, n _ estimators, and learning _ rate base _ estimator : it is a weak"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p145#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 145, "snippet": "adaboost ( adaptive boosting ) pros adaboost is easy to implement. it iteratively corrects the mistakes of the weak classifier and improves accuracy b"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p146#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 146, "snippet": "gradient boosting gradient boosting is an ensemble learning technique that combines multiple weak learners ( usually decision trees ) to build a stron"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p147#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 147, "snippet": "gradient boosting here are some of the most important parameters : n _ estimators : this parameter controls the number of trees in the ensemble. learn"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p148#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 148, "snippet": "xgboost ( extreme gradient boosting ) xgboost ( extreme gradient boosting ) is a decision tree - based ensemble machine learning algorithm that uses g"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p149#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 149, "snippet": "xgboost ( extreme gradient boosting ) here ' s how it works : initialization : the algorithm starts with a single decision tree, which serves as the i"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p150#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 150, "snippet": "comparison of boosting algorithms 2 / 8 / 2024 # ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p151#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 151, "snippet": "2 / 8 / 2024 data science # ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p152#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 152, "snippet": "2 / 8 / 2024 data science # ›"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p153#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 153, "snippet": "stacking it is an ensemble method that combines multiple models ( classification or regression ) via meta - model ( meta - classifier or meta - regres"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p154#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 154, "snippet": "stacking stacking is a bit different from the basic ensembling methods because it has first - level and second - level models. stacking features are f"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p155#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 155, "snippet": "stacking stacking is a popular ensemble learning technique used in machine learning to improve the predictive performance of models. the idea behind s"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p156#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 156, "snippet": "# › the stacking process involves the following steps : splitting the training data into two or more folds. training several base models on each fold "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p157#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 157, "snippet": "# › stacking can improve the predictive performance of models because it combines the strengths of different models. for example, one base model may b"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p158#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 158, "snippet": "# › stacking for deep learning"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p159#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 159, "snippet": "how to evaluate ml models 2 / 8 / 2024 # › classification metrics : confusion matrix a confusion matrix is a table that is often used to evaluate the "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p160#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 160, "snippet": "how to evaluate ml models 2 / 8 / 2024 # › confusion matrix : example of a pregnancy test, where an actual pregnant woman and a fat man consult a doct"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p161#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 161, "snippet": "how to evaluate ml models 2 / 8 / 2024 # › tp ( true positive ) : the woman is pregnant, and she is predicted as pregnant. here p represents positive "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p162#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 162, "snippet": "2 / 8 / 2024 # › accuracy : is the simplest metric and can be defined as the number of test cases correctly classified divided by the total number of "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p163#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 163, "snippet": "precision 2 / 8 / 2024 # › precision is the proportion of true positive predictions among all positive predictions made by the classifier. precision ="}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p164#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 164, "snippet": "recall & f1 score 2 / 8 / 2024 # › recall ( also known as sensitivity or true positive rate - tpr ) is the proportion of true positive predictions amo"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p165#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 165, "snippet": "classifier evaluation metrics : confusion matrix actual class \\ predicted class buy _ computer = yes buy _ computer = no total buy _ computer = yes 69"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p166#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 166, "snippet": "confusion matrix - example # › actual class \\ predicted class c1 ¬ c1 c1 true positives ( tp ) false negatives ( fn ) ¬ c1 false positives ( fp ) true"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p167#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 167, "snippet": "classifier evaluation metrics : accuracy, error rate, sensitivity and specificity classifier accuracy, or recognition rate : percentage of test set tu"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p168#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 168, "snippet": "classifier evaluation metrics : precision and recall, and f - measures precision : exactness – what % of tuples that the classifier labeled as positiv"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p169#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 169, "snippet": "classifier evaluation metrics : example # › precision = 90 / 230 = 39. 13 % recall = 90 / 300 = 30. 00 % actual class \\ predicted class cancer = yes c"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p170#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 170, "snippet": "tpr, fpr, tnr, fnr 2 / 8 / 2024 # › rate is a measure factor in a confusion matrix. it has also 4 type tpr, fpr, tnr, fnr true positive rate ( tpr ) :"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p171#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 171, "snippet": "confusion matrix target positive negative model positive 70 tp 20 fp positive predictive value precision 0. 78 negative 30 fn 80 tn negative predictiv"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p172#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 172, "snippet": "classification rate / accuracy accuracy or classification accuracy tells the number of correct predictions made by the model. it is the ratio of the n"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p173#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 173, "snippet": "misclassification rate ( error ) accuracy or classification accuracy tells the number of wrong predictions made by the model. it is the ratio of the n"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p174#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 174, "snippet": "precision / positive predictive value ( ppv ) when it predicts yes, how often is it correct? precision is defined as the number of true positives divi"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p175#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 175, "snippet": "sensitivity / recall / true positive rate when it ’ s actually yes, how often does it predict yes? the recall is the fraction of positive events that "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p176#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 176, "snippet": "specificity / true negative rate when it ’ s no, how often does it predict no? it is the true negative rate or the proportion of true negatives to eve"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p177#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 177, "snippet": "f1 score the f1 score is the harmonic mean of the precision and recall, where f1 score reaches its best value at 1 ( perfect precision and recall ) f1"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p178#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 178, "snippet": "roc chart.. receiver operating characteristic the roc chart : false positive rate ( fpr ) ( 1 - specificity ) on x - axis, the probability of target ="}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p179#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 179, "snippet": "area under the curve ( auc ) area under roc curve is often used as a measure of quality of the classification models. a random classifier has an area "}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p180#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 180, "snippet": "roc - auc an area under the roc curve of 0. 8, for example, means that a randomly selected case from the group with the target equals 1 has a score la"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p181#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 181, "snippet": "2 / 8 / 2024 csp42b : aml # › roc curve"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p182#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 182, "snippet": "auc - roc 2 / 8 / 2024 # › roc curve is a plot of true positive rate ( recall ) against false positive rate ( tn / ( tn + fp ) ). auc - roc stands for"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p183#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 183, "snippet": "auc - roc 2 / 8 / 2024 # › the auc roc score is the area under the roc curve and ranges from 0 to 1, with a score of 0. 5 indicating a random classifi"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p184#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 184, "snippet": "classifier evaluation metrics : confusion matrix actual class \\ predicted class buy _ computer = yes buy _ computer = no total buy _ computer = yes 69"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p185#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 185, "snippet": "classifier evaluation metrics : accuracy, error rate, sensitivity and specificity classifier accuracy, or recognition rate : percentage of test set tu"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p186#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 186, "snippet": "classifier evaluation metrics : precision and recall, and f - measures precision : exactness – what % of tuples that the classifier labeled as positiv"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p187#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 187, "snippet": "classifier evaluation metrics : example # › precision = 90 / 230 = 39. 13 % recall = 90 / 300 = 30. 00 % actual class \\ predicted class cancer = yes c"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p188#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 188, "snippet": "overfitting natural end of process in dt is 100 % purity in each leaf this overfits the data, which end up fitting noise in the data overfitting leads"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p189#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 189, "snippet": "overfitting & underfitting training a model : overfit : if the model is performing much better on train set but not performing well on cross - validat"}
{"id": "AML/Mtech DSA of ML Unit II (1).pptx#p190#c1", "source": "AML/Mtech DSA of ML Unit II (1).pptx", "page": 190, "snippet": "2 / 8 / 2024 csp42b : aml # ›"}
{"id": "AML/AML Unit IV_Final.pdf#p1#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 1, "snippet": "aml - unit iv school of computer engineering & technology"}
{"id": "AML/AML Unit IV_Final.pdf#p2#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 2, "snippet": "model evaluation and selection • to properly evaluate your machine learning models and select the best one, you need a good validation strategy and so"}
{"id": "AML/AML Unit IV_Final.pdf#p3#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 3, "snippet": "model evaluation and selection how to evaluate machine learning models and select the best one? • step 1 : choose a proper validation strategy. • step"}
{"id": "AML/AML Unit IV_Final.pdf#p4#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 4, "snippet": "model selection in machine learning resampling methods • resampling methods - rearranging data samples to inspect if the model performs well on data s"}
{"id": "AML/AML Unit IV_Final.pdf#p5#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 5, "snippet": "model selection in machine learning time - based split • there are some types of data where random splits are not possible. • for example, if we have "}
{"id": "AML/AML Unit IV_Final.pdf#p6#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 6, "snippet": "model selection in machine learning k - fold cross - validation • the cross - validation technique works by randomly shuffling the dataset and then sp"}
{"id": "AML/AML Unit IV_Final.pdf#p7#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 7, "snippet": "model selection in machine learning stratified k - fold • similar to that of k - fold cross - validation with one single point of difference – unlike "}
{"id": "AML/AML Unit IV_Final.pdf#p8#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 8, "snippet": "model selection in machine learning bootstrap : bootstrap is one of the most powerful ways to obtain a stabilized model. • it is close to the random s"}
{"id": "AML/AML Unit IV_Final.pdf#p9#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 9, "snippet": "how to evaluate ml models 12 / 12 / 2024 9 classification metrics : confusion matrix a confusion matrix is a table that is often used to evaluate the "}
{"id": "AML/AML Unit IV_Final.pdf#p10#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 10, "snippet": "how to evaluate ml models 12 / 12 / 2024 10 confusion matrix : example of a pregnancy test, where an actual pregnant woman and a fat man consult a doc"}
{"id": "AML/AML Unit IV_Final.pdf#p11#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 11, "snippet": "how to evaluate ml models 12 / 12 / 2024 11 tp ( true positive ) : the woman is pregnant, and she is predicted as pregnant. here p represents positive"}
{"id": "AML/AML Unit IV_Final.pdf#p12#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 12, "snippet": "12 / 12 / 2024 12 accuracy : is the simplest metric and can be defined as the number of test cases correctly classified divided by the total number of"}
{"id": "AML/AML Unit IV_Final.pdf#p13#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 13, "snippet": "precision 12 / 12 / 2024 13 precision is the proportion of true positive predictions among all positive predictions made by the classifier. precision "}
{"id": "AML/AML Unit IV_Final.pdf#p14#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 14, "snippet": "recall & f1 score 12 / 12 / 2024 14 recall ( also known as sensitivity or true positive rate - tpr ) is the proportion of true positive predictions am"}
{"id": "AML/AML Unit IV_Final.pdf#p15#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 15, "snippet": "classifier evaluation metrics : confusion matrix actual class \\ predicted class buy _ computer = yes buy _ computer = no total buy _ computer = yes 69"}
{"id": "AML/AML Unit IV_Final.pdf#p16#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 16, "snippet": "dr. rajendra pawar confusion matrix - example 16 actual class \\ predicted class c1 ¬ c1 c1 true positives ( tp ) false negatives ( fn ) ¬ c1 false pos"}
{"id": "AML/AML Unit IV_Final.pdf#p17#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 17, "snippet": "classifier evaluation metrics : accuracy, error rate, sensitivity and specificity • classifier accuracy, or recognition rate : percentage of test set "}
{"id": "AML/AML Unit IV_Final.pdf#p18#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 18, "snippet": "classifier evaluation metrics : precision and recall, and f - measures • precision : exactness – what % of tuples that the classifier labeled as posit"}
{"id": "AML/AML Unit IV_Final.pdf#p19#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 19, "snippet": "classifier evaluation metrics : example 19 • precision = 90 / 230 = 39. 13 % recall = 90 / 300 = 30. 00 % actual class \\ predicted class cancer = yes "}
{"id": "AML/AML Unit IV_Final.pdf#p20#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 20, "snippet": "tpr, fpr, tnr, fnr 12 / 12 / 2024 20 rate is a measure factor in a confusion matrix. it has also 4 type tpr, fpr, tnr, fnr • true positive rate ( tpr "}
{"id": "AML/AML Unit IV_Final.pdf#p21#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 21, "snippet": "confusion matrix target positive negative model positive 70 tp 20 fp positive predictive value precision 0. 78 negative 30 fn 80 tn negative predictiv"}
{"id": "AML/AML Unit IV_Final.pdf#p22#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 22, "snippet": "classification rate / accuracy ▪ accuracy or classification accuracy tells the number of correct predictions made by the model. ▪ it is the ratio of t"}
{"id": "AML/AML Unit IV_Final.pdf#p23#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 23, "snippet": "misclassification rate ( error ) ▪ accuracy or classification accuracy tells the number of wrong predictions made by the model. ▪ it is the ratio of t"}
{"id": "AML/AML Unit IV_Final.pdf#p24#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 24, "snippet": "precision / positive predictive value ( ppv ) ▪ when it predicts yes, how often is it correct? ▪ precision is defined as the number of true positives "}
{"id": "AML/AML Unit IV_Final.pdf#p25#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 25, "snippet": "sensitivity / recall / true positive rate ▪ when it ’ s actually yes, how often does it predict yes? ▪ the recall is the fraction of positive events t"}
{"id": "AML/AML Unit IV_Final.pdf#p26#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 26, "snippet": "specificity / true negative rate ▪ when it ’ s no, how often does it predict no? ▪ it is the true negative rate or the proportion of true negatives to"}
{"id": "AML/AML Unit IV_Final.pdf#p27#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 27, "snippet": "f1 score ▪ the f1 score is the harmonic mean of the precision and recall, where ▪ f1 score reaches its best value at 1 ( perfect precision and recall "}
{"id": "AML/AML Unit IV_Final.pdf#p28#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 28, "snippet": "regression metrics 12 / 12 / 2024 28 mean squared error or mse • mse is a simple metric that calculates the difference between the actual value and th"}
{"id": "AML/AML Unit IV_Final.pdf#p29#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 29, "snippet": "regression metrics 12 / 12 / 2024 29 r - squared ( r² ) is a statistical measure that represents the proportion of the variance in the dependent varia"}
{"id": "AML/AML Unit IV_Final.pdf#p30#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 30, "snippet": "regression metrics 12 / 12 / 2024 30 r - squared is calculated as follows : r² = 1 - ( ssres / sstot ) where ssres is the sum of squared residuals ( t"}
{"id": "AML/AML Unit IV_Final.pdf#p31#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 31, "snippet": "regression metrics 12 / 12 / 2024 31 • adjusted r - squared is a modified version of r - squared that takes into account the number of independent var"}
{"id": "AML/AML Unit IV_Final.pdf#p32#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 32, "snippet": "bias and variance 12 / 12 / 2024 32 errors in machine learning • irreducible errors are errors which will always be present in a machine learning mode"}
{"id": "AML/AML Unit IV_Final.pdf#p33#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 33, "snippet": "bias and variance 12 / 12 / 2024 33 what is bias? • bias refers to the error that is introduced by approximating a real - world problem with a simplif"}
{"id": "AML/AML Unit IV_Final.pdf#p34#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 34, "snippet": "bias and variance 12 / 12 / 2024 34 • when the bias is high, assumptions made by our model are too basic, the model can ’ t capture the important feat"}
{"id": "AML/AML Unit IV_Final.pdf#p35#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 35, "snippet": "bias and variance 12 / 12 / 2024 35 what is variance? • variance, on the other hand, refers to the amount by which the predictions of a model would ch"}
{"id": "AML/AML Unit IV_Final.pdf#p36#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 36, "snippet": "bias and variance 12 / 12 / 2024 36 what is variance? • we can see that our model has learned extremely well for our training data, which has taught i"}
{"id": "AML/AML Unit IV_Final.pdf#p37#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 37, "snippet": "bias and variance 12 / 12 / 2024 37 • hence, model will perform really well on testing data and get high accuracy but will fail to perform on new, uns"}
{"id": "AML/AML Unit IV_Final.pdf#p38#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 38, "snippet": "bias - variance tradeoff 12 / 12 / 2024 38 • in general, the goal of model training is to achieve a balance between bias and variance. • this is often"}
{"id": "AML/AML Unit IV_Final.pdf#p39#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 39, "snippet": "bias - variance tradeoff 12 / 12 / 2024 39 • the above bull ’ s eye graph helps explain bias and variance tradeoff better. • the best fit is when the "}
{"id": "AML/AML Unit IV_Final.pdf#p40#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 40, "snippet": "bias - variance tradeoff 12 / 12 / 2024 40 • a model with high bias and low variance may be too simple and fail to capture the complexity of the under"}
{"id": "AML/AML Unit IV_Final.pdf#p41#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 41, "snippet": "bias - variance tradeoff 12 / 12 / 2024 41 • in machine learning, bias & variance are two sources of errors that can affect the performance of a model"}
{"id": "AML/AML Unit IV_Final.pdf#p42#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 42, "snippet": "bias - variance tradeoff 12 / 12 / 2024 42 • cross - validation : cross - validation can be used to evaluate the performance of the model and identify"}
{"id": "AML/AML Unit IV_Final.pdf#p43#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 43, "snippet": "ensemble method"}
{"id": "AML/AML Unit IV_Final.pdf#p44#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 44, "snippet": "12 / 12 / 2024 data science 44"}
{"id": "AML/AML Unit IV_Final.pdf#p45#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 45, "snippet": "positive vs. negative class • in machine learning, a binary classification problem involves dividing a dataset into two classes : positive and negativ"}
{"id": "AML/AML Unit IV_Final.pdf#p46#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 46, "snippet": "positive vs. negative class • to address this issue, techniques such as oversampling the minority class, undersampling the majority class, or using we"}
{"id": "AML/AML Unit IV_Final.pdf#p47#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 47, "snippet": "improving classification accuracy of class imbalanced data improving classification accuracy of class imbalanced data is a challenging problem in mach"}
{"id": "AML/AML Unit IV_Final.pdf#p48#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 48, "snippet": "positive vs. negative class • algorithm selection : certain algorithms perform better on imbalanced datasets than others. algorithms such as random fo"}
{"id": "AML/AML Unit IV_Final.pdf#p49#c1", "source": "AML/AML Unit IV_Final.pdf", "page": 49, "snippet": "12 / 12 / 2024 data science 49"}
{"id": "AML/MLUnit_1.pptx#p1#c1", "source": "AML/MLUnit_1.pptx", "page": 1, "snippet": "1 ai vs. ml"}
{"id": "AML/MLUnit_1.pptx#p2#c1", "source": "AML/MLUnit_1.pptx", "page": 2, "snippet": "2 introduction"}
{"id": "AML/MLUnit_1.pptx#p3#c1", "source": "AML/MLUnit_1.pptx", "page": 3, "snippet": "to solve a problem on a computer, we need an algorithm. an algorithm is a sequence of instructions that should be carried out to transform the input t"}
{"id": "AML/MLUnit_1.pptx#p4#c1", "source": "AML/MLUnit_1.pptx", "page": 4, "snippet": "( source : https : / / medium. com / analytics - vidhya / introduction - to - machine - learning - e1b9c055039c ) machine learning is a “ field of stu"}
{"id": "AML/MLUnit_1.pptx#p5#c1", "source": "AML/MLUnit_1.pptx", "page": 5, "snippet": "a computer program is said to learn from experience ‘ e ’ with respect to some class of task ‘ t ’ and performance measure ‘ p ’ if its performance at"}
{"id": "AML/MLUnit_1.pptx#p6#c1", "source": "AML/MLUnit_1.pptx", "page": 6, "snippet": "example 1 classify email as spam or not spam task ( t ) : classify email as spam or not spam experience ( e ) : watching the user to mark / label the "}
{"id": "AML/MLUnit_1.pptx#p7#c1", "source": "AML/MLUnit_1.pptx", "page": 7, "snippet": "cntd.. example 2 recognizing hand written digits / characters task ( t ) : recognizing hand written digit experience ( e ) : watching the user to mark"}
{"id": "AML/MLUnit_1.pptx#p8#c1", "source": "AML/MLUnit_1.pptx", "page": 8, "snippet": "why machine learning important?. human expertise does not exist navigating on mars industrial / manufacturing control mass spectrometer analysis, drug"}
{"id": "AML/MLUnit_1.pptx#p9#c1", "source": "AML/MLUnit_1.pptx", "page": 9, "snippet": "cntd.. the amount of knowledge available about certain tasks might be too large for explicit encoding by humans ( e. g., medical diagnostic ). new kno"}
{"id": "AML/MLUnit_1.pptx#p10#c1", "source": "AML/MLUnit_1.pptx", "page": 10, "snippet": "10 how does machine learning help us in daily life? social networking : use of the appropriate emotions, suggestions about friend tags on facebook, fi"}
{"id": "AML/MLUnit_1.pptx#p11#c1", "source": "AML/MLUnit_1.pptx", "page": 11, "snippet": "face detection stock prediction spam email detection machine translation self - parking cars airplane navigation systems medicine data mining 11 appli"}
{"id": "AML/MLUnit_1.pptx#p12#c1", "source": "AML/MLUnit_1.pptx", "page": 12, "snippet": "examples … example 1 : hand - written digit recognition : output learn a classifier f ( x ) such that, f : x → { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 } input "}
{"id": "AML/MLUnit_1.pptx#p13#c1", "source": "AML/MLUnit_1.pptx", "page": 13, "snippet": "example 2 : face detection input : an image, the classes are people to be recognized … [ non - face, frontal - face, profile - face ] and the learning"}
{"id": "AML/MLUnit_1.pptx#p14#c1", "source": "AML/MLUnit_1.pptx", "page": 14, "snippet": "example 3 : spam detection this is a classification problem task is to classify email into spam / non - spam requires a learning system as “ enemy ” k"}
{"id": "AML/MLUnit_1.pptx#p15#c1", "source": "AML/MLUnit_1.pptx", "page": 15, "snippet": "example 4 : stock price prediction task is to predict stock price at future date • this is a regression task, as the output is continuous 15"}
{"id": "AML/MLUnit_1.pptx#p16#c1", "source": "AML/MLUnit_1.pptx", "page": 16, "snippet": "example 5 : computational biology 16"}
{"id": "AML/MLUnit_1.pptx#p17#c1", "source": "AML/MLUnit_1.pptx", "page": 17, "snippet": "17"}
{"id": "AML/MLUnit_1.pptx#p18#c1", "source": "AML/MLUnit_1.pptx", "page": 18, "snippet": "18"}
{"id": "AML/MLUnit_1.pptx#p19#c1", "source": "AML/MLUnit_1.pptx", "page": 19, "snippet": "example : weather prediction 19"}
{"id": "AML/MLUnit_1.pptx#p20#c1", "source": "AML/MLUnit_1.pptx", "page": 20, "snippet": "inputs are the relevant information about the patient and the classes are the illnesses. the inputs contain the patient ’ s age, gender, past medical "}
{"id": "AML/MLUnit_1.pptx#p21#c1", "source": "AML/MLUnit_1.pptx", "page": 21, "snippet": "a crop yield prediction app in senegal using satellite imagery ( video link ) https : / / www. youtube. com / watch? v = 4onbgkha4jc & t = 160s. examp"}
{"id": "AML/MLUnit_1.pptx#p22#c1", "source": "AML/MLUnit_1.pptx", "page": 22, "snippet": "data preparation data preparation pipeline 22"}
{"id": "AML/MLUnit_1.pptx#p23#c1", "source": "AML/MLUnit_1.pptx", "page": 23, "snippet": "data preparation 23"}
{"id": "AML/MLUnit_1.pptx#p24#c1", "source": "AML/MLUnit_1.pptx", "page": 24, "snippet": "data preparation 24"}
{"id": "AML/MLUnit_1.pptx#p25#c1", "source": "AML/MLUnit_1.pptx", "page": 25, "snippet": "why is data preparation important? sometimes, data in data sets have missing or incomplete information, which leads to less accurate or incorrect pred"}
{"id": "AML/MLUnit_1.pptx#p26#c1", "source": "AML/MLUnit_1.pptx", "page": 26, "snippet": "steps in data preparation process 1. understand the problem : understand the actual problem and try to solve it. 2. data collection : collect data fro"}
{"id": "AML/MLUnit_1.pptx#p27#c1", "source": "AML/MLUnit_1.pptx", "page": 27, "snippet": "steps in data preparation process 4. data cleaning and validation : data cleaning and validation techniques help determine and solve inconsistencies, "}
{"id": "AML/MLUnit_1.pptx#p28#c1", "source": "AML/MLUnit_1.pptx", "page": 28, "snippet": "steps in data preparation process 6. feature engineering and selection : feature engineering is defined as the study of selecting, manipulating, and t"}
{"id": "AML/MLUnit_1.pptx#p29#c1", "source": "AML/MLUnit_1.pptx", "page": 29, "snippet": "data pre - processing data preprocessing data cleaning data integration data transformation data reduction data discretization 29"}
{"id": "AML/MLUnit_1.pptx#p30#c1", "source": "AML/MLUnit_1.pptx", "page": 30, "snippet": "cntd.. data preparation is also known as data \" pre - processing, \" \" data wrangling, \" \" data cleaning, \" \" data pre - processing, \" and \" feature en"}
{"id": "AML/MLUnit_1.pptx#p31#c1", "source": "AML/MLUnit_1.pptx", "page": 31, "snippet": "cntd.. 31"}
{"id": "AML/MLUnit_1.pptx#p32#c1", "source": "AML/MLUnit_1.pptx", "page": 32, "snippet": "cntd.. 32"}
{"id": "AML/MLUnit_1.pptx#p33#c1", "source": "AML/MLUnit_1.pptx", "page": 33, "snippet": "cntd.. 33"}
{"id": "AML/MLUnit_1.pptx#p34#c1", "source": "AML/MLUnit_1.pptx", "page": 34, "snippet": "cntd.. 34"}
{"id": "AML/MLUnit_1.pptx#p35#c1", "source": "AML/MLUnit_1.pptx", "page": 35, "snippet": "cntd.. 35"}
{"id": "AML/MLUnit_1.pptx#p36#c1", "source": "AML/MLUnit_1.pptx", "page": 36, "snippet": "cntd.. 2. outliers or anomalies : unexpected values ml algorithms are sensitive to the range and distribution of values when data comes from unknown s"}
{"id": "AML/MLUnit_1.pptx#p37#c1", "source": "AML/MLUnit_1.pptx", "page": 37, "snippet": "cntd.. 3. unstructured data format : data comes from various sources and needs to be extracted into a different format. hence, before deploying an ml "}
{"id": "AML/MLUnit_1.pptx#p38#c1", "source": "AML/MLUnit_1.pptx", "page": 38, "snippet": "cntd.. 38"}
{"id": "AML/MLUnit_1.pptx#p39#c1", "source": "AML/MLUnit_1.pptx", "page": 39, "snippet": "cntd.. 39"}
{"id": "AML/MLUnit_1.pptx#p40#c1", "source": "AML/MLUnit_1.pptx", "page": 40, "snippet": "cntd.. 40"}
{"id": "AML/MLUnit_1.pptx#p41#c1", "source": "AML/MLUnit_1.pptx", "page": 41, "snippet": "cntd.. 41"}
{"id": "AML/MLUnit_1.pptx#p42#c1", "source": "AML/MLUnit_1.pptx", "page": 42, "snippet": "cntd.. 42"}
{"id": "AML/MLUnit_1.pptx#p43#c1", "source": "AML/MLUnit_1.pptx", "page": 43, "snippet": "cntd.. 43"}
{"id": "AML/MLUnit_1.pptx#p44#c1", "source": "AML/MLUnit_1.pptx", "page": 44, "snippet": "cntd.. 44"}
{"id": "AML/MLUnit_1.pptx#p45#c1", "source": "AML/MLUnit_1.pptx", "page": 45, "snippet": "feature engineering 45 feature engineering is the pre - processing step of machine learning, which is used to transform raw data into features that ca"}
{"id": "AML/MLUnit_1.pptx#p46#c1", "source": "AML/MLUnit_1.pptx", "page": 46, "snippet": "feature engineering 46 what is a feature? generally, all machine learning algorithms take input data to generate the output. the input data remains in"}
{"id": "AML/MLUnit_1.pptx#p47#c1", "source": "AML/MLUnit_1.pptx", "page": 47, "snippet": "feature engineering 47 3. feature extraction : feature extraction is an automated feature engineering process that generates new variables by extracti"}
{"id": "AML/MLUnit_1.pptx#p48#c1", "source": "AML/MLUnit_1.pptx", "page": 48, "snippet": "feature engineering 48 steps in feature engineering data preparation : in this step, raw data acquired from different resources are prepared to make i"}
{"id": "AML/MLUnit_1.pptx#p49#c1", "source": "AML/MLUnit_1.pptx", "page": 49, "snippet": "feature engineering 49 feature engineering techniques : 1. imputation : imputation is responsible for handling irregularities within the dataset. for "}
{"id": "AML/MLUnit_1.pptx#p50#c1", "source": "AML/MLUnit_1.pptx", "page": 50, "snippet": "feature engineering 50 feature engineering techniques : 3. log transform : log transform helps in handling the skewed data, and it makes the distribut"}
{"id": "AML/MLUnit_1.pptx#p51#c1", "source": "AML/MLUnit_1.pptx", "page": 51, "snippet": "supervised learning types of learning 51"}
{"id": "AML/MLUnit_1.pptx#p52#c1", "source": "AML/MLUnit_1.pptx", "page": 52, "snippet": "types of learning supervised learning : aim is to learn a mapping from the input to an output whose correct values are provided by a supervisor. 1. cl"}
{"id": "AML/MLUnit_1.pptx#p53#c1", "source": "AML/MLUnit_1.pptx", "page": 53, "snippet": "types of learning 53"}
{"id": "AML/MLUnit_1.pptx#p54#c1", "source": "AML/MLUnit_1.pptx", "page": 54, "snippet": "types of learning 54"}
{"id": "AML/MLUnit_1.pptx#p55#c1", "source": "AML/MLUnit_1.pptx", "page": 55, "snippet": "types of learning supervised learning : 55"}
{"id": "AML/MLUnit_1.pptx#p56#c1", "source": "AML/MLUnit_1.pptx", "page": 56, "snippet": "types of learning supervised learning : 56"}
{"id": "AML/MLUnit_1.pptx#p57#c1", "source": "AML/MLUnit_1.pptx", "page": 57, "snippet": "unsupervised learning unsupervised learning 57"}
{"id": "AML/MLUnit_1.pptx#p58#c1", "source": "AML/MLUnit_1.pptx", "page": 58, "snippet": "clustering association 58 example of unsupervised learning"}
{"id": "AML/MLUnit_1.pptx#p59#c1", "source": "AML/MLUnit_1.pptx", "page": 59, "snippet": "clustering association 59 example of unsupervised learning"}
{"id": "AML/MLUnit_1.pptx#p60#c1", "source": "AML/MLUnit_1.pptx", "page": 60, "snippet": "60 example of unsupervised learning"}
{"id": "AML/MLUnit_1.pptx#p61#c1", "source": "AML/MLUnit_1.pptx", "page": 61, "snippet": "61 example of semi - supervised learning"}
{"id": "AML/MLUnit_1.pptx#p62#c1", "source": "AML/MLUnit_1.pptx", "page": 62, "snippet": "reinforcement learning learning from mistakes place a reinforcement learning algorithm into any environment and it will make a lot of mistakes in the "}
{"id": "AML/MLUnit_1.pptx#p63#c1", "source": "AML/MLUnit_1.pptx", "page": 63, "snippet": "reinforcement learning 63"}
{"id": "AML/MLUnit_1.pptx#p64#c1", "source": "AML/MLUnit_1.pptx", "page": 64, "snippet": "where is reinforcement learning in the real world? video games industrial simulation : resource management reinforcement learning 64"}
{"id": "AML/MLUnit_1.pptx#p65#c1", "source": "AML/MLUnit_1.pptx", "page": 65, "snippet": "65"}
{"id": "AML/MLUnit_1.pptx#p66#c1", "source": "AML/MLUnit_1.pptx", "page": 66, "snippet": "key elements of machine learning there are tens of thousands of machine learning algorithms and hundreds of new algorithms are developed every year. e"}
{"id": "AML/MLUnit_1.pptx#p67#c1", "source": "AML/MLUnit_1.pptx", "page": 67, "snippet": "3. optimization : the way candidate programs are generated known as the search process. for example combinatorial optimization, convex optimization, c"}
{"id": "AML/MLUnit_1.pptx#p68#c1", "source": "AML/MLUnit_1.pptx", "page": 68, "snippet": "aspects of developing a learning system : training data, concept representation, function approximation for training and testing purpose of our model "}
{"id": "AML/MLUnit_1.pptx#p69#c1", "source": "AML/MLUnit_1.pptx", "page": 69, "snippet": "validation set validation set is the set of data separate from the training data it is used to validate our model during training it gives information"}
{"id": "AML/MLUnit_1.pptx#p70#c1", "source": "AML/MLUnit_1.pptx", "page": 70, "snippet": "test set a set of data use to test the model the test set is separated from both the train set and validation set once the model is train and validate"}
{"id": "AML/MLUnit_1.pptx#p71#c1", "source": "AML/MLUnit_1.pptx", "page": 71, "snippet": "data split rules for performing data split operation in order to avoid a correlation between the original dataset must be randomly shuffled before app"}
{"id": "AML/MLUnit_1.pptx#p72#c1", "source": "AML/MLUnit_1.pptx", "page": 72, "snippet": "exploratory data analysis refers to the critical process of performing initial investigations on data so as to discover patterns, to spot anomalies, t"}
{"id": "AML/MLUnit_1.pptx#p73#c1", "source": "AML/MLUnit_1.pptx", "page": 73, "snippet": "typical graphical techniques used in eda are : box plot histogram multi - vari chart run chart pareto chart scatter plot stem - and - leaf plot stem -"}
{"id": "AML/MLUnit_1.pptx#p74#c1", "source": "AML/MLUnit_1.pptx", "page": 74, "snippet": "eda example wine quality data set from uci ml repository imported necessary libraries ( for this example pandas, numpy, matplotlib and seaborn ) and l"}
{"id": "AML/MLUnit_1.pptx#p75#c1", "source": "AML/MLUnit_1.pptx", "page": 75, "snippet": "eda original data is separated by delimiter “ ; “ in given data set. to take a closer look at the data took help of “. head ( ) ” function of pandas l"}
{"id": "AML/MLUnit_1.pptx#p76#c1", "source": "AML/MLUnit_1.pptx", "page": 76, "snippet": "eda techniques found out the total number of rows and columns in the data set using “. shape ” dataset comprises of 4898 observations and 12 character"}
{"id": "AML/MLUnit_1.pptx#p77#c1", "source": "AML/MLUnit_1.pptx", "page": 77, "snippet": "eda : exploratory data analysis 77 plotting using matplotlib import pandas as pd import matplotlib. pyplot as plt iris = pd. read _ csv ( \" iris. csv "}
{"id": "AML/MLUnit_1.pptx#p78#c1", "source": "AML/MLUnit_1.pptx", "page": 78, "snippet": "78"}
{"id": "AML/MLUnit_1.pptx#p79#c1", "source": "AML/MLUnit_1.pptx", "page": 79, "snippet": "eda : exploratory data analysis 79 3d scatter plot https : / / plot. ly / pandas / 3d - scatter - plots / import plotly import plotly. express as px i"}
{"id": "AML/MLUnit_1.pptx#p80#c1", "source": "AML/MLUnit_1.pptx", "page": 80, "snippet": "80"}
{"id": "AML/MLUnit_1.pptx#p81#c1", "source": "AML/MLUnit_1.pptx", "page": 81, "snippet": "81"}
{"id": "AML/MLUnit_1.pptx#p82#c1", "source": "AML/MLUnit_1.pptx", "page": 82, "snippet": "82"}
{"id": "AML/MLUnit_1.pptx#p83#c1", "source": "AML/MLUnit_1.pptx", "page": 83, "snippet": "progressive data analysis 83 data has only float and integer values. no variable column has null / missing values."}
{"id": "AML/MLUnit_1.pptx#p84#c1", "source": "AML/MLUnit_1.pptx", "page": 84, "snippet": "pda techniques the describe ( ) function in pandas is very handy in getting various summary statistics. this function returns the count, mean, standar"}
{"id": "AML/MLUnit_1.pptx#p85#c1", "source": "AML/MLUnit_1.pptx", "page": 85, "snippet": "data preparation : types of data here as you can notice mean value is larger than median value of each column which is represented by 50 % ( 50th perc"}
{"id": "AML/MLUnit_1.pptx#p86#c1", "source": "AML/MLUnit_1.pptx", "page": 86, "snippet": "graph visualisation techniques let ’ s now explore data with beautiful graphs. python has a visualization library, seaborn which build on top of matpl"}
{"id": "AML/MLUnit_1.pptx#p87#c1", "source": "AML/MLUnit_1.pptx", "page": 87, "snippet": "87"}
{"id": "AML/MLUnit_1.pptx#p88#c1", "source": "AML/MLUnit_1.pptx", "page": 88, "snippet": "data pre - processing techniques for ml applications dark shades represents positive correlation while lighter shades represents negative correlation."}
{"id": "AML/MLUnit_1.pptx#p89#c1", "source": "AML/MLUnit_1.pptx", "page": 89, "snippet": "89"}
{"id": "AML/MLUnit_1.pptx#p90#c1", "source": "AML/MLUnit_1.pptx", "page": 90, "snippet": "box plot a box plot ( or box - and - whisker plot ) shows the distribution of quantitative data in a way that facilitates comparisons between variable"}
{"id": "AML/MLUnit_1.pptx#p91#c1", "source": "AML/MLUnit_1.pptx", "page": 91, "snippet": "minimum first quartile median third quartile maximum. in the simplest box plot the central rectangle spans the first quartile to the third quartile ( "}
{"id": "AML/MLUnit_1.pptx#p92#c1", "source": "AML/MLUnit_1.pptx", "page": 92, "snippet": "92"}
{"id": "AML/MLUnit_1.pptx#p93#c1", "source": "AML/MLUnit_1.pptx", "page": 93, "snippet": "sparse matrix in numerical analysis and scientific computing, a sparse matrix or sparse array is a matrix in which most of the elements are zero. by c"}
{"id": "AML/MLUnit_1.pptx#p94#c1", "source": "AML/MLUnit_1.pptx", "page": 94, "snippet": "feature engineering : feature selection in machine learning and statistics, feature selection, also known as variable selection, attribute selection o"}
{"id": "AML/MLUnit_1.pptx#p95#c1", "source": "AML/MLUnit_1.pptx", "page": 95, "snippet": "feature engineering : feature selection feature selection is primarily focused on removing non - informative or redundant predictors from the model. f"}
{"id": "AML/MLUnit_1.pptx#p96#c1", "source": "AML/MLUnit_1.pptx", "page": 96, "snippet": "feature engineering : feature selection there are two main types of feature selection techniques : supervised and unsupervised supervised methods may "}
{"id": "AML/MLUnit_1.pptx#p97#c1", "source": "AML/MLUnit_1.pptx", "page": 97, "snippet": "feature engineering : feature selection unsupervised feature selection techniques ignores the target variable, such as methods that remove redundant v"}
{"id": "AML/MLUnit_1.pptx#p98#c1", "source": "AML/MLUnit_1.pptx", "page": 98, "snippet": "feature engineering : feature selection filter : statistical - based feature selection methods involve evaluating the relationship between each input "}
{"id": "AML/MLUnit_1.pptx#p99#c1", "source": "AML/MLUnit_1.pptx", "page": 99, "snippet": "feature engineering : feature selection wrapper : wrapper feature selection methods create many models with different subsets of input features and se"}
{"id": "AML/MLUnit_1.pptx#p100#c1", "source": "AML/MLUnit_1.pptx", "page": 100, "snippet": "feature engineering : feature selection statistics for filter - based feature selection methods it is common to use correlation type statistical measu"}
{"id": "AML/MLUnit_1.pptx#p101#c1", "source": "AML/MLUnit_1.pptx", "page": 101, "snippet": "feature engineering : feature selection filter - based feature selection method. consider two broad categories of variable types : numerical and categ"}
{"id": "AML/MLUnit_1.pptx#p102#c1", "source": "AML/MLUnit_1.pptx", "page": 102, "snippet": "feature engineering : feature selection 102"}
{"id": "AML/MLUnit_1.pptx#p103#c1", "source": "AML/MLUnit_1.pptx", "page": 103, "snippet": "feature engineering : feature selection 1. numerical input, numerical output this is a regression predictive modeling problem with numerical input var"}
{"id": "AML/MLUnit_1.pptx#p104#c1", "source": "AML/MLUnit_1.pptx", "page": 104, "snippet": "feature engineering : feature selection 3. categorical input, numerical output this is a regression predictive modeling problem with categorical input"}
{"id": "AML/MLUnit_1.pptx#p105#c1", "source": "AML/MLUnit_1.pptx", "page": 105, "snippet": "feature engineering : feature selection recursive feature elimination recursive feature elimination, or rfe for short, is a feature selection algorith"}
{"id": "AML/MLUnit_1.pptx#p106#c1", "source": "AML/MLUnit_1.pptx", "page": 106, "snippet": "feature engineering : feature selection rfe is a wrapper - type feature selection algorithm. this means that a different machine learning algorithm is"}
{"id": "AML/MLUnit_1.pptx#p107#c1", "source": "AML/MLUnit_1.pptx", "page": 107, "snippet": "we can see / visualize 2d, 3d data ….. by scatterplot 4d, 5d, 6d … … ….. use pair plot ….. ( nc2 pairs ) 10d, 100d, 1000d data? visualization of high "}
{"id": "AML/MLUnit_1.pptx#p108#c1", "source": "AML/MLUnit_1.pptx", "page": 108, "snippet": "why pca? for dimensionality reduction i. e. d - dim d ’ - dim e. g. mnist dataset of 784 - dim to 2 dim # mnist dataset downloaded from kaggle : # htt"}
{"id": "AML/MLUnit_1.pptx#p109#c1", "source": "AML/MLUnit_1.pptx", "page": 109, "snippet": "pca steps for dimensionality reduction : column standardization of data find covariance matrix find eigen values and eigen vectors find principal comp"}
{"id": "AML/MLUnit_1.pptx#p110#c1", "source": "AML/MLUnit_1.pptx", "page": 110, "snippet": "1. standardization of the data missing out on standardization will probably result in a biased outcome. standardization is all about scaling your data"}
{"id": "AML/MLUnit_1.pptx#p111#c1", "source": "AML/MLUnit_1.pptx", "page": 111, "snippet": "2 computing the covariance matrix a covariance matrix expresses the correlation between the different variables in the data set. it is essential to id"}
{"id": "AML/MLUnit_1.pptx#p112#c1", "source": "AML/MLUnit_1.pptx", "page": 112, "snippet": "3. calculating the eigenvectors and eigenvalues eigenvectors and eigenvalues are computed from the covariance matrix in order to determine the princip"}
{"id": "AML/MLUnit_1.pptx#p113#c1", "source": "AML/MLUnit_1.pptx", "page": 113, "snippet": "4. computing the principal components eigenvectors and eigenvalues placed in the descending order where the eigenvector with the highest eigenvalue is"}
{"id": "AML/MLUnit_1.pptx#p114#c1", "source": "AML/MLUnit_1.pptx", "page": 114, "snippet": "5. reducing the dimensions of the data set performing pca is to re - arrange the original data with the final principal components which represent the"}
{"id": "AML/MLUnit_1.pptx#p115#c1", "source": "AML/MLUnit_1.pptx", "page": 115, "snippet": "t - sne https : / / distill. pub / 2016 / misread - tsne / https : / / colah. github. io / posts / 2014 - 10 - visualizing - mnist / t - sne is t - di"}
{"id": "AML/MLUnit_1.pptx#p116#c1", "source": "AML/MLUnit_1.pptx", "page": 116, "snippet": "t - sne. neighborhood and embedding points are geometrically together … …. neighborhood embedding …. for every points in high - dim space finding its "}
{"id": "AML/MLUnit_1.pptx#p117#c1", "source": "AML/MLUnit_1.pptx", "page": 117, "snippet": "t - sne crowding problem : e. g … 2dim to 1 dim sometimes it is impossible to preserve distance in all the neighborhood points such problem is called "}
{"id": "AML/MLUnit_1.pptx#p118#c1", "source": "AML/MLUnit_1.pptx", "page": 118, "snippet": "t - sne https : / / distill. pub / 2016 / misread - tsne / run t - sne on simple dataset perplexity : points in neighbors epsilion : learning rate ste"}
{"id": "AML/MLUnit_1.pptx#p119#c1", "source": "AML/MLUnit_1.pptx", "page": 119, "snippet": "119"}
{"id": "AML/MLUnit_1.pptx#p120#c1", "source": "AML/MLUnit_1.pptx", "page": 120, "snippet": "120"}
{"id": "AML/MLUnit_1.pptx#p121#c1", "source": "AML/MLUnit_1.pptx", "page": 121, "snippet": "121"}
{"id": "AML/REGRESSION (1).pptx#p1#c1", "source": "AML/REGRESSION (1).pptx", "page": 1, "snippet": "why machine learning important? 1"}
{"id": "AML/REGRESSION (1).pptx#p2#c1", "source": "AML/REGRESSION (1).pptx", "page": 2, "snippet": "what is regression “ regression is the process of estimating the relationship between a dependent variable and independent variables it is a fitting a"}
{"id": "AML/REGRESSION (1).pptx#p3#c1", "source": "AML/REGRESSION (1).pptx", "page": 3, "snippet": "types of regression techniques there are many types of regression analysis techniques, and the use of each method depends upon the number of factors t"}
{"id": "AML/REGRESSION (1).pptx#p4#c1", "source": "AML/REGRESSION (1).pptx", "page": 4, "snippet": "linear regression “ linear regression predicts the relationship between two variables by assuming a linear connection between the independent and depe"}
{"id": "AML/REGRESSION (1).pptx#p5#c1", "source": "AML/REGRESSION (1).pptx", "page": 5, "snippet": "simple linear regression in a simple linear regression, there is one independent variable and one dependent variable the model estimates the slope and"}
{"id": "AML/REGRESSION (1).pptx#p6#c1", "source": "AML/REGRESSION (1).pptx", "page": 6, "snippet": "simple linear regression linear regression shows the linear relationship between the independent ( predictor ) variable i. e. x - axis and the depende"}
{"id": "AML/REGRESSION (1).pptx#p7#c1", "source": "AML/REGRESSION (1).pptx", "page": 7, "snippet": "simple regression calculation to calculate best - fit line linear regression uses a traditional slope - intercept form which is given below, yi = β0 +"}
{"id": "AML/REGRESSION (1).pptx#p8#c1", "source": "AML/REGRESSION (1).pptx", "page": 8, "snippet": "least - squares method finds the line of best fit for a dataset, providing a visual demonstration of the relationship between the data points. the dif"}
{"id": "AML/REGRESSION (1).pptx#p9#c1", "source": "AML/REGRESSION (1).pptx", "page": 9, "snippet": "least - squares method formula of least square method step 1 : denote the independent variable values as xi and the dependent ones as yi. step 2 : cal"}
{"id": "AML/REGRESSION (1).pptx#p10#c1", "source": "AML/REGRESSION (1).pptx", "page": 10, "snippet": "least - squares method problem 1 : find the line of best fit for the following data points using the least squares method : ( x, y ) = ( 1, 3 ), ( 2, "}
{"id": "AML/REGRESSION (1).pptx#p11#c1", "source": "AML/REGRESSION (1).pptx", "page": 11, "snippet": "least - squares method the slope of the line of best fit can be calculated from the formula as follows : m = ( σ ( x – xi ) * ( y – yi ) ) / σ ( x – x"}
{"id": "AML/REGRESSION (1).pptx#p12#c1", "source": "AML/REGRESSION (1).pptx", "page": 12, "snippet": "multiple linear regression / multivariable regression multiple linear regression is a technique to understand the relationship between a single depend"}
{"id": "AML/REGRESSION (1).pptx#p13#c1", "source": "AML/REGRESSION (1).pptx", "page": 13, "snippet": "considerations of multiple linear regression multiple linear regression assumptions overfitting : when more and more variables are added to a model, t"}
{"id": "AML/REGRESSION (1).pptx#p14#c1", "source": "AML/REGRESSION (1).pptx", "page": 14, "snippet": "multiple linear regression / multivariable regression"}
{"id": "AML/REGRESSION (1).pptx#p15#c1", "source": "AML/REGRESSION (1).pptx", "page": 15, "snippet": "multiple linear regression / multivariable regression"}
{"id": "AML/REGRESSION (1).pptx#p16#c1", "source": "AML/REGRESSION (1).pptx", "page": 16, "snippet": "multiple linear regression / multivariable regression suppose we have the following dataset with one response variable y and two predictor variables x"}
{"id": "AML/REGRESSION (1).pptx#p17#c1", "source": "AML/REGRESSION (1).pptx", "page": 17, "snippet": "multiple linear regression / multivariable regression step 1 : calculate x12, x22, x1y, x2y and x1x2."}
{"id": "AML/REGRESSION (1).pptx#p18#c1", "source": "AML/REGRESSION (1).pptx", "page": 18, "snippet": "multiple linear regression / multivariable regression step 2 : calculate regression sums. next, make the following regression sum calculations : σx12 "}
{"id": "AML/REGRESSION (1).pptx#p19#c1", "source": "AML/REGRESSION (1).pptx", "page": 19, "snippet": "multiple linear regression / multivariable regression step 3 : calculate b0, b1, and b2. the formula to calculate b1 is : [ ( σx22 ) ( σx1y ) – ( σx1x"}
{"id": "AML/REGRESSION (1).pptx#p20#c1", "source": "AML/REGRESSION (1).pptx", "page": 20, "snippet": "logistic regression logistic regression is one of the types of regression analysis technique, which gets used when the dependent variable is discrete."}
{"id": "AML/REGRESSION (1).pptx#p21#c1", "source": "AML/REGRESSION (1).pptx", "page": 21, "snippet": "logistic regression if the output of the sigmoid function ( estimated probability ) is greater than a predefined threshold on the graph, the model pre"}
{"id": "AML/REGRESSION (1).pptx#p22#c1", "source": "AML/REGRESSION (1).pptx", "page": 22, "snippet": "linear vs logistic"}
{"id": "AML/REGRESSION (1).pptx#p23#c1", "source": "AML/REGRESSION (1).pptx", "page": 23, "snippet": "lasso regression lasso regression is one of the types of regression in machine learning that performs regularization along with feature selection. it "}
{"id": "AML/REGRESSION (1).pptx#p24#c1", "source": "AML/REGRESSION (1).pptx", "page": 24, "snippet": "lasso regression lasso regression is a type of linear regression that uses shrinkage. shrinkage is where data values are shrunk towards a central poin"}
{"id": "AML/REGRESSION (1).pptx#p25#c1", "source": "AML/REGRESSION (1).pptx", "page": 25, "snippet": "lasso regression l1 regularization lasso regression performs l1 regularization, which adds a penalty equal to the absolute value of the magnitude of c"}
{"id": "AML/REGRESSION (1).pptx#p26#c1", "source": "AML/REGRESSION (1).pptx", "page": 26, "snippet": "ridge regression this is another one of the types of regression in machine learning which is usually used when there is a high correlation between the"}
{"id": "AML/REGRESSION (1).pptx#p27#c1", "source": "AML/REGRESSION (1).pptx", "page": 27, "snippet": "polynomial regression polynomial regression is another one of the types of regression analysis techniques in machine learning, which is the same as mu"}
{"id": "AML/REGRESSION (1).pptx#p28#c1", "source": "AML/REGRESSION (1).pptx", "page": 28, "snippet": "polynomial regression it is also called the special case of multiple linear regression in ml. because we add some polynomial terms to the multiple lin"}
{"id": "AML/REGRESSION (1).pptx#p29#c1", "source": "AML/REGRESSION (1).pptx", "page": 29, "snippet": "polynomial regression in the above image, we have taken a dataset which is arranged non - linearly. so if we try to cover it with a linear model, then"}
{"id": "AML/REGRESSION (1).pptx#p30#c1", "source": "AML/REGRESSION (1).pptx", "page": 30, "snippet": "polynomial regression equation of the polynomial regression model : simple linear regression equation : y = b0 + b1x......... ( a ) multiple linear re"}
{"id": "AML/REGRESSION (1).pptx#p31#c1", "source": "AML/REGRESSION (1).pptx", "page": 31, "snippet": "regression metrics true values and predicted values : in regression, we ’ ve got two units of values to compare : the actual target values ( authentic"}
{"id": "AML/REGRESSION (1).pptx#p32#c1", "source": "AML/REGRESSION (1).pptx", "page": 32, "snippet": "regression metrics mean absolute error ( mae ) in the fields of statistics and machine learning, the mean absolute error ( mae ) is a frequently emplo"}
{"id": "AML/REGRESSION (1).pptx#p33#c1", "source": "AML/REGRESSION (1).pptx", "page": 33, "snippet": "regression metrics mean squared error ( mse ) a popular metric in statistics and machine learning is the mean squared error ( mse ). it measures the s"}
{"id": "AML/REGRESSION (1).pptx#p34#c1", "source": "AML/REGRESSION (1).pptx", "page": 34, "snippet": "regression metrics r - squared ( r² ) score a statistical metric frequently used to assess the goodness of fit of a regression model is the r - square"}
{"id": "AML/REGRESSION (1).pptx#p35#c1", "source": "AML/REGRESSION (1).pptx", "page": 35, "snippet": "regression metrics root mean squared error rmse stands for root mean squared error. it is a usually used metric in regression analysis and machine lea"}
{"id": "AML/REGRESSION (1).pptx#p36#c1", "source": "AML/REGRESSION (1).pptx", "page": 36, "snippet": "regularization overfitting is one of the most serious kinds of problems related to machine learning. it occurs when a model learns the training data t"}
{"id": "AML/REGRESSION (1).pptx#p37#c1", "source": "AML/REGRESSION (1).pptx", "page": 37, "snippet": "applications of regression financial forecasting ( like house price estimates, or stock prices ) sales and promotions forecasting testing automobiles "}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p1#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 1, "snippet": "you said : advance unsupervised learning techniques : k medoid, with diagram chatgpt said : chatgpt k - medoid clustering : an overview k - medoid is "}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p2#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 2, "snippet": "2. scaling issues : not suitable for very large datasets without optimization techniques. diagram explanation 1. initial medoid selection : medoids ar"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p3#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 3, "snippet": "• select the top kk eigenvectors corresponding to the smallest kk eigenvalues to form an embedding of the data in a lower - dimensional space. 4. clus"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p4#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 4, "snippet": "dbscan clustering chatgpt said : chatgpt dbscan clustering : an overview dbscan ( density - based spatial clustering of applications with noise ) is a"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p5#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 5, "snippet": "advantages 1. can find arbitrarily shaped clusters. 2. robust to noise and outliers. 3. does not require specifying the number of clusters in advance."}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p6#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 6, "snippet": "• computational complexity grows exponentially with the number of dimensions. 2. data sparsity : • high - dimensional data often contains many zero or"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p7#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 7, "snippet": "• subspace clustering : • algorithms like clique and subclu identify clusters in subsets of dimensions, assuming clusters exist in specific subspaces."}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p8#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 8, "snippet": "• agglomerative methods like ward ' s method can be adapted to high - dimensional data with appropriate similarity measures. best practices and recomm"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p9#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 9, "snippet": "• how it works : pca uses eigen decomposition to find orthogonal axes ( principal components ) that capture the maximum variance in the data. • advant"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p10#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 10, "snippet": "• how it works : select features with the most discriminative power using techniques like mutual information, correlation, or recursive feature elimin"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p11#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 11, "snippet": "4. spectral clustering uses graph theory to cluster data. • how it works : • constructs a similarity graph where edges represent pairwise similarity. "}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p12#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 12, "snippet": "• algorithms like cheng and church are popular for biological datasets. • advantages : • ideal for gene expression and text data where row - column re"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p13#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 13, "snippet": "methodology best for challenges dimensionality reduction data with many irrelevant features may lose information in projection feature selection / sub"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p14#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 14, "snippet": "• determines the \" fuzziness \" of the clustering. • higher values of mm make the boundaries between clusters more indistinct, while m = 1m = 1 reduces"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p15#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 15, "snippet": "advantages 1. handles overlapping clusters effectively. 2. suitable for datasets where crisp boundaries are unrealistic. 3. produces more informative "}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p16#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 16, "snippet": "the em algorithm alternates between two steps to iteratively estimate the parameters of a probabilistic model : 1. expectation ( e ) step : • estimate"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p17#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 17, "snippet": "• θθ represents the model parameters. e - step : compute the posterior probabilities for unlabeled data using bayes ’ theorem : p (, θ ) = p (, θ ) p "}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p18#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 18, "snippet": "would you like an implementation example in python or a conceptual diagram explaining the process? you said : outlier analysis : types chatgpt said : "}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p19#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 19, "snippet": "• a sudden drop in stock prices across several companies during a market crash. • abnormal traffic patterns in network data indicating a distributed d"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p20#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 20, "snippet": "• example : • a person who is unusually tall and light - weight, given the typical height - weight correlation. • detection methods : • mahalanobis di"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p21#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 21, "snippet": "a. extreme value outliers : • definition : outliers that lie at the extreme ends of a distribution. • example : • an individual with an annual income "}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p22#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 22, "snippet": "type of outlier examples detection methods spatial outliers anomalous income in a neighborhood spatial statistics, k - nearest neighbors network outli"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p23#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 23, "snippet": "• points with high average distances to their kk - nearest neighbors are marked as outliers. 2. density - based outlier detection • description : dete"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p24#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 24, "snippet": "• evaluates connectivity to detect outliers in network data. cluster evaluation methods cluster evaluation methods measure the quality and validity of"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p25#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 25, "snippet": "3. relative cluster evaluation • compares clustering results obtained by varying parameters or algorithms. • methods : • elbow method : • plots the su"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p26#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 26, "snippet": "you said : ground truth - homogeneity chatgpt said : chatgpt ground truth - homogeneity in clustering homogeneity is a metric used to evaluate the qua"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p27#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 27, "snippet": "interpretation of homogeneity • homogeneity = 1 : • a perfect clustering : each cluster contains only data points from a single class. the clustering "}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p28#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 28, "snippet": "example calculation of homogeneity consider a simple dataset with true labels ( y ) and clustering results ( c ) as follows : data point true label yy"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p29#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 29, "snippet": "completeness in clustering completeness is a metric used to evaluate how well all data points that belong to the same true class are assigned to the s"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p30#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 30, "snippet": "• completeness = 0 : • a poor clustering : points from the same true class are distributed across multiple clusters. this indicates the clustering has"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p31#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 31, "snippet": "completeness < 1 : if, for example, cluster 1 contained both class a and class b points, then the completeness would decrease because not all points f"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p32#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 32, "snippet": "the adjusted rand index ( ari ) is a metric used to evaluate the similarity between two data clusterings. unlike the rand index, which is sensitive to"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p33#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 33, "snippet": "• dd = number of pairs of points in different clusters in both clusterings. • ( n2 ) ( 2n ) = total number of pairs of data points. expected rand inde"}
{"id": "AML/AML Unit 3 (Chatgpt).pdf#p34#c1", "source": "AML/AML Unit 3 (Chatgpt).pdf", "page": 34, "snippet": "• points 5 and 6 are in the same cluster in both ground truth ( c ) and predicted clusters ( cluster 2 ). • agreement ( different clusters ) : • pairs"}
{"id": "AML/MLUnit_1,2,3.pdf#p1#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 1, "snippet": "1 ai vs. ml"}
{"id": "AML/MLUnit_1,2,3.pdf#p2#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 2, "snippet": "2 introduction"}
{"id": "AML/MLUnit_1,2,3.pdf#p3#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 3, "snippet": "to solve a problem on a computer, we need an algorithm. an algorithm is a sequence of instructions that should be carried out to transform the input t"}
{"id": "AML/MLUnit_1,2,3.pdf#p4#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 4, "snippet": "( source : https : / / medium. com / analytics - vidhya / introduction - to - machine - learning - e1b9c055039c ) • machine learning is a “ field of s"}
{"id": "AML/MLUnit_1,2,3.pdf#p5#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 5, "snippet": "• a computer program is said to learn from experience ‘ e ’ with respect to some class of task ‘ t ’ and performance measure ‘ p ’ if its performance "}
{"id": "AML/MLUnit_1,2,3.pdf#p6#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 6, "snippet": "example 1 classify email as spam or not spam • task ( t ) : classify email as spam or not spam • experience ( e ) : watching the user to mark / label "}
{"id": "AML/MLUnit_1,2,3.pdf#p7#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 7, "snippet": "cntd.. example 2 recognizing hand written digits / characters • task ( t ) : recognizing hand written digit • experience ( e ) : watching the user to "}
{"id": "AML/MLUnit_1,2,3.pdf#p8#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 8, "snippet": "why machine learning important?. • human expertise does not exist navigating on mars industrial / manufacturing control mass spectrometer analysis, dr"}
{"id": "AML/MLUnit_1,2,3.pdf#p9#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 9, "snippet": "cntd.. • the amount of knowledge available about certain tasks might be too large for explicit encoding by humans ( e. g., medical diagnostic ). • new"}
{"id": "AML/MLUnit_1,2,3.pdf#p10#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 10, "snippet": "10 how does machine learning help us in daily life? social networking : • use of the appropriate emotions, suggestions about friend tags on facebook, "}
{"id": "AML/MLUnit_1,2,3.pdf#p11#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 11, "snippet": "• face detection • stock prediction • spam email detection • machine translation • self - parking cars • airplane navigation systems • medicine • data"}
{"id": "AML/MLUnit_1,2,3.pdf#p12#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 12, "snippet": "examples … example 1 : hand - written digit recognition : output learn a classifier f ( x ) such that, f : x → { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 } input "}
{"id": "AML/MLUnit_1,2,3.pdf#p13#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 13, "snippet": "example 2 : face detection input : an image, the classes are people to be recognized … [ non - face, frontal - face, profile - face ] and the learning"}
{"id": "AML/MLUnit_1,2,3.pdf#p14#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 14, "snippet": "example 3 : spam detection • this is a classification problem • task is to classify email into spam / non - spam • requires a learning system as “ ene"}
{"id": "AML/MLUnit_1,2,3.pdf#p15#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 15, "snippet": "example 4 : stock price prediction • task is to predict stock price at future date • this is a regression task, as the output is continuous 15"}
{"id": "AML/MLUnit_1,2,3.pdf#p16#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 16, "snippet": "example 5 : computational biology 16"}
{"id": "AML/MLUnit_1,2,3.pdf#p17#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 17, "snippet": "17"}
{"id": "AML/MLUnit_1,2,3.pdf#p18#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 18, "snippet": "18"}
{"id": "AML/MLUnit_1,2,3.pdf#p19#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 19, "snippet": "example : weather prediction 19"}
{"id": "AML/MLUnit_1,2,3.pdf#p20#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 20, "snippet": "inputs are the relevant information about the patient and the classes are the illnesses. the inputs contain the patient ’ s age, gender, past medical "}
{"id": "AML/MLUnit_1,2,3.pdf#p21#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 21, "snippet": "a crop yield prediction app in senegal using satellite imagery ( video link ) https : / / www. youtube. com / watch? v = 4onbgkha4jc & t = 160s. examp"}
{"id": "AML/MLUnit_1,2,3.pdf#p22#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 22, "snippet": "data preparation data preparation pipeline 22"}
{"id": "AML/MLUnit_1,2,3.pdf#p23#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 23, "snippet": "data preparation 23"}
{"id": "AML/MLUnit_1,2,3.pdf#p24#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 24, "snippet": "data preparation 24"}
{"id": "AML/MLUnit_1,2,3.pdf#p25#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 25, "snippet": "why is data preparation important? sometimes, data in data sets have missing or incomplete information, which leads to less accurate or incorrect pred"}
{"id": "AML/MLUnit_1,2,3.pdf#p26#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 26, "snippet": "steps in data preparation process 1. understand the problem : understand the actual problem and try to solve it. 2. data collection : collect data fro"}
{"id": "AML/MLUnit_1,2,3.pdf#p27#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 27, "snippet": "steps in data preparation process 4. data cleaning and validation : data cleaning and validation techniques help determine and solve inconsistencies, "}
{"id": "AML/MLUnit_1,2,3.pdf#p28#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 28, "snippet": "steps in data preparation process 6. feature engineering and selection : • feature engineering is defined as the study of selecting, manipulating, and"}
{"id": "AML/MLUnit_1,2,3.pdf#p29#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 29, "snippet": "data pre - processing data preprocessing 1. data cleaning 2. data integration 3. data transformation 4. data reduction 5. data discretization 29"}
{"id": "AML/MLUnit_1,2,3.pdf#p30#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 30, "snippet": "cntd.. • data preparation is also known as data \" pre - processing, \" \" data wrangling, \" \" data cleaning, \" \" data pre - processing, \" and \" feature "}
{"id": "AML/MLUnit_1,2,3.pdf#p31#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 31, "snippet": "cntd.. 31"}
{"id": "AML/MLUnit_1,2,3.pdf#p32#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 32, "snippet": "cntd.. 32"}
{"id": "AML/MLUnit_1,2,3.pdf#p33#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 33, "snippet": "cntd.. 33"}
{"id": "AML/MLUnit_1,2,3.pdf#p34#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 34, "snippet": "cntd.. 34"}
{"id": "AML/MLUnit_1,2,3.pdf#p35#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 35, "snippet": "cntd.. 35"}
{"id": "AML/MLUnit_1,2,3.pdf#p36#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 36, "snippet": "cntd.. 2. outliers or anomalies : unexpected values • ml algorithms are sensitive to the range and distribution of values when data comes from unknown"}
{"id": "AML/MLUnit_1,2,3.pdf#p37#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 37, "snippet": "cntd.. 3. unstructured data format : • data comes from various sources and needs to be extracted into a different format. • hence, before deploying an"}
{"id": "AML/MLUnit_1,2,3.pdf#p38#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 38, "snippet": "cntd.. 38"}
{"id": "AML/MLUnit_1,2,3.pdf#p39#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 39, "snippet": "cntd.. 39"}
{"id": "AML/MLUnit_1,2,3.pdf#p40#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 40, "snippet": "cntd.. 40"}
{"id": "AML/MLUnit_1,2,3.pdf#p41#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 41, "snippet": "cntd.. 41"}
{"id": "AML/MLUnit_1,2,3.pdf#p42#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 42, "snippet": "cntd.. 42"}
{"id": "AML/MLUnit_1,2,3.pdf#p43#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 43, "snippet": "cntd.. 43"}
{"id": "AML/MLUnit_1,2,3.pdf#p44#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 44, "snippet": "cntd.. 44"}
{"id": "AML/MLUnit_1,2,3.pdf#p45#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 45, "snippet": "feature engineering 45 feature engineering is the pre - processing step of machine learning, which is used to transform raw data into features that ca"}
{"id": "AML/MLUnit_1,2,3.pdf#p46#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 46, "snippet": "feature engineering 46 what is a feature? • generally, all machine learning algorithms take input data to generate the output. • the input data remain"}
{"id": "AML/MLUnit_1,2,3.pdf#p47#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 47, "snippet": "feature engineering 47 3. feature extraction : feature extraction is an automated feature engineering process that generates new variables by extracti"}
{"id": "AML/MLUnit_1,2,3.pdf#p48#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 48, "snippet": "feature engineering 48 steps in feature engineering data preparation : • in this step, raw data acquired from different resources are prepared to make"}
{"id": "AML/MLUnit_1,2,3.pdf#p49#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 49, "snippet": "feature engineering 49 feature engineering techniques : 1. imputation : imputation is responsible for handling irregularities within the dataset. • fo"}
{"id": "AML/MLUnit_1,2,3.pdf#p50#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 50, "snippet": "feature engineering 50 feature engineering techniques : 3. log transform : log transform helps in handling the skewed data, and it makes the distribut"}
{"id": "AML/MLUnit_1,2,3.pdf#p51#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 51, "snippet": "• supervised learning types of learning 51"}
{"id": "AML/MLUnit_1,2,3.pdf#p52#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 52, "snippet": "types of learning supervised learning : aim is to learn a mapping from the input to an output whose correct values are provided by a supervisor. 1. cl"}
{"id": "AML/MLUnit_1,2,3.pdf#p53#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 53, "snippet": "types of learning 53"}
{"id": "AML/MLUnit_1,2,3.pdf#p54#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 54, "snippet": "types of learning 54"}
{"id": "AML/MLUnit_1,2,3.pdf#p55#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 55, "snippet": "types of learning supervised learning : 55"}
{"id": "AML/MLUnit_1,2,3.pdf#p56#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 56, "snippet": "types of learning supervised learning : 56"}
{"id": "AML/MLUnit_1,2,3.pdf#p57#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 57, "snippet": "• unsupervised learning unsupervised learning 57"}
{"id": "AML/MLUnit_1,2,3.pdf#p58#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 58, "snippet": "• clustering • association 58 example of unsupervised learning"}
{"id": "AML/MLUnit_1,2,3.pdf#p59#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 59, "snippet": "• clustering • association 59 example of unsupervised learning"}
{"id": "AML/MLUnit_1,2,3.pdf#p60#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 60, "snippet": "60 example of unsupervised learning"}
{"id": "AML/MLUnit_1,2,3.pdf#p61#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 61, "snippet": "61 example of semi - supervised learning"}
{"id": "AML/MLUnit_1,2,3.pdf#p62#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 62, "snippet": "reinforcement learning • learning from mistakes • place a reinforcement learning algorithm into any environment and it will make a lot of mistakes in "}
{"id": "AML/MLUnit_1,2,3.pdf#p63#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 63, "snippet": "reinforcement learning 63"}
{"id": "AML/MLUnit_1,2,3.pdf#p64#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 64, "snippet": "where is reinforcement learning in the real world? • video games • industrial simulation : • resource management reinforcement learning 64"}
{"id": "AML/MLUnit_1,2,3.pdf#p65#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 65, "snippet": "65"}
{"id": "AML/MLUnit_1,2,3.pdf#p66#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 66, "snippet": "key elements of machine learning • there are tens of thousands of machine learning algorithms and hundreds of new algorithms are developed every year."}
{"id": "AML/MLUnit_1,2,3.pdf#p67#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 67, "snippet": "3. optimization : the way candidate programs are generated known as the search process. for example combinatorial optimization, convex optimization, c"}
{"id": "AML/MLUnit_1,2,3.pdf#p68#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 68, "snippet": "aspects of developing a learning system : training data, concept representation, function approximation • for training and testing purpose of our mode"}
{"id": "AML/MLUnit_1,2,3.pdf#p69#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 69, "snippet": "validation set • validation set is the set of data separate from the training data • it is used to validate our model during training • it gives infor"}
{"id": "AML/MLUnit_1,2,3.pdf#p70#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 70, "snippet": "test set • a set of data use to test the model • the test set is separated from both the train set and validation set • once the model is train and va"}
{"id": "AML/MLUnit_1,2,3.pdf#p71#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 71, "snippet": "data split • rules for performing data split operation • in order to avoid a correlation between the original dataset must be randomly shuffled before"}
{"id": "AML/MLUnit_1,2,3.pdf#p72#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 72, "snippet": "exploratory data analysis refers to the critical process of performing initial investigations on data so as to discover patterns, to spot anomalies, t"}
{"id": "AML/MLUnit_1,2,3.pdf#p73#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 73, "snippet": "typical graphical techniques used in eda are : • box plot • histogram • multi - vari chart • run chart • pareto chart • scatter plot • stem - and - le"}
{"id": "AML/MLUnit_1,2,3.pdf#p74#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 74, "snippet": "eda example • wine quality data set from uci ml repository • imported necessary libraries ( for this example pandas, numpy, matplotlib and seaborn ) a"}
{"id": "AML/MLUnit_1,2,3.pdf#p75#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 75, "snippet": "eda • original data is separated by delimiter “ ; “ in given data set. • to take a closer look at the data took help of “. head ( ) ” function of pand"}
{"id": "AML/MLUnit_1,2,3.pdf#p76#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 76, "snippet": "eda techniques • found out the total number of rows and columns in the data set using “. shape ” • dataset comprises of 4898 observations and 12 chara"}
{"id": "AML/MLUnit_1,2,3.pdf#p77#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 77, "snippet": "eda : exploratory data analysis 77 plotting using matplotlib import pandas as pd import matplotlib. pyplot as plt iris = pd. read _ csv ( \" iris. csv "}
{"id": "AML/MLUnit_1,2,3.pdf#p78#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 78, "snippet": "78"}
{"id": "AML/MLUnit_1,2,3.pdf#p79#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 79, "snippet": "eda : exploratory data analysis 79 3d scatter plot https : / / plot. ly / pandas / 3d - scatter - plots / import plotly import plotly. express as px i"}
{"id": "AML/MLUnit_1,2,3.pdf#p80#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 80, "snippet": "80"}
{"id": "AML/MLUnit_1,2,3.pdf#p81#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 81, "snippet": "81"}
{"id": "AML/MLUnit_1,2,3.pdf#p82#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 82, "snippet": "82"}
{"id": "AML/MLUnit_1,2,3.pdf#p83#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 83, "snippet": "progressive data analysis 83 • data has only float and integer values. • no variable column has null / missing values."}
{"id": "AML/MLUnit_1,2,3.pdf#p84#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 84, "snippet": "pda techniques • the describe ( ) function in pandas is very handy in getting various summary statistics. • this function returns the count, mean, sta"}
{"id": "AML/MLUnit_1,2,3.pdf#p85#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 85, "snippet": "data preparation : types of data • here as you can notice mean value is larger than median value of each column which is represented by 50 % ( 50th pe"}
{"id": "AML/MLUnit_1,2,3.pdf#p86#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 86, "snippet": "graph visualisation techniques • let ’ s now explore data with beautiful graphs. python has a visualization library, seaborn which build on top of mat"}
{"id": "AML/MLUnit_1,2,3.pdf#p87#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 87, "snippet": "87"}
{"id": "AML/MLUnit_1,2,3.pdf#p88#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 88, "snippet": "data pre - processing techniques for ml applications • dark shades represents positive correlation while lighter shades represents negative correlatio"}
{"id": "AML/MLUnit_1,2,3.pdf#p89#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 89, "snippet": "89"}
{"id": "AML/MLUnit_1,2,3.pdf#p90#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 90, "snippet": "box plot • a box plot ( or box - and - whisker plot ) shows the distribution of quantitative data in a way that facilitates comparisons between variab"}
{"id": "AML/MLUnit_1,2,3.pdf#p91#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 91, "snippet": "• minimum • first quartile • median • third quartile • maximum. • in the simplest box plot the central rectangle spans the first quartile to the third"}
{"id": "AML/MLUnit_1,2,3.pdf#p92#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 92, "snippet": "92"}
{"id": "AML/MLUnit_1,2,3.pdf#p93#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 93, "snippet": "sparse matrix • in numerical analysis and scientific computing, a sparse matrix or sparse array is a matrix in which most of the elements are zero. • "}
{"id": "AML/MLUnit_1,2,3.pdf#p94#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 94, "snippet": "feature engineering : feature selection • in machine learning and statistics, feature selection, also known as variable selection, attribute selection"}
{"id": "AML/MLUnit_1,2,3.pdf#p95#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 95, "snippet": "feature engineering : feature selection feature selection is primarily focused on removing non - informative or redundant predictors from the model. f"}
{"id": "AML/MLUnit_1,2,3.pdf#p96#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 96, "snippet": "feature engineering : feature selection • there are two main types of feature selection techniques : supervised and unsupervised • supervised methods "}
{"id": "AML/MLUnit_1,2,3.pdf#p97#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 97, "snippet": "feature engineering : feature selection • unsupervised feature selection techniques ignores the target variable, such as methods that remove redundant"}
{"id": "AML/MLUnit_1,2,3.pdf#p98#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 98, "snippet": "feature engineering : feature selection filter : • statistical - based feature selection methods involve evaluating the relationship between each inpu"}
{"id": "AML/MLUnit_1,2,3.pdf#p99#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 99, "snippet": "feature engineering : feature selection wrapper : • wrapper feature selection methods create many models with different subsets of input features and "}
{"id": "AML/MLUnit_1,2,3.pdf#p100#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 100, "snippet": "feature engineering : feature selection statistics for filter - based feature selection methods it is common to use correlation type statistical measu"}
{"id": "AML/MLUnit_1,2,3.pdf#p101#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 101, "snippet": "feature engineering : feature selection filter - based feature selection method. • consider two broad categories of variable types : numerical and cat"}
{"id": "AML/MLUnit_1,2,3.pdf#p102#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 102, "snippet": "feature engineering : feature selection 102"}
{"id": "AML/MLUnit_1,2,3.pdf#p103#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 103, "snippet": "feature engineering : feature selection 1. numerical input, numerical output • this is a regression predictive modeling problem with numerical input v"}
{"id": "AML/MLUnit_1,2,3.pdf#p104#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 104, "snippet": "feature engineering : feature selection 3. categorical input, numerical output • this is a regression predictive modeling problem with categorical inp"}
{"id": "AML/MLUnit_1,2,3.pdf#p105#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 105, "snippet": "feature engineering : feature selection recursive feature elimination • recursive feature elimination, or rfe for short, is a feature selection algori"}
{"id": "AML/MLUnit_1,2,3.pdf#p106#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 106, "snippet": "feature engineering : feature selection • rfe is a wrapper - type feature selection algorithm. • this means that a different machine learning algorith"}
{"id": "AML/MLUnit_1,2,3.pdf#p107#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 107, "snippet": "• we can see / visualize 2d, 3d data ….. by scatterplot • 4d, 5d, 6d … … ….. use pair plot ….. ( nc2 pairs ) • 10d, 100d, 1000d data? • visualization "}
{"id": "AML/MLUnit_1,2,3.pdf#p108#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 108, "snippet": "why pca? • for dimensionality reduction i. e. d - dim d ’ - dim e. g. mnist dataset of 784 - dim to 2 dim # mnist dataset downloaded from kaggle : # h"}
{"id": "AML/MLUnit_1,2,3.pdf#p109#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 109, "snippet": "pca steps for dimensionality reduction : 1. column standardization of data 2. find covariance matrix 3. find eigen values and eigen vectors 4. find pr"}
{"id": "AML/MLUnit_1,2,3.pdf#p110#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 110, "snippet": "1. standardization of the data • missing out on standardization will probably result in a biased outcome. • standardization is all about scaling your "}
{"id": "AML/MLUnit_1,2,3.pdf#p111#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 111, "snippet": "2 computing the covariance matrix • a covariance matrix expresses the correlation between the different variables in the data set. • it is essential t"}
{"id": "AML/MLUnit_1,2,3.pdf#p112#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 112, "snippet": "3. calculating the eigenvectors and eigenvalues eigenvectors and eigenvalues are computed from the covariance matrix in order to determine the princip"}
{"id": "AML/MLUnit_1,2,3.pdf#p113#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 113, "snippet": "4. computing the principal components • eigenvectors and eigenvalues placed in the descending order • where the eigenvector with the highest eigenvalu"}
{"id": "AML/MLUnit_1,2,3.pdf#p114#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 114, "snippet": "5. reducing the dimensions of the data set • performing pca is to re - arrange the original data with the final principal components which represent t"}
{"id": "AML/MLUnit_1,2,3.pdf#p115#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 115, "snippet": "t - sne https : / / distill. pub / 2016 / misread - tsne / https : / / colah. github. io / posts / 2014 - 10 - visualizing - mnist / t - sne is t - di"}
{"id": "AML/MLUnit_1,2,3.pdf#p116#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 116, "snippet": "t - sne. neighborhood and embedding • points are geometrically together … …. neighborhood • embedding …. for every points in high - dim space finding "}
{"id": "AML/MLUnit_1,2,3.pdf#p117#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 117, "snippet": "t - sne crowding problem : e. g … 2dim to 1 dim sometimes it is impossible to preserve distance in all the neighborhood points such problem is called "}
{"id": "AML/MLUnit_1,2,3.pdf#p118#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 118, "snippet": "t - sne https : / / distill. pub / 2016 / misread - tsne / • run t - sne on simple dataset • perplexity : points in neighbors • epsilion : learning ra"}
{"id": "AML/MLUnit_1,2,3.pdf#p119#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 119, "snippet": "119"}
{"id": "AML/MLUnit_1,2,3.pdf#p120#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 120, "snippet": "120"}
{"id": "AML/MLUnit_1,2,3.pdf#p121#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 121, "snippet": "121"}
{"id": "AML/MLUnit_1,2,3.pdf#p122#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 122, "snippet": "ml - unit ii school of computer engineering & technology"}
{"id": "AML/MLUnit_1,2,3.pdf#p123#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 123, "snippet": "supervised learning techniques : classification"}
{"id": "AML/MLUnit_1,2,3.pdf#p124#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 124, "snippet": "3 supervised vs. unsupervised learning • supervised learning ( classification ) • supervision : the training data ( observations, measurements, etc. )"}
{"id": "AML/MLUnit_1,2,3.pdf#p125#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 125, "snippet": "4 • classification • predicts categorical class labels ( discrete or nominal ) • classifies data ( constructs a model ) based on the training set and "}
{"id": "AML/MLUnit_1,2,3.pdf#p126#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 126, "snippet": "5 classification — a two - step process • model construction : describing a set of predetermined classes • each tuple / sample is assumed to belong to"}
{"id": "AML/MLUnit_1,2,3.pdf#p127#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 127, "snippet": "6 process ( 1 ) : model construction training data classification algorithms if rank = ‘ professor ’ or years > 6 then tenured = ‘ yes ’ classifier ( "}
{"id": "AML/MLUnit_1,2,3.pdf#p128#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 128, "snippet": "7 process ( 2 ) : using the model in prediction classifier testing data unseen data ( jeff, professor, 4 ) tenured?"}
{"id": "AML/MLUnit_1,2,3.pdf#p129#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 129, "snippet": "8 classification : basic concepts • classification : basic concepts • decision tree induction"}
{"id": "AML/MLUnit_1,2,3.pdf#p130#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 130, "snippet": "9 decision tree induction : an example age? overcast student? credit rating? < = 30 > 40 no yes yes yes 31.. 40 fair excellent yes no training data se"}
{"id": "AML/MLUnit_1,2,3.pdf#p131#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 131, "snippet": "10 algorithm for decision tree induction • basic algorithm ( a greedy algorithm ) • tree is constructed in a top - down recursive divide - and - conqu"}
{"id": "AML/MLUnit_1,2,3.pdf#p132#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 132, "snippet": "brief review of entropy • 11 m = 2"}
{"id": "AML/MLUnit_1,2,3.pdf#p133#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 133, "snippet": "12 attribute selection measure : information gain ( id3 / c4. 5 ) ■ select the attribute with the highest information gain ■ let pi be the probability"}
{"id": "AML/MLUnit_1,2,3.pdf#p134#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 134, "snippet": "attribute selection : information gain class p : buys _ computer = “ yes ” class n : buys _ computer = “ no ” gain ( age ) > any other gain. so age is"}
{"id": "AML/MLUnit_1,2,3.pdf#p135#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 135, "snippet": "14 attribute selection : information gain g class p : buys _ computer = “ yes ” g class n : buys _ computer = “ no ” means “ age < = 30 ” has 5 out of"}
{"id": "AML/MLUnit_1,2,3.pdf#p136#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 136, "snippet": "intermediate dt of the buys _ computer dataset age is the splitting attribute at the root node of dt. repeat the procedure to determine the splitting "}
{"id": "AML/MLUnit_1,2,3.pdf#p137#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 137, "snippet": "final dt of the buys _ computer dataset"}
{"id": "AML/MLUnit_1,2,3.pdf#p138#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 138, "snippet": "example : dt creation"}
{"id": "AML/MLUnit_1,2,3.pdf#p139#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 139, "snippet": "example : dt creation"}
{"id": "AML/MLUnit_1,2,3.pdf#p140#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 140, "snippet": "example : dt creation"}
{"id": "AML/MLUnit_1,2,3.pdf#p141#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 141, "snippet": "example : usage of information gain and entropy in dt creation"}
{"id": "AML/MLUnit_1,2,3.pdf#p142#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 142, "snippet": "21 computing information - gain for continuous - valued attributes • let attribute a be a continuous - valued attribute • must determine the best spli"}
{"id": "AML/MLUnit_1,2,3.pdf#p143#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 143, "snippet": "decision tree ▪a classifier ( tree structure ) : used in classification and regression ▪classification mostly uses decision tree ▪decision tree model "}
{"id": "AML/MLUnit_1,2,3.pdf#p144#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 144, "snippet": "classification & regression trees ( cart ) ▪ dt creates a model that predicts the value of a target ( or dependent variable ) based on the values of s"}
{"id": "AML/MLUnit_1,2,3.pdf#p145#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 145, "snippet": "24 gain ratio for attribute selection ( c4. 5 ) • information gain measure is biased towards attributes with a large number of values • c4. 5 ( a succ"}
{"id": "AML/MLUnit_1,2,3.pdf#p146#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 146, "snippet": "25"}
{"id": "AML/MLUnit_1,2,3.pdf#p147#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 147, "snippet": "26 gini index ( cart, ibm intelligentminer ) • if a data set d contains examples from n classes, gini index, gini ( d ) is defined as where pj is the "}
{"id": "AML/MLUnit_1,2,3.pdf#p148#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 148, "snippet": "27 computation of gini index • ex. d has 9 tuples in buys _ computer = “ yes ” and 5 in “ no ” • suppose the attribute income partitions d into 10 in "}
{"id": "AML/MLUnit_1,2,3.pdf#p149#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 149, "snippet": "28"}
{"id": "AML/MLUnit_1,2,3.pdf#p150#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 150, "snippet": "29"}
{"id": "AML/MLUnit_1,2,3.pdf#p151#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 151, "snippet": "30"}
{"id": "AML/MLUnit_1,2,3.pdf#p152#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 152, "snippet": "31"}
{"id": "AML/MLUnit_1,2,3.pdf#p153#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 153, "snippet": "32"}
{"id": "AML/MLUnit_1,2,3.pdf#p154#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 154, "snippet": "33"}
{"id": "AML/MLUnit_1,2,3.pdf#p155#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 155, "snippet": "34 comparing attribute selection measures • the three measures, in general, return good results but • information gain : • biased towards multivalued "}
{"id": "AML/MLUnit_1,2,3.pdf#p156#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 156, "snippet": "35 overfitting and tree pruning • overfitting : an induced tree may overfit the training data • too many branches, some may reflect anomalies due to n"}
{"id": "AML/MLUnit_1,2,3.pdf#p157#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 157, "snippet": "36 enhancements to basic decision tree induction • allow for continuous - valued attributes • dynamically define new discrete - valued attributes that"}
{"id": "AML/MLUnit_1,2,3.pdf#p158#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 158, "snippet": "37 classification in large databases • classification — a classical problem extensively studied by statisticians and machine learning researchers • sc"}
{"id": "AML/MLUnit_1,2,3.pdf#p159#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 159, "snippet": "support vector machines ( svm ) unit ii"}
{"id": "AML/MLUnit_1,2,3.pdf#p160#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 160, "snippet": "39 svm — support vector machines • a relatively new classification method for both linear and nonlinear data • it uses a nonlinear mapping to transfor"}
{"id": "AML/MLUnit_1,2,3.pdf#p161#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 161, "snippet": "svm • “ support vector machine ” ( svm ) is a supervised machine learning algorithm which can be used for both classification or regression challenges"}
{"id": "AML/MLUnit_1,2,3.pdf#p162#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 162, "snippet": "how does it work? a thumb rule to identify the right hyper - plane : “ select the hyper - plane which segregates the two classes better ”. in this sce"}
{"id": "AML/MLUnit_1,2,3.pdf#p163#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 163, "snippet": "how does it work? maximizing the distances between nearest data point ( either class ) and hyper - plane will help us to decide the right hyper - plan"}
{"id": "AML/MLUnit_1,2,3.pdf#p164#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 164, "snippet": "margin in svm margin for hyper - plane c is high as compared to both a and b. hence, we name the right hyper - plane as c. if we select a hyper - plan"}
{"id": "AML/MLUnit_1,2,3.pdf#p165#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 165, "snippet": "how does it work? hyper - plane b as it has higher margin compared to a. but, here is the catch, svm selects the hyper - plane which classifies the cl"}
{"id": "AML/MLUnit_1,2,3.pdf#p166#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 166, "snippet": "how does it work? the svm algorithm has a feature to ignore outliers and find the hyper - plane that has the maximum margin. hence, we can say, svm cl"}
{"id": "AML/MLUnit_1,2,3.pdf#p167#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 167, "snippet": "how does it work? svm can solve this problem. easily! it solves this problem by introducing additional feature. here, we will add a new feature z = x "}
{"id": "AML/MLUnit_1,2,3.pdf#p168#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 168, "snippet": "kernel trick • the svm kernel is a function that takes low dimensional input space and transforms it to a higher dimensional space i. e. it converts n"}
{"id": "AML/MLUnit_1,2,3.pdf#p169#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 169, "snippet": "different kernel functions"}
{"id": "AML/MLUnit_1,2,3.pdf#p170#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 170, "snippet": "49 svm — linearly separable ■ a separating hyperplane can be written as w ● x + b = 0 where w = { w1, w2, …, wn } is a weight vector and b a scalar ( "}
{"id": "AML/MLUnit_1,2,3.pdf#p171#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 171, "snippet": "50 why is svm effective on high dimensional data? ■ the complexity of trained classifier is characterized by the # of support vectors rather than the "}
{"id": "AML/MLUnit_1,2,3.pdf#p172#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 172, "snippet": "51 svm — linearly inseparable ■ transform the original input data into a higher dimensional space ■ search for a linear separating hyperplane in the n"}
{"id": "AML/MLUnit_1,2,3.pdf#p173#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 173, "snippet": "52 svm : different kernel functions ■ instead of computing the dot product on the transformed data, it is math. equivalent to applying a kernel functi"}
{"id": "AML/MLUnit_1,2,3.pdf#p174#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 174, "snippet": "pros and cons associated with svm * pros : it works really well with a clear margin of separation it is effective in high dimensional spaces. it is ef"}
{"id": "AML/MLUnit_1,2,3.pdf#p175#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 175, "snippet": "svm in python"}
{"id": "AML/MLUnit_1,2,3.pdf#p176#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 176, "snippet": "exercise • https : / / www. youtube. com / watch? v = lxgayvxkgtg & t = 189s"}
{"id": "AML/MLUnit_1,2,3.pdf#p177#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 177, "snippet": "svm"}
{"id": "AML/MLUnit_1,2,3.pdf#p178#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 178, "snippet": "svm"}
{"id": "AML/MLUnit_1,2,3.pdf#p179#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 179, "snippet": "scholastic home. video. tutor @, = @ 2 = - 3. 25 and a3 = 3. 5 @ email. com * the hyper plane that discriminates the positive class from the negative "}
{"id": "AML/MLUnit_1,2,3.pdf#p180#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 180, "snippet": "our vectors are augmented with a bias. hence we can equate the entry in w as the hyper plane with an offset b. therefore the separating hyper plane eq"}
{"id": "AML/MLUnit_1,2,3.pdf#p181#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 181, "snippet": "y = wx + bwithw = ( g ) and offset b = — 3."}
{"id": "AML/MLUnit_1,2,3.pdf#p182#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 182, "snippet": "bayesian classification ▪ a statistical classifier : performs probabilistic prediction, i. e., predicts class membership probabilities ▪ foundation : "}
{"id": "AML/MLUnit_1,2,3.pdf#p183#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 183, "snippet": "bayesian classification - example"}
{"id": "AML/MLUnit_1,2,3.pdf#p184#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 184, "snippet": "▪ probability : how likely something is to happen ▪ probability of an event happening = number of times it can happen _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ "}
{"id": "AML/MLUnit_1,2,3.pdf#p185#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 185, "snippet": "▪ let x be a data sample ( “ evidence ” ) : class label is unknown ▪ let h be a hypothesis that x belongs to class c ▪ classification is to determine "}
{"id": "AML/MLUnit_1,2,3.pdf#p186#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 186, "snippet": "▪ given training data x, posteriori probability of a hypothesis h, p ( h | x ), follows the bayes theorem ▪ ▪ predicts x belongs to ci iff the probabi"}
{"id": "AML/MLUnit_1,2,3.pdf#p187#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 187, "snippet": "▪ let d be a training set of tuples and their associated class labels, and each tuple is represented by an n - d attribute vector x = ( x1, x2, …, xn "}
{"id": "AML/MLUnit_1,2,3.pdf#p188#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 188, "snippet": "class : c1 : buys _ computer = ‘ yes ’ c2 : buys _ computer = ‘ no ’ data sample x = ( age = youth, income = medium, student = yes credit _ rating = f"}
{"id": "AML/MLUnit_1,2,3.pdf#p189#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 189, "snippet": "test for x = ( age = youth, income = medium, student = yes, credit _ rating = fair ) • prior probability p ( ci ) : p ( buys _ computer = yes ) = 9 / "}
{"id": "AML/MLUnit_1,2,3.pdf#p190#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 190, "snippet": "comment on naive bayes classification ▪ advantages ▪ easy to implement ▪ good results obtained in most of the cases ▪ disadvantages ▪ assumption : cla"}
{"id": "AML/MLUnit_1,2,3.pdf#p191#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 191, "snippet": "70 lazy vs. eager learning • lazy vs. eager learning • lazy learning ( e. g., instance - based learning ) : simply stores training data ( or only mino"}
{"id": "AML/MLUnit_1,2,3.pdf#p192#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 192, "snippet": "71 lazy learner : instance - based methods • instance - based learning : • store training examples and delay the processing ( “ lazy evaluation ” ) un"}
{"id": "AML/MLUnit_1,2,3.pdf#p193#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 193, "snippet": "72"}
{"id": "AML/MLUnit_1,2,3.pdf#p194#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 194, "snippet": "73"}
{"id": "AML/MLUnit_1,2,3.pdf#p195#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 195, "snippet": "74"}
{"id": "AML/MLUnit_1,2,3.pdf#p196#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 196, "snippet": "75"}
{"id": "AML/MLUnit_1,2,3.pdf#p197#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 197, "snippet": "76"}
{"id": "AML/MLUnit_1,2,3.pdf#p198#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 198, "snippet": "77"}
{"id": "AML/MLUnit_1,2,3.pdf#p199#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 199, "snippet": "78"}
{"id": "AML/MLUnit_1,2,3.pdf#p200#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 200, "snippet": "79"}
{"id": "AML/MLUnit_1,2,3.pdf#p201#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 201, "snippet": "80"}
{"id": "AML/MLUnit_1,2,3.pdf#p202#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 202, "snippet": "81"}
{"id": "AML/MLUnit_1,2,3.pdf#p203#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 203, "snippet": "82"}
{"id": "AML/MLUnit_1,2,3.pdf#p204#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 204, "snippet": "83"}
{"id": "AML/MLUnit_1,2,3.pdf#p205#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 205, "snippet": "84"}
{"id": "AML/MLUnit_1,2,3.pdf#p206#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 206, "snippet": "85"}
{"id": "AML/MLUnit_1,2,3.pdf#p207#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 207, "snippet": "86"}
{"id": "AML/MLUnit_1,2,3.pdf#p208#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 208, "snippet": "87"}
{"id": "AML/MLUnit_1,2,3.pdf#p209#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 209, "snippet": "88"}
{"id": "AML/MLUnit_1,2,3.pdf#p210#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 210, "snippet": "89"}
{"id": "AML/MLUnit_1,2,3.pdf#p211#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 211, "snippet": "90"}
{"id": "AML/MLUnit_1,2,3.pdf#p212#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 212, "snippet": "91 the k - nearest neighbor algorithm • all instances correspond to points in the n - d space • the nearest neighbor are defined in terms of euclidean"}
{"id": "AML/MLUnit_1,2,3.pdf#p213#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 213, "snippet": "92 discussion on the k - nn algorithm • k - nn for real - valued prediction for a given unknown tuple • returns the mean values of the k nearest neigh"}
{"id": "AML/MLUnit_1,2,3.pdf#p214#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 214, "snippet": "93 https : / / www. kaggle. com / code / skalskip / iris - data - visualization - and - knn - classification"}
{"id": "AML/MLUnit_1,2,3.pdf#p215#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 215, "snippet": "feature scaling • feature scaling is the method to limit the range of variables so that they can be compared on common grounds."}
{"id": "AML/MLUnit_1,2,3.pdf#p216#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 216, "snippet": "methods { exit } rescaling ( min - max normalization ) [ edit } also known as min - max scaling or min - max normalisation, is the simplest method and"}
{"id": "AML/MLUnit_1,2,3.pdf#p217#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 217, "snippet": "in [ 23 ] : from sklearn. preprocessing import standardscaler standard _ x = standardscaler ( ) in [ 24 ] : x _ train = standard _ x. fit _ transform "}
{"id": "AML/MLUnit_1,2,3.pdf#p218#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 218, "snippet": "model evaluation and selection • evaluation metrics : how can we measure accuracy? other metrics to consider? • use validation test set of class - lab"}
{"id": "AML/MLUnit_1,2,3.pdf#p219#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 219, "snippet": "evaluating classifier accuracy : holdout & cross - validation methods • holdout method • given data is randomly partitioned into two independent sets "}
{"id": "AML/MLUnit_1,2,3.pdf#p220#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 220, "snippet": "evaluating classifier accuracy : bootstrap • bootstrap • works well with small data sets • samples the given training tuples uniformly with replacemen"}
{"id": "AML/MLUnit_1,2,3.pdf#p221#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 221, "snippet": "estimating confidence intervals : classifier models m1 vs. m2 • suppose we have 2 classifiers, m1 and m2, which one is better? • use 10 - fold cross -"}
{"id": "AML/MLUnit_1,2,3.pdf#p222#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 222, "snippet": "estimating confidence intervals : null hypothesis • perform 10 - fold cross - validation • assume samples follow a t distribution with k – 1 degrees o"}
{"id": "AML/MLUnit_1,2,3.pdf#p223#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 223, "snippet": "estimating confidence intervals : t - test • if only 1 test set available : pairwise comparison • for ith round of 10 - fold cross - validation, the s"}
{"id": "AML/MLUnit_1,2,3.pdf#p224#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 224, "snippet": "estimating confidence intervals : table for t - distribution • symmetric • significance level, e. g., sig = 0. 05 or 5 % means m1 & m2 are significant"}
{"id": "AML/MLUnit_1,2,3.pdf#p225#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 225, "snippet": "estimating confidence intervals : statistical significance • are m1 & m2 significantly different? • compute t. select significance level ( e. g. sig ="}
{"id": "AML/MLUnit_1,2,3.pdf#p226#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 226, "snippet": "model selection : roc curves • roc ( receiver operating characteristics ) curves : for visual comparison of classification models • originated from si"}
{"id": "AML/MLUnit_1,2,3.pdf#p227#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 227, "snippet": "issues affecting model selection • accuracy • classifier accuracy : predicting class label • speed • time to construct the model ( training time ) • t"}
{"id": "AML/MLUnit_1,2,3.pdf#p228#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 228, "snippet": "bias and variance 2 / 8 / 2024 107 errors in machine learning • irreducible errors are errors which will always be present in a machine learning model"}
{"id": "AML/MLUnit_1,2,3.pdf#p229#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 229, "snippet": "bias and variance 2 / 8 / 2024 108 what is bias? • bias refers to the error that is introduced by approximating a real - world problem with a simplifi"}
{"id": "AML/MLUnit_1,2,3.pdf#p230#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 230, "snippet": "bias and variance 2 / 8 / 2024 109 • when the bias is high, assumptions made by our model are too basic, the model can ’ t capture the important featu"}
{"id": "AML/MLUnit_1,2,3.pdf#p231#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 231, "snippet": "bias and variance 2 / 8 / 2024 110 what is variance? • variance, on the other hand, refers to the amount by which the predictions of a model would cha"}
{"id": "AML/MLUnit_1,2,3.pdf#p232#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 232, "snippet": "bias and variance 2 / 8 / 2024 111 what is variance? • we can see that our model has learned extremely well for our training data, which has taught it"}
{"id": "AML/MLUnit_1,2,3.pdf#p233#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 233, "snippet": "bias and variance 2 / 8 / 2024 112 • hence, model will perform really well on testing data and get high accuracy but will fail to perform on new, unse"}
{"id": "AML/MLUnit_1,2,3.pdf#p234#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 234, "snippet": "bias - variance tradeoff 2 / 8 / 2024 113 • in general, the goal of model training is to achieve a balance between bias and variance. • this is often "}
{"id": "AML/MLUnit_1,2,3.pdf#p235#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 235, "snippet": "bias - variance tradeoff 2 / 8 / 2024 114 • the above bull ’ s eye graph helps explain bias and variance tradeoff better. • the best fit is when the d"}
{"id": "AML/MLUnit_1,2,3.pdf#p236#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 236, "snippet": "bias - variance tradeoff 2 / 8 / 2024 115 • a model with high bias and low variance may be too simple and fail to capture the complexity of the underl"}
{"id": "AML/MLUnit_1,2,3.pdf#p237#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 237, "snippet": "bias - variance tradeoff 2 / 8 / 2024 116 • in machine learning, bias & variance are two sources of errors that can affect the performance of a model."}
{"id": "AML/MLUnit_1,2,3.pdf#p238#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 238, "snippet": "bias - variance tradeoff 2 / 8 / 2024 117 • cross - validation : cross - validation can be used to evaluate the performance of the model and identify "}
{"id": "AML/MLUnit_1,2,3.pdf#p239#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 239, "snippet": "ensemble method"}
{"id": "AML/MLUnit_1,2,3.pdf#p240#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 240, "snippet": "2 / 8 / 2024 data science 119"}
{"id": "AML/MLUnit_1,2,3.pdf#p241#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 241, "snippet": "2 / 8 / 2024 data science 120"}
{"id": "AML/MLUnit_1,2,3.pdf#p242#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 242, "snippet": "2 / 8 / 2024 data science 121"}
{"id": "AML/MLUnit_1,2,3.pdf#p243#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 243, "snippet": "ensemble method training data l1 l2 l3 l4 l * ensemble learning : multiple machine learning models ( classifiers ) are combined to solve a particular "}
{"id": "AML/MLUnit_1,2,3.pdf#p244#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 244, "snippet": "▪ “ a group of item viewed as a whole rather than individually ” ▪ ensemble method : not depend on just one model or algorithm for your output. rather"}
{"id": "AML/MLUnit_1,2,3.pdf#p245#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 245, "snippet": "d d1 d2 d3 dn c1 c2 c3 cn c * x 1 ensemble method 2 / 8 / 2024 124 csp42b : aml original training data randomly chosen sample data classifier with dif"}
{"id": "AML/MLUnit_1,2,3.pdf#p246#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 246, "snippet": "2 / 8 / 2024 data science 125"}
{"id": "AML/MLUnit_1,2,3.pdf#p247#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 247, "snippet": "2 / 8 / 2024 data science 126"}
{"id": "AML/MLUnit_1,2,3.pdf#p248#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 248, "snippet": "2 / 8 / 2024 data science 127"}
{"id": "AML/MLUnit_1,2,3.pdf#p249#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 249, "snippet": "2 / 8 / 2024 data science 128"}
{"id": "AML/MLUnit_1,2,3.pdf#p250#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 250, "snippet": "2 / 8 / 2024 data science 129"}
{"id": "AML/MLUnit_1,2,3.pdf#p251#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 251, "snippet": "2 / 8 / 2024 data science 130"}
{"id": "AML/MLUnit_1,2,3.pdf#p252#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 252, "snippet": "bootstrap aggregation ( bagging )"}
{"id": "AML/MLUnit_1,2,3.pdf#p253#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 253, "snippet": "bootstrap aggregation ( bagging ) ▪ d : original training data ( contains many sample, records ) ▪ d1 : randomly chosen sample data record to make new"}
{"id": "AML/MLUnit_1,2,3.pdf#p254#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 254, "snippet": "bagging ▪ reduces overfitting ( variance ) ▪ normally uses one type of classifier ▪ decision trees are popular ▪ easy to parallelize 2 / 8 / 2024 133 "}
{"id": "AML/MLUnit_1,2,3.pdf#p255#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 255, "snippet": "bagging ( random forest )"}
{"id": "AML/MLUnit_1,2,3.pdf#p256#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 256, "snippet": "random forest algorithm • ensemble method specifically designed for decision tree classifiers • random forests grows many trees • ensemble of unpruned"}
{"id": "AML/MLUnit_1,2,3.pdf#p257#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 257, "snippet": "rf 2 / 8 / 2024 136 csp42b : aml"}
{"id": "AML/MLUnit_1,2,3.pdf#p258#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 258, "snippet": "2 / 8 / 2024 data science 137"}
{"id": "AML/MLUnit_1,2,3.pdf#p259#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 259, "snippet": "boosting • boosting is an ensemble learning method that combines a set of weak learners into strong learners to minimize training errors. • in boostin"}
{"id": "AML/MLUnit_1,2,3.pdf#p260#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 260, "snippet": "boosting • in boosting, the weak learners are typically decision trees, but they can also be other types of classifiers, such as linear models or neur"}
{"id": "AML/MLUnit_1,2,3.pdf#p261#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 261, "snippet": "boosting here ' s how the algorithm works : • step 1 : the base algorithm reads the data and assigns equal weight to each sample observation. • step 2"}
{"id": "AML/MLUnit_1,2,3.pdf#p262#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 262, "snippet": "boosting some popular boosting algorithms include 1. adaboost ( adaptive boosting ), 2. gradient boosting, 3. xgboost. • these algorithms differ in th"}
{"id": "AML/MLUnit_1,2,3.pdf#p263#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 263, "snippet": "adaboost ( adaptive boosting ) • adaboost, short for adaptive boosting, is a machine learning algorithm used for classification and regression tasks. "}
{"id": "AML/MLUnit_1,2,3.pdf#p264#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 264, "snippet": "adaboost ( adaptive boosting ) here are the steps involved in adaboost : • initialize the weights of the training examples to be equal. • train a weak"}
{"id": "AML/MLUnit_1,2,3.pdf#p265#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 265, "snippet": "adaboost ( adaptive boosting ) the most important parameters are base _ estimator, n _ estimators, and learning _ rate • base _ estimator : it is a we"}
{"id": "AML/MLUnit_1,2,3.pdf#p266#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 266, "snippet": "adaboost ( adaptive boosting ) pros • adaboost is easy to implement. • it iteratively corrects the mistakes of the weak classifier and improves accura"}
{"id": "AML/MLUnit_1,2,3.pdf#p267#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 267, "snippet": "gradient boosting • gradient boosting is an ensemble learning technique that combines multiple weak learners ( usually decision trees ) to build a str"}
{"id": "AML/MLUnit_1,2,3.pdf#p268#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 268, "snippet": "gradient boosting here are some of the most important parameters : • n _ estimators : this parameter controls the number of trees in the ensemble. • l"}
{"id": "AML/MLUnit_1,2,3.pdf#p269#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 269, "snippet": "xgboost ( extreme gradient boosting ) • xgboost ( extreme gradient boosting ) is a decision tree - based ensemble machine learning algorithm that uses"}
{"id": "AML/MLUnit_1,2,3.pdf#p270#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 270, "snippet": "xgboost ( extreme gradient boosting ) here ' s how it works : • initialization : the algorithm starts with a single decision tree, which serves as the"}
{"id": "AML/MLUnit_1,2,3.pdf#p271#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 271, "snippet": "comparison of boosting algorithms 2 / 8 / 2024 150"}
{"id": "AML/MLUnit_1,2,3.pdf#p272#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 272, "snippet": "2 / 8 / 2024 data science 151"}
{"id": "AML/MLUnit_1,2,3.pdf#p273#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 273, "snippet": "2 / 8 / 2024 data science 152"}
{"id": "AML/MLUnit_1,2,3.pdf#p274#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 274, "snippet": "stacking • it is an ensemble method that combines multiple models ( classification or regression ) via meta - model ( meta - classifier or meta - regr"}
{"id": "AML/MLUnit_1,2,3.pdf#p275#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 275, "snippet": "stacking • stacking is a bit different from the basic ensembling methods because it has first - level and second - level models. • stacking features a"}
{"id": "AML/MLUnit_1,2,3.pdf#p276#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 276, "snippet": "stacking • stacking is a popular ensemble learning technique used in machine learning to improve the predictive performance of models. • the idea behi"}
{"id": "AML/MLUnit_1,2,3.pdf#p277#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 277, "snippet": "156 the stacking process involves the following steps : • splitting the training data into two or more folds. • training several base models on each f"}
{"id": "AML/MLUnit_1,2,3.pdf#p278#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 278, "snippet": "157 • stacking can improve the predictive performance of models because it combines the strengths of different models. • for example, one base model m"}
{"id": "AML/MLUnit_1,2,3.pdf#p279#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 279, "snippet": "158 stacking for deep learning"}
{"id": "AML/MLUnit_1,2,3.pdf#p280#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 280, "snippet": "how to evaluate ml models 2 / 8 / 2024 159 classification metrics : confusion matrix a confusion matrix is a table that is often used to evaluate the "}
{"id": "AML/MLUnit_1,2,3.pdf#p281#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 281, "snippet": "how to evaluate ml models 2 / 8 / 2024 160 confusion matrix : example of a pregnancy test, where an actual pregnant woman and a fat man consult a doct"}
{"id": "AML/MLUnit_1,2,3.pdf#p282#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 282, "snippet": "how to evaluate ml models 2 / 8 / 2024 161 tp ( true positive ) : the woman is pregnant, and she is predicted as pregnant. here p represents positive "}
{"id": "AML/MLUnit_1,2,3.pdf#p283#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 283, "snippet": "2 / 8 / 2024 162 accuracy : is the simplest metric and can be defined as the number of test cases correctly classified divided by the total number of "}
{"id": "AML/MLUnit_1,2,3.pdf#p284#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 284, "snippet": "precision 2 / 8 / 2024 163 precision is the proportion of true positive predictions among all positive predictions made by the classifier. precision ="}
{"id": "AML/MLUnit_1,2,3.pdf#p285#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 285, "snippet": "recall & f1 score 2 / 8 / 2024 164 recall ( also known as sensitivity or true positive rate - tpr ) is the proportion of true positive predictions amo"}
{"id": "AML/MLUnit_1,2,3.pdf#p286#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 286, "snippet": "classifier evaluation metrics : confusion matrix actual class \\ predicted class buy _ computer = yes buy _ computer = no total buy _ computer = yes 69"}
{"id": "AML/MLUnit_1,2,3.pdf#p287#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 287, "snippet": "confusion matrix - example 166 actual class \\ predicted class c1 ¬ c1 c1 true positives ( tp ) false negatives ( fn ) ¬ c1 false positives ( fp ) true"}
{"id": "AML/MLUnit_1,2,3.pdf#p288#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 288, "snippet": "classifier evaluation metrics : accuracy, error rate, sensitivity and specificity • classifier accuracy, or recognition rate : percentage of test set "}
{"id": "AML/MLUnit_1,2,3.pdf#p289#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 289, "snippet": "classifier evaluation metrics : precision and recall, and f - measures • precision : exactness – what % of tuples that the classifier labeled as posit"}
{"id": "AML/MLUnit_1,2,3.pdf#p290#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 290, "snippet": "classifier evaluation metrics : example 169 • precision = 90 / 230 = 39. 13 % recall = 90 / 300 = 30. 00 % actual class \\ predicted class cancer = yes"}
{"id": "AML/MLUnit_1,2,3.pdf#p291#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 291, "snippet": "tpr, fpr, tnr, fnr 2 / 8 / 2024 170 rate is a measure factor in a confusion matrix. it has also 4 type tpr, fpr, tnr, fnr • true positive rate ( tpr )"}
{"id": "AML/MLUnit_1,2,3.pdf#p292#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 292, "snippet": "confusion matrix target positive negative model positive 70 tp 20 fp positive predictive value precision 0. 78 negative 30 fn 80 tn negative predictiv"}
{"id": "AML/MLUnit_1,2,3.pdf#p293#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 293, "snippet": "classification rate / accuracy ▪ accuracy or classification accuracy tells the number of correct predictions made by the model. ▪ it is the ratio of t"}
{"id": "AML/MLUnit_1,2,3.pdf#p294#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 294, "snippet": "misclassification rate ( error ) ▪ accuracy or classification accuracy tells the number of wrong predictions made by the model. ▪ it is the ratio of t"}
{"id": "AML/MLUnit_1,2,3.pdf#p295#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 295, "snippet": "precision / positive predictive value ( ppv ) ▪ when it predicts yes, how often is it correct? ▪ precision is defined as the number of true positives "}
{"id": "AML/MLUnit_1,2,3.pdf#p296#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 296, "snippet": "sensitivity / recall / true positive rate ▪ when it ’ s actually yes, how often does it predict yes? ▪ the recall is the fraction of positive events t"}
{"id": "AML/MLUnit_1,2,3.pdf#p297#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 297, "snippet": "specificity / true negative rate ▪ when it ’ s no, how often does it predict no? ▪ it is the true negative rate or the proportion of true negatives to"}
{"id": "AML/MLUnit_1,2,3.pdf#p298#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 298, "snippet": "f1 score ▪ the f1 score is the harmonic mean of the precision and recall, where ▪ f1 score reaches its best value at 1 ( perfect precision and recall "}
{"id": "AML/MLUnit_1,2,3.pdf#p299#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 299, "snippet": "roc chart.. receiver operating characteristic ▪ the roc chart : false positive rate ( fp r ) ( 1 - specificity ) on x - axis, the probability of targe"}
{"id": "AML/MLUnit_1,2,3.pdf#p300#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 300, "snippet": "area under the curve ( auc ) ▪ area under roc curve is often used as a measure of quality of the classification models. ▪ a random classifier has an a"}
{"id": "AML/MLUnit_1,2,3.pdf#p301#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 301, "snippet": "roc - auc ▪ an area under the roc curve of 0. 8, for example, means that a randomly selected case from the group with the target equals 1 has a score "}
{"id": "AML/MLUnit_1,2,3.pdf#p302#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 302, "snippet": "2 / 8 / 2024 csp42b : aml 181 roc curve"}
{"id": "AML/MLUnit_1,2,3.pdf#p303#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 303, "snippet": "auc - roc 2 / 8 / 2024 182 • roc curve is a plot of true positive rate ( recall ) against false positive rate ( tn / ( tn + fp ) ). • auc - roc stands"}
{"id": "AML/MLUnit_1,2,3.pdf#p304#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 304, "snippet": "auc - roc 2 / 8 / 2024 183 • the auc roc score is the area under the roc curve and ranges from 0 to 1, with a score of 0. 5 indicating a random classi"}
{"id": "AML/MLUnit_1,2,3.pdf#p305#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 305, "snippet": "classifier evaluation metrics : confusion matrix actual class \\ predicted class buy _ computer = yes buy _ computer = no total buy _ computer = yes 69"}
{"id": "AML/MLUnit_1,2,3.pdf#p306#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 306, "snippet": "classifier evaluation metrics : accuracy, error rate, sensitivity and specificity • classifier accuracy, or recognition rate : percentage of test set "}
{"id": "AML/MLUnit_1,2,3.pdf#p307#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 307, "snippet": "classifier evaluation metrics : precision and recall, and f - measures • precision : exactness – what % of tuples that the classifier labeled as posit"}
{"id": "AML/MLUnit_1,2,3.pdf#p308#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 308, "snippet": "classifier evaluation metrics : example 187 • precision = 90 / 230 = 39. 13 % recall = 90 / 300 = 30. 00 % actual class \\ predicted class cancer = yes"}
{"id": "AML/MLUnit_1,2,3.pdf#p309#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 309, "snippet": "overfitting ▪ natural end of process in dt is 100 % purity in each leaf ▪ this overfits the data, which end up fitting noise in the data ▪ overfitting"}
{"id": "AML/MLUnit_1,2,3.pdf#p310#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 310, "snippet": "overfitting & underfitting ▪ training a model : ▪ overfit : if the model is performing much better on train set but not performing well on cross - val"}
{"id": "AML/MLUnit_1,2,3.pdf#p311#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 311, "snippet": "2 / 8 / 2024 csp42b : aml 190"}
{"id": "AML/MLUnit_1,2,3.pdf#p312#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 312, "snippet": "why machine learning important? 1"}
{"id": "AML/MLUnit_1,2,3.pdf#p313#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 313, "snippet": "what is regression “ regression is the process of estimating the relationship between a dependent variable and independent variables • it is a fitting"}
{"id": "AML/MLUnit_1,2,3.pdf#p314#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 314, "snippet": "types of regression techniques • there are many types of regression analysis techniques, and the use of each method depends upon the number of factors"}
{"id": "AML/MLUnit_1,2,3.pdf#p315#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 315, "snippet": "linear regression • “ linear regression predicts the relationship between two variables by assuming a linear connection between the independent and de"}
{"id": "AML/MLUnit_1,2,3.pdf#p316#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 316, "snippet": "simple linear regression • in a simple linear regression, there is one independent variable and one dependent variable • the model estimates the slope"}
{"id": "AML/MLUnit_1,2,3.pdf#p317#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 317, "snippet": "simple linear regression • linear regression shows the linear relationship between the independent ( predictor ) variable i. e. x - axis and the depen"}
{"id": "AML/MLUnit_1,2,3.pdf#p318#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 318, "snippet": "simple regression calculation • to calculate best - fit line linear regression uses a traditional slope - intercept form which is given below, yi = β0"}
{"id": "AML/MLUnit_1,2,3.pdf#p319#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 319, "snippet": "least - squares method • finds the line of best fit for a dataset, providing a visual demonstration of the relationship between the data points. • the"}
{"id": "AML/MLUnit_1,2,3.pdf#p320#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 320, "snippet": "least - squares method formula of least square method step 1 : denote the independent variable values as xi and the dependent ones as yi. step 2 : cal"}
{"id": "AML/MLUnit_1,2,3.pdf#p321#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 321, "snippet": "least - squares method problem 1 : find the line of best fit for the following data points using the least squares method : ( x, y ) = ( 1, 3 ), ( 2, "}
{"id": "AML/MLUnit_1,2,3.pdf#p322#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 322, "snippet": "least - squares method the slope of the line of best fit can be calculated from the formula as follows : m = ( σ ( x – xi ) * ( y – yi ) ) / σ ( x – x"}
{"id": "AML/MLUnit_1,2,3.pdf#p323#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 323, "snippet": "multiple linear regression / multivariable regression • multiple linear regression is a technique to understand the relationship between a single depe"}
{"id": "AML/MLUnit_1,2,3.pdf#p324#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 324, "snippet": "considerations of multiple linear regression multiple linear regression assumptions 1. overfitting : when more and more variables are added to a model"}
{"id": "AML/MLUnit_1,2,3.pdf#p325#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 325, "snippet": "multiple linear regression / multivariable regression"}
{"id": "AML/MLUnit_1,2,3.pdf#p326#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 326, "snippet": "multiple linear regression / multivariable regression"}
{"id": "AML/MLUnit_1,2,3.pdf#p327#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 327, "snippet": "multiple linear regression / multivariable regression suppose we have the following dataset with one response variable y and two predictor variables x"}
{"id": "AML/MLUnit_1,2,3.pdf#p328#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 328, "snippet": "multiple linear regression / multivariable regression step 1 : calculate x12, x22, x1y, x2y and x1x2."}
{"id": "AML/MLUnit_1,2,3.pdf#p329#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 329, "snippet": "multiple linear regression / multivariable regression step 2 : calculate regression sums. next, make the following regression sum calculations : σx1 2"}
{"id": "AML/MLUnit_1,2,3.pdf#p330#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 330, "snippet": "multiple linear regression / multivariable regression step 3 : calculate b0, b1, and b2. the formula to calculate b1 is : [ ( σx22 ) ( σx1y ) – ( σx1x"}
{"id": "AML/MLUnit_1,2,3.pdf#p331#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 331, "snippet": "logistic regression • logistic regression is one of the types of regression analysis technique, which gets used when the dependent variable is discret"}
{"id": "AML/MLUnit_1,2,3.pdf#p332#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 332, "snippet": "logistic regression • if the output of the sigmoid function ( estimated probability ) is greater than a predefined threshold on the graph, the model p"}
{"id": "AML/MLUnit_1,2,3.pdf#p333#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 333, "snippet": "linear vs logistic"}
{"id": "AML/MLUnit_1,2,3.pdf#p334#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 334, "snippet": "lasso regression • lasso regression is one of the types of regression in machine learning that performs regularization along with feature selection. i"}
{"id": "AML/MLUnit_1,2,3.pdf#p335#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 335, "snippet": "lasso regression lasso regression is a type of linear regression that uses shrinkage. shrinkage is where data values are shrunk towards a central poin"}
{"id": "AML/MLUnit_1,2,3.pdf#p336#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 336, "snippet": "lasso regression l1 regularization lasso regression performs l1 regularization, which adds a penalty equal to the absolute value of the magnitude of c"}
{"id": "AML/MLUnit_1,2,3.pdf#p337#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 337, "snippet": "ridge regression • this is another one of the types of regression in machine learning which is usually used when there is a high correlation between t"}
{"id": "AML/MLUnit_1,2,3.pdf#p338#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 338, "snippet": "polynomial regression • polynomial regression is another one of the types of regression analysis techniques in machine learning, which is the same as "}
{"id": "AML/MLUnit_1,2,3.pdf#p339#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 339, "snippet": "polynomial regression • it is also called the special case of multiple linear regression in ml. because we add some polynomial terms to the multiple l"}
{"id": "AML/MLUnit_1,2,3.pdf#p340#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 340, "snippet": "polynomial regression in the above image, we have taken a dataset which is arranged non - linearly. so if we try to cover it with a linear model, then"}
{"id": "AML/MLUnit_1,2,3.pdf#p341#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 341, "snippet": "polynomial regression equation of the polynomial regression model : simple linear regression equation : y = b0 + b1x......... ( a ) multiple linear re"}
{"id": "AML/MLUnit_1,2,3.pdf#p342#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 342, "snippet": "regression metrics true values and predicted values : in regression, we ’ ve got two units of values to compare : the actual target values ( authentic"}
{"id": "AML/MLUnit_1,2,3.pdf#p343#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 343, "snippet": "regression metrics mean absolute error ( mae ) in the fields of statistics and machine learning, the mean absolute error ( mae ) is a frequently emplo"}
{"id": "AML/MLUnit_1,2,3.pdf#p344#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 344, "snippet": "regression metrics mean squared error ( mse ) a popular metric in statistics and machine learning is the mean squared error ( mse ). it measures the s"}
{"id": "AML/MLUnit_1,2,3.pdf#p345#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 345, "snippet": "regression metrics r - squared ( r² ) score a statistical metric frequently used to assess the goodness of fit of a regression model is the r - square"}
{"id": "AML/MLUnit_1,2,3.pdf#p346#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 346, "snippet": "regression metrics root mean squared error • rmse stands for root mean squared error. it is a usually used metric in regression analysis and machine l"}
{"id": "AML/MLUnit_1,2,3.pdf#p347#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 347, "snippet": "regularization overfitting is one of the most serious kinds of problems related to machine learning. it occurs when a model learns the training data t"}
{"id": "AML/MLUnit_1,2,3.pdf#p348#c1", "source": "AML/MLUnit_1,2,3.pdf", "page": 348, "snippet": "applications of regression • financial forecasting ( like house price estimates, or stock prices ) • sales and promotions forecasting • testing automo"}
{"id": "AML/ML Unit IV_Final.pptx#p1#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 1, "snippet": "aml - unit iv school of computer engineering & technology"}
{"id": "AML/ML Unit IV_Final.pptx#p2#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 2, "snippet": "model evaluation and selection to properly evaluate your machine learning models and select the best one, you need a good validation strategy and soli"}
{"id": "AML/ML Unit IV_Final.pptx#p3#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 3, "snippet": "model evaluation and selection how to evaluate machine learning models and select the best one? step 1 : choose a proper validation strategy. step 2 :"}
{"id": "AML/ML Unit IV_Final.pptx#p4#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 4, "snippet": "model selection in machine learning resampling methods resampling methods - rearranging data samples to inspect if the model performs well on data sam"}
{"id": "AML/ML Unit IV_Final.pptx#p5#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 5, "snippet": "model selection in machine learning time - based split there are some types of data where random splits are not possible. for example, if we have to t"}
{"id": "AML/ML Unit IV_Final.pptx#p6#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 6, "snippet": "model selection in machine learning k - fold cross - validation the cross - validation technique works by randomly shuffling the dataset and then spli"}
{"id": "AML/ML Unit IV_Final.pptx#p7#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 7, "snippet": "model selection in machine learning stratified k - fold similar to that of k - fold cross - validation with one single point of difference – unlike in"}
{"id": "AML/ML Unit IV_Final.pptx#p8#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 8, "snippet": "model selection in machine learning bootstrap : bootstrap is one of the most powerful ways to obtain a stabilized model. it is close to the random spl"}
{"id": "AML/ML Unit IV_Final.pptx#p9#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 9, "snippet": "how to evaluate ml models 12 / 12 / 2024 9 classification metrics : confusion matrix a confusion matrix is a table that is often used to evaluate the "}
{"id": "AML/ML Unit IV_Final.pptx#p10#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 10, "snippet": "how to evaluate ml models 12 / 12 / 2024 10 confusion matrix : example of a pregnancy test, where an actual pregnant woman and a fat man consult a doc"}
{"id": "AML/ML Unit IV_Final.pptx#p11#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 11, "snippet": "how to evaluate ml models 12 / 12 / 2024 11 tp ( true positive ) : the woman is pregnant, and she is predicted as pregnant. here p represents positive"}
{"id": "AML/ML Unit IV_Final.pptx#p12#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 12, "snippet": "12 / 12 / 2024 12 accuracy : is the simplest metric and can be defined as the number of test cases correctly classified divided by the total number of"}
{"id": "AML/ML Unit IV_Final.pptx#p13#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 13, "snippet": "precision 12 / 12 / 2024 13 precision is the proportion of true positive predictions among all positive predictions made by the classifier. precision "}
{"id": "AML/ML Unit IV_Final.pptx#p14#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 14, "snippet": "recall & f1 score 12 / 12 / 2024 14 recall ( also known as sensitivity or true positive rate - tpr ) is the proportion of true positive predictions am"}
{"id": "AML/ML Unit IV_Final.pptx#p15#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 15, "snippet": "classifier evaluation metrics : confusion matrix actual class \\ predicted class buy _ computer = yes buy _ computer = no total buy _ computer = yes 69"}
{"id": "AML/ML Unit IV_Final.pptx#p16#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 16, "snippet": "confusion matrix - example 16 actual class \\ predicted class c1 ¬ c1 c1 true positives ( tp ) false negatives ( fn ) ¬ c1 false positives ( fp ) true "}
{"id": "AML/ML Unit IV_Final.pptx#p17#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 17, "snippet": "classifier evaluation metrics : accuracy, error rate, sensitivity and specificity classifier accuracy, or recognition rate : percentage of test set tu"}
{"id": "AML/ML Unit IV_Final.pptx#p18#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 18, "snippet": "classifier evaluation metrics : precision and recall, and f - measures precision : exactness – what % of tuples that the classifier labeled as positiv"}
{"id": "AML/ML Unit IV_Final.pptx#p19#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 19, "snippet": "classifier evaluation metrics : example 19 precision = 90 / 230 = 39. 13 % recall = 90 / 300 = 30. 00 % actual class \\ predicted class cancer = yes ca"}
{"id": "AML/ML Unit IV_Final.pptx#p20#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 20, "snippet": "tpr, fpr, tnr, fnr 12 / 12 / 2024 20 rate is a measure factor in a confusion matrix. it has also 4 type tpr, fpr, tnr, fnr true positive rate ( tpr ) "}
{"id": "AML/ML Unit IV_Final.pptx#p21#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 21, "snippet": "confusion matrix target positive negative model positive 70 tp 20 fp positive predictive value precision 0. 78 negative 30 fn 80 tn negative predictiv"}
{"id": "AML/ML Unit IV_Final.pptx#p22#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 22, "snippet": "classification rate / accuracy accuracy or classification accuracy tells the number of correct predictions made by the model. it is the ratio of the n"}
{"id": "AML/ML Unit IV_Final.pptx#p23#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 23, "snippet": "misclassification rate ( error ) accuracy or classification accuracy tells the number of wrong predictions made by the model. it is the ratio of the n"}
{"id": "AML/ML Unit IV_Final.pptx#p24#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 24, "snippet": "precision / positive predictive value ( ppv ) when it predicts yes, how often is it correct? precision is defined as the number of true positives divi"}
{"id": "AML/ML Unit IV_Final.pptx#p25#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 25, "snippet": "sensitivity / recall / true positive rate when it ’ s actually yes, how often does it predict yes? the recall is the fraction of positive events that "}
{"id": "AML/ML Unit IV_Final.pptx#p26#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 26, "snippet": "specificity / true negative rate when it ’ s no, how often does it predict no? it is the true negative rate or the proportion of true negatives to eve"}
{"id": "AML/ML Unit IV_Final.pptx#p27#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 27, "snippet": "f1 score the f1 score is the harmonic mean of the precision and recall, where f1 score reaches its best value at 1 ( perfect precision and recall ) f1"}
{"id": "AML/ML Unit IV_Final.pptx#p28#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 28, "snippet": "regression metrics 12 / 12 / 2024 28 mean squared error or mse mse is a simple metric that calculates the difference between the actual value and the "}
{"id": "AML/ML Unit IV_Final.pptx#p29#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 29, "snippet": "regression metrics 12 / 12 / 2024 29 r - squared ( r² ) is a statistical measure that represents the proportion of the variance in the dependent varia"}
{"id": "AML/ML Unit IV_Final.pptx#p30#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 30, "snippet": "regression metrics 12 / 12 / 2024 30 r - squared is calculated as follows : r² = 1 - ( ssres / sstot ) where ssres is the sum of squared residuals ( t"}
{"id": "AML/ML Unit IV_Final.pptx#p31#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 31, "snippet": "regression metrics 12 / 12 / 2024 31 adjusted r - squared is a modified version of r - squared that takes into account the number of independent varia"}
{"id": "AML/ML Unit IV_Final.pptx#p32#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 32, "snippet": "bias and variance 12 / 12 / 2024 32 errors in machine learning irreducible errors are errors which will always be present in a machine learning model,"}
{"id": "AML/ML Unit IV_Final.pptx#p33#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 33, "snippet": "bias and variance 12 / 12 / 2024 33 what is bias? bias refers to the error that is introduced by approximating a real - world problem with a simplifie"}
{"id": "AML/ML Unit IV_Final.pptx#p34#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 34, "snippet": "bias and variance 12 / 12 / 2024 34 when the bias is high, assumptions made by our model are too basic, the model can ’ t capture the important featur"}
{"id": "AML/ML Unit IV_Final.pptx#p35#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 35, "snippet": "bias and variance 12 / 12 / 2024 35 what is variance? variance, on the other hand, refers to the amount by which the predictions of a model would chan"}
{"id": "AML/ML Unit IV_Final.pptx#p36#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 36, "snippet": "bias and variance 12 / 12 / 2024 36 what is variance? we can see that our model has learned extremely well for our training data, which has taught it "}
{"id": "AML/ML Unit IV_Final.pptx#p37#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 37, "snippet": "bias and variance 12 / 12 / 2024 37 hence, model will perform really well on testing data and get high accuracy but will fail to perform on new, unsee"}
{"id": "AML/ML Unit IV_Final.pptx#p38#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 38, "snippet": "bias - variance tradeoff 12 / 12 / 2024 38 in general, the goal of model training is to achieve a balance between bias and variance. this is often ref"}
{"id": "AML/ML Unit IV_Final.pptx#p39#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 39, "snippet": "bias - variance tradeoff 12 / 12 / 2024 39 the above bull ’ s eye graph helps explain bias and variance tradeoff better. the best fit is when the data"}
{"id": "AML/ML Unit IV_Final.pptx#p40#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 40, "snippet": "bias - variance tradeoff 12 / 12 / 2024 40 a model with high bias and low variance may be too simple and fail to capture the complexity of the underly"}
{"id": "AML/ML Unit IV_Final.pptx#p41#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 41, "snippet": "bias - variance tradeoff 12 / 12 / 2024 41 in machine learning, bias & variance are two sources of errors that can affect the performance of a model. "}
{"id": "AML/ML Unit IV_Final.pptx#p42#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 42, "snippet": "bias - variance tradeoff 12 / 12 / 2024 42 cross - validation : cross - validation can be used to evaluate the performance of the model and identify t"}
{"id": "AML/ML Unit IV_Final.pptx#p43#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 43, "snippet": "ensemble method"}
{"id": "AML/ML Unit IV_Final.pptx#p44#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 44, "snippet": "12 / 12 / 2024 data science 44"}
{"id": "AML/ML Unit IV_Final.pptx#p45#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 45, "snippet": "positive vs. negative class in machine learning, a binary classification problem involves dividing a dataset into two classes : positive and negative."}
{"id": "AML/ML Unit IV_Final.pptx#p46#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 46, "snippet": "positive vs. negative class to address this issue, techniques such as oversampling the minority class, undersampling the majority class, or using weig"}
{"id": "AML/ML Unit IV_Final.pptx#p47#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 47, "snippet": "improving classification accuracy of class imbalanced data improving classification accuracy of class imbalanced data is a challenging problem in mach"}
{"id": "AML/ML Unit IV_Final.pptx#p48#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 48, "snippet": "positive vs. negative class algorithm selection : certain algorithms perform better on imbalanced datasets than others. algorithms such as random fore"}
{"id": "AML/ML Unit IV_Final.pptx#p49#c1", "source": "AML/ML Unit IV_Final.pptx", "page": 49, "snippet": "12 / 12 / 2024 data science 49"}
{"id": "AML/AML Unit V (chatgpt).pdf#p1#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 1, "snippet": "you said : bayesian belief networks chatgpt said : chatgpt a bayesian belief network ( bbn ), also known as a bayesian network ( bn ), is a graphical "}
{"id": "AML/AML Unit V (chatgpt).pdf#p2#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 2, "snippet": "• robotics and ai : planning and decision - making. example consider a simple network for a medical diagnosis : • nodes : disease ( d ), symptom 1 ( s"}
{"id": "AML/AML Unit V (chatgpt).pdf#p3#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 3, "snippet": "• the process of computing probabilities of interest, such as p ( ) p ( ), using the structure and cpds. mechanisms of bayesian networks 1. constructi"}
{"id": "AML/AML Unit V (chatgpt).pdf#p4#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 4, "snippet": "• d - separation ( dependency separation ) : a set of nodes zz makes nodes xx and yy independent if zz \" blocks \" all paths between them. this is used"}
{"id": "AML/AML Unit V (chatgpt).pdf#p5#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 5, "snippet": "• relies on statistical tests for conditional independence. • steps : 1. perform pairwise independence tests to identify relationships. 2. use conditi"}
{"id": "AML/AML Unit V (chatgpt).pdf#p6#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 6, "snippet": "• for discrete variables, the dirichlet distribution is commonly used as a prior. c. expectation - maximization ( em ) algorithm • used when the data "}
{"id": "AML/AML Unit V (chatgpt).pdf#p7#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 7, "snippet": "• bnlearn : provides tools for learning and inference in bns. 3. other tools : • genie : gui - based tool for bayesian networks. • netica : commercial"}
{"id": "AML/AML Unit V (chatgpt).pdf#p8#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 8, "snippet": "• selection : chooses individuals to reproduce based on their fitness. • crossover ( recombination ) : combines two parent individuals to create offsp"}
{"id": "AML/AML Unit V (chatgpt).pdf#p9#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 9, "snippet": "• strategies include generational replacement or elitism ( preserving the best individuals ). 7. termination : • repeat the process until a stopping c"}
{"id": "AML/AML Unit V (chatgpt).pdf#p10#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 10, "snippet": "2. population initialization : start with a random population of 5 - bit strings. 3. fitness function : f ( x ) = x2f ( x ) = x2. 4. selection, crosso"}
{"id": "AML/AML Unit V (chatgpt).pdf#p11#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 11, "snippet": "• formally : p ( st +, st−1, …, s1 ) = p ( st + ) p ( st +, st−1, …, s1 ) = p ( st + ). 2. hidden states : • the system exists in one of several state"}
{"id": "AML/AML Unit V (chatgpt).pdf#p12#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 12, "snippet": "3. learning : • given observations oo, estimate the model parameters ( π, a, b ) ( π, a, b ) that maximize p ( ) p ( ). • solved using the baum - welc"}
{"id": "AML/AML Unit V (chatgpt).pdf#p13#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 13, "snippet": "example : weather prediction problem : • hidden states : weather ( s = { sunny, rainy } s = { sunny, rainy } ). • observations : activities ( o = { wa"}
{"id": "AML/AML Unit V (chatgpt).pdf#p14#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 14, "snippet": "• the learner or decision - maker. 2. environment : • the system the agent interacts with, which provides feedback. 3. state ( ss ) : • a representati"}
{"id": "AML/AML Unit V (chatgpt).pdf#p15#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 15, "snippet": "• the agent learns or has access to a model of the environment ( p ( s ′, a ) p ( s ′, a ) and r ( s, a ) r ( s, a ) ). • useful for planning ( e. g.,"}
{"id": "AML/AML Unit V (chatgpt).pdf#p16#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 16, "snippet": "• combines the strengths of monte carlo methods and dynamic programming. • example : td ( 0 ), sarsa ( on - policy td control ). exploration vs. explo"}
{"id": "AML/AML Unit V (chatgpt).pdf#p17#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 17, "snippet": "rewards rewards = - 1 * np. ones ( 25 ) # default reward rewards [ 24 ] = 10 # goal state reward # transitions ( simplified for gridworld ) def next _"}
{"id": "AML/AML Unit V (chatgpt).pdf#p18#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 18, "snippet": "• cost efficiency : reduces the need for extensive manual labeling by focusing on the most valuable data points. • improved accuracy : targets the mos"}
{"id": "AML/AML Unit V (chatgpt).pdf#p19#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 19, "snippet": "• least confidence : min ( p ( ) ) min ( p ( ) ) • margin sampling : difference between the top two predicted probabilities. • entropy : measures the "}
{"id": "AML/AML Unit V (chatgpt).pdf#p20#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 20, "snippet": "• accelerates model convergence with fewer labeled examples. challenges 1. cold start problem : • initial model may not perform well without sufficien"}
{"id": "AML/AML Unit V (chatgpt).pdf#p21#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 21, "snippet": "deep learning is a subset of machine learning that uses artificial neural networks to model and understand complex patterns in data. it has achieved r"}
{"id": "AML/AML Unit V (chatgpt).pdf#p22#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 22, "snippet": "• tanh : tanh ( x ) = ex−e−xex + e−xtanh ( x ) = ex + e−xex−e−x how deep learning works 1. forward propagation : • input data flows through the networ"}
{"id": "AML/AML Unit V (chatgpt).pdf#p23#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 23, "snippet": "• utilize self - attention mechanisms for parallelized learning of sequence data. 5. generative adversarial networks ( gans ) : • composed of a genera"}
{"id": "AML/AML Unit V (chatgpt).pdf#p24#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 24, "snippet": "4. overfitting : • may perform poorly on unseen data if not properly regularized. 5. hyperparameter tuning : • requires careful tuning of parameters l"}
{"id": "AML/AML Unit V (chatgpt).pdf#p25#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 25, "snippet": "• detects diseases like cancer, alzheimer ' s, and heart conditions using image classification and segmentation. • example : identifying tumors in mri"}
{"id": "AML/AML Unit V (chatgpt).pdf#p26#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 26, "snippet": "• example : biometric systems for airport security. medical imaging • diagnoses diseases from x - rays, mris, and histopathological images. • example "}
{"id": "AML/AML Unit V (chatgpt).pdf#p27#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 27, "snippet": "• example : monitoring poaching activity with drones. 6. manufacturing and industry 4. 0 predictive maintenance • anticipates equipment failures using"}
{"id": "AML/AML Unit V (chatgpt).pdf#p28#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 28, "snippet": "gaming • builds intelligent non - player characters ( npcs ) using reinforcement learning. • example : ai opponents in real - time strategy games. 9. "}
{"id": "AML/AML Unit V (chatgpt).pdf#p29#c1", "source": "AML/AML Unit V (chatgpt).pdf", "page": 29, "snippet": "• analyzes data from experiments like the large hadron collider ( lhc ). • example : identifying rare particle interactions. genomics • decodes dna se"}
{"id": "ESD/M.tech Resume - Google Docs.pdf#p1#c1", "source": "ESD/M.tech Resume - Google Docs.pdf", "page": 1, "snippet": "[ your full name ] [ your address ] [ city, state, pin ] [ phone number ] [ email address ] [ linkedin profile ] professional summary result - driven "}
{"id": "ESD/M.tech Resume - Google Docs.pdf#p2#c1", "source": "ESD/M.tech Resume - Google Docs.pdf", "page": 2, "snippet": "● automated data cleaning and report generation for government agencies. research & publications ● “ predictive analytics for urban air quality using "}
{"id": "ESD/M.tech Resume - Google Docs.pdf#p3#c1", "source": "ESD/M.tech Resume - Google Docs.pdf", "page": 3, "snippet": "tips : ● replace placeholders like [ your university name ] with your actual details. ● quantify your achievements ( e. g., “ reduced reporting time b"}
{"id": "ESD/CV vs Resume - Google Docs.pdf#p1#c1", "source": "ESD/CV vs Resume - Google Docs.pdf", "page": 1, "snippet": "key differences in the indian context ● resume : typically 1 – 2 pages long. offers a summary of your skills, education, and work experience, tailored"}
{"id": "ESD/CV vs Resume - Google Docs.pdf#p2#c1", "source": "ESD/CV vs Resume - Google Docs.pdf", "page": 2, "snippet": "simple comparison table feature resume ( india ) cv ( india ) length 1 – 2 pages 2 + pages content skills, key experience detailed history ( education"}
{"id": "ESD/Building an ATS-Ready Résumé - Google Docs.pdf#p1#c1", "source": "ESD/Building an ATS-Ready Résumé - Google Docs.pdf", "page": 1, "snippet": "building an ats - ready resume a resume is your personal marketing brochure. to land interviews, it must pass two gatekeepers : applicant tracking ( a"}
{"id": "ESD/Building an ATS-Ready Résumé - Google Docs.pdf#p2#c1", "source": "ESD/Building an ATS-Ready Résumé - Google Docs.pdf", "page": 2, "snippet": "5 technical skills ( grouped ) programming web data tools os python, java html5, css3, react sql, pandas git, docker linux, windows use vertical bars "}
{"id": "ESD/Building an ATS-Ready Résumé - Google Docs.pdf#p3#c1", "source": "ESD/Building an ATS-Ready Résumé - Google Docs.pdf", "page": 3, "snippet": "formatting guidelines for ats compatibility checklist why it matters one - page, single - column layout ats misreads multi - columns 4 18. standard se"}
{"id": "ESD/Building an ATS-Ready Résumé - Google Docs.pdf#p4#c1", "source": "ESD/Building an ATS-Ready Résumé - Google Docs.pdf", "page": 4, "snippet": "tailoring for specific tech paths target role keywords to prioritize suggested projects front - end dev react, redux, figma portfolio spa, pwa, access"}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p1#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 1, "snippet": "module 1 : explainable ai ( xai ) ante - hoc models types gam ( generalised additive models ) s - aog ( and or graphs )"}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p2#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 2, "snippet": "gams generalised additive models ( gams ) are an adaptation that allows us to model non - linear data while maintaining explainability a gam is a line"}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p3#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 3, "snippet": "to do this, we simply replace beta coefficients from linear regression with a flexible function which allows nonlinear relationships this flexible fun"}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p4#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 4, "snippet": "linear regression lets go back to our linear regression for a minute the equation is defined by the sum of a linear combination of variables each vari"}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p5#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 5, "snippet": "how do gam ’ s work? in gams, we drop the assumption that our target can be calculated using a linear combination of variables by simply saying we can"}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p6#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 6, "snippet": "but what it s? we define it with the equation below, here we see β coming back and it represents the same thing ; a weight our other term, b is a basi"}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p7#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 7, "snippet": "smooth function eqation the great thing about this is that we can have k weights and functions per variable in our equation. this is much more flexibl"}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p8#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 8, "snippet": "a random selection of 4 splines from a gam fit in the bike dataset"}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p9#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 9, "snippet": "white box xai for ai bias and ethics ai provides complex algorithms that can replace or emulate human intelligence we tend to think that ai will sprea"}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p10#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 10, "snippet": "as such, just like any other method that processes data automatically, ai must follow the rules established by the international community, which comp"}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p11#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 11, "snippet": "we need to add ethical rules to our ai algorithms to avoid being slowed down by regulations and fines we must be ready to explain the alleged bias in "}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p12#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 12, "snippet": "we will determine how to approach ai ethically we will explain the risk of bias in our algorithms when an issue comes up we will apply explainable ai "}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p13#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 13, "snippet": "moral ai bias in self - driving cars explaining ai goes well beyond understanding how an ai algorithm works from a mathematical point of view to reach"}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p14#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 14, "snippet": "will use mit ' s moral machine experiment, which addresses the issue of how an ai machine will make moral decisions in life and death situations to un"}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p15#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 15, "snippet": "the trolley problem involves four protagonists : a runaway trolley going down a track : its brakes have failed, and it is out of control a group of fi"}
{"id": "AAI/Module 1_Ante Hoc Models (1).pptx#p17#c1", "source": "AAI/Module 1_Ante Hoc Models (1).pptx", "page": 17, "snippet": "in the following diagram, you can see the trolley on the left ; you in the middle, next to the lever ; the five people that will be killed if the trol"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p1#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 1, "snippet": "module 2 : edge ai? what is edge ai? how will edge ai transform enterprises? the advantages of applying machine learning on edge technologies influenc"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p2#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 2, "snippet": "module 2 edge ai dr. nitin pise, professor, scet, mit wpu"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p3#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 3, "snippet": "what is edge ai? ai relies heavily on data transmission and computation of complex machine learning algorithms. edge computing sets up a new age compu"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p4#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 4, "snippet": "how will edge ai transform enterprises? an efficient edge ai model has an optimized infrastructure for edge computing that can handle bulkier ai workl"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p5#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 5, "snippet": "benefits a few benefits of edge computing powered by ai on enterprises include : an efficient predictive maintenance and asset management inspection s"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p6#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 6, "snippet": "the advantages of applying machine learning on edge machine learning is the artificial simulation of the human learning process with the use of data a"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p7#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 7, "snippet": "the advantages of applying machine learning on edge continued … reduced latency : most of the data processes are carried out both on network and devic"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p8#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 8, "snippet": "however, edge ai implements cloudlet technology, which is small - scale cloud storage located at the network ’ s edge. cloudlet technology enhances mo"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p9#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 9, "snippet": "technologies influencing edge ai development developments in knowledge such as data science, machine learning, and iot development have a more signifi"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p10#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 10, "snippet": "in august 2017, intel acquired mobileye, a tel aviv - based vision - safety technology company, for $ 15. 3 billion. recently, baidu, a chinese multin"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p11#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 11, "snippet": "challenges of edge ai poor data quality : poor quality of data of major internet service providers worldwide stands as a major hindrance for the resea"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p12#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 12, "snippet": "limited machine learning power : machine learning requires greater computational power on edge computing hardware platforms. in edge ai infrastructure"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p13#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 13, "snippet": "use cases for edge ai virtual assistants virtual assistants like amazon ’ s alexa or apple ’ s siri are great benefactors of developments in edge ai, "}
{"id": "AAI/Module 2_Edge AI (2).pptx#p14#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 14, "snippet": "automated optical inspection automated optical inspection plays a major role in manufacturing lines. it enables the detection of faulty parts of assem"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p15#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 15, "snippet": "autonomous vehicles the quicker and accurate decision - making capability of edge ai - enabled autonomous vehicles results in better identification of"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p16#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 16, "snippet": "use cases by intel intel has collaborated with numerous industry partners and customers to implement thousands of edge computing solutions here are fo"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p17#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 17, "snippet": "retail : edge computing can enhance retail accuracy using sensors and cameras and make supply chains and product development more efficient it can als"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p18#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 18, "snippet": "industrial : edge computing can support industry 4. 0 by integrating digital and physical technologies for more flexible and responsive manufacturing "}
{"id": "AAI/Module 2_Edge AI (2).pptx#p19#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 19, "snippet": "healthcare : edge computing can improve healthcare outcomes with inpatient and outpatient monitoring, telehealth services, and ai inference on medical"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p20#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 20, "snippet": "in the future edge computing will continue to shape the way we process and analyze data this will be driven by a number of factors including ” increas"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p21#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 21, "snippet": "improved performance : as edge computing technology continues to evolve, performance is expected to improve this will make edge computing even more ef"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p22#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 22, "snippet": "increased security : as edge computing continues to gain popularity, security will become a top priority companies will need to ensure that their edge"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p23#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 23, "snippet": "the evolution of edge devices with enhanced processing, computation, analytics, and storage capabilities poses a new challenge : deciding on the best "}
{"id": "AAI/Module 2_Edge AI (2).pptx#p24#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 24, "snippet": "and beyond apart from all of the use cases discussed above, edge ai can also play a crucial role in facial recognition technologies, enhancement of in"}
{"id": "AAI/Module 2_Edge AI (2).pptx#p25#c1", "source": "AAI/Module 2_Edge AI (2).pptx", "page": 25, "snippet": "references the future of computing : how edge computing is revolutionizing the tech industry - telecom reseller /? cid = em & source = elo & campid = "}
{"id": "AAI/Module 5_Ethics of AI.pptx#p1#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 1, "snippet": "module 5 ethics of artificial intelligence"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p2#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 2, "snippet": "ethics of artificial intelligence philosophical issues raised by current and future ai systems, ethics principles of artificial intelligence : transpa"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p3#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 3, "snippet": "what is the philosophical issue of ai? the philosophy of artificial intelligence attempts to answer such questions as follows : can a machine act inte"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p4#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 4, "snippet": "1. on the question of intentionality — can a machine have mind, consciousness. if yes, then can it intentionally harm humans. whether computers have i"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p5#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 5, "snippet": "3 ) can intentionality be programmed? searle believes that “ the way the brain functions to produce the heart cannot be a way of simply operating a co"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p6#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 6, "snippet": "if the program can include grammar and semantics together, do we need to distinguish between grammar and semantics? searle ’ s point is that even if t"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p7#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 7, "snippet": "the information that makes the computer “ know everything. ” however, at that time, can we still be like searle said, artificial intelligence is not i"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p8#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 8, "snippet": "relying on different foundations to form the same function, artificial intelligence is just a special way of realizing our human intelligence searle u"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p9#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 9, "snippet": "on the question of intelligence — is it possible for machines to solve problems using intelligence the same way humans do or there is a limit to which"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p10#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 10, "snippet": "in order to solve this problem, computer scientists did not try to reverse engineer human intelligence, but developed a new way of thinking for artifi"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p11#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 11, "snippet": "today, people generally think that smart computers will take away our work before you finish your breakfast, it has already completed your week ’ s wo"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p12#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 12, "snippet": "you know the input data and the results, but you don ’ t know how the box in front of you reached the conclusion caruana said, “ we now have two diffe"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p13#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 13, "snippet": "3. on the question of ethics — can machines be dangerous for humans, how scientists will make sure that machines behave ethically and will not be a th"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p14#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 14, "snippet": "in order to avoid being obstructed from hostile forces, “ close ” these weapons programs will be designed to be extremely complex, and therefore peopl"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p15#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 15, "snippet": "if the mission of a super - intelligent system is an ambitious geoengineering project, the side effect may be the destruction of the ecosystem, and hu"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p16#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 16, "snippet": "the signal is not only determined by the image on the observer ’ s retina even two people watching the same object will have different visual experien"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p17#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 17, "snippet": "they contain theories with varying degrees of universality and complexity. ” this shows that observation requires theory. science needs theory as the "}
{"id": "AAI/Module 5_Ethics of AI.pptx#p18#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 18, "snippet": "references top 5 philosophical issues of artificial intelligence ( ai ), https : / / intelligence - in - aiml. medium. com / top - 5 - philosophical -"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p19#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 19, "snippet": "what is ai weaponization? weaponized ai technologies do not face the same barriers as human soldiers, allowing them to traverse all kinds of terrain a"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p20#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 20, "snippet": "while the reality of anthropomorphic robots may still remain in the distant future, many of us do not even realize the extent to which we interact wit"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p21#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 21, "snippet": "the weaponization of ai is one of the biggest threats facing the international community weaponized ai technologies do not face the same barriers as h"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p22#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 22, "snippet": "laws do not need human operation and at this point have been integrated into nearly all areas of warfare the united states armed forces utilizes laws "}
{"id": "AAI/Module 5_Ethics of AI.pptx#p23#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 23, "snippet": "as such, the issue of the weaponization of artificial intelligence must be confronted, and it is the responsibility of disec to find an innovative sol"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p24#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 24, "snippet": "some questions delegates may wish to consider include : ● how can a balance be struck which allows the positives of ai to continue to be enjoyed by ci"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p25#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 25, "snippet": "how is ai a threat to human dignity? it feeds into the popular notion that our dignity and worth is solely dependent on our usefulness to society rath"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p26#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 26, "snippet": "ai - enabled terrorism : artificial intelligence will change the way conflicts are fought from autonomous drones, robotic swarms, and remote and nanor"}
{"id": "AAI/Module 5_Ethics of AI.pptx#p27#c1", "source": "AAI/Module 5_Ethics of AI.pptx", "page": 27, "snippet": "social manipulation and ai bias : so far, ai is still at risk for being biased by the humans that build it if there is bias in the data sets the ai is"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p1#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 1, "snippet": "what is edge ai? chatgpt said : chatgpt edge ai refers to the deployment of artificial intelligence ( ai ) algorithms directly on devices at the \" edg"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p2#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 2, "snippet": "lag associated with sending data to the cloud for processing, allowing for faster and more accurate responses in dynamic environments like manufacturi"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p3#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 3, "snippet": "6. empowering remote and offline locations • reliable operations in remote areas : edge ai enables enterprises to operate in remote or unstable networ"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p4#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 4, "snippet": "• immediate data processing : since data is processed locally on the edge device, the need to send it to the cloud and wait for a response is eliminat"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p5#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 5, "snippet": "• resilience to network failures : if connectivity to the cloud is lost, edge devices can continue functioning and making decisions locally. for criti"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p6#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 6, "snippet": "2. healthcare : wearable health devices can monitor vital signs and detect abnormalities in real - time, alerting healthcare providers immediately if "}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p7#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 7, "snippet": "2. 5g networks • ultra - low latency and high bandwidth : the advent of 5g technology is a game - changer for edge ai, as it enables high - speed data"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p8#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 8, "snippet": "important in industries like agriculture, smart cities, and healthcare, where devices need to analyze sensor data on - site. • edge - to - cloud commu"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p9#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 9, "snippet": "conclusion : the convergence of advanced edge ai hardware, 5g networks, tinyml, and sophisticated software frameworks is fueling the rapid development"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p10#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 10, "snippet": "• maintaining accuracy : compressing models or reducing their complexity to fit edge devices may result in a loss of accuracy, especially in use cases"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p11#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 11, "snippet": "handled locally on edge devices and which should be offloaded to the cloud for more in - depth analysis. finding the right balance can be difficult, a"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p12#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 12, "snippet": "the challenges of edge ai are primarily driven by the limitations of hardware, power, and connectivity on edge devices, as well as the complexities of"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p13#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 13, "snippet": "• continuity of service : even if the device loses network access, edge ai allows virtual assistants to continue functioning for local tasks and comma"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p14#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 14, "snippet": "• local data storage : for enterprise virtual assistants used in corporate environments, edge ai ensures that sensitive corporate data ( e. g., meetin"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p15#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 15, "snippet": "chatgpt said : chatgpt edge ai for autonomous vehicles is a powerful use case that leverages real - time, on - device processing to enable self - driv"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p16#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 16, "snippet": "• autonomous operation without internet : edge ai allows autonomous vehicles to operate independently of cloud connectivity. this is critical in areas"}
{"id": "AAI/AAI Unit 2 chatgpt(Read in short).pdf#p17#c1", "source": "AAI/AAI Unit 2 chatgpt(Read in short).pdf", "page": 17, "snippet": "experiences of other vehicles in the same region. this continuous learning ensures that the vehicle ’ s ai system is always improving, even while oper"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p1#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 1, "snippet": "unit 4 : ai in iot cyber physical systems introduction, ai and internet of things, ai enabled iot - cps, cognitive ai, cognitive ai and iot - cps, exa"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p2#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 2, "snippet": "the cyber - physical system ( cps ) the cyber - physical system ( cps ) is the key concept of industry 4. 0, which the german government advocates for"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p3#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 3, "snippet": "cyber - physical systems ( cps ) cyber - physical systems ( cps ) are smart systems that include engineered interacting networks of physical and compu"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p4#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 4, "snippet": "what is cyber physical system? cyber refers to computing, networking, and controlling that is distinct, switched, and logical. physical systems are na"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p5#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 5, "snippet": "figure 1 the 3c minimal requirements a for a cyber physical system unit 4 : ai in iot 5"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p6#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 6, "snippet": "figure 2 main building blocks of a cyber physical system unit 4 : ai in iot 6"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p7#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 7, "snippet": "cyber physical production systems ( cpps ) building blocks unit 4 : ai in iot 7 cyber - physical production systems are basically a particular class o"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p8#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 8, "snippet": "figure 3 cyber physical production system building blocks a cyber physical production system is composed of the following building blocks ( for more d"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p9#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 9, "snippet": "cyber - physical systems architecture requirements andrelated aspects unit 4 : ai in iot 9 figure 4 three main characteristics of cyber physical syste"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p10#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 10, "snippet": "object abstraction represents the ability to provide virtual representations for physical objects ( devices, components, sub - systems, systems, etc )"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p11#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 11, "snippet": "artificial intelligence in internet of things ( aiot ) ai is a technology that targets at making computers do human - like reasoning. this development"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p12#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 12, "snippet": "internet of things even a few decades back, nobody could have imagined having a video chat with their families in a different continent. nowadays, it "}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p13#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 13, "snippet": "fig 1. different fields merging into iot unit 4 : ai in iot 13"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p14#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 14, "snippet": "fig 2. venn diagram for iot, ios and ioe unit 4 : ai in iot 14"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p15#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 15, "snippet": "ai enabled iot iot is a vast concept encompassing too many sensors, actuators, data storage, and data processing capabilities interconnected by the in"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p16#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 16, "snippet": "1 voice assistants these are cloud - based voice services which act as table - top personal assistants for users. they perform various tasks through t"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p17#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 17, "snippet": "2. robotics robots : recent advancements in this ﬁeld of robotics have led to the creation of robots who have increased human likeness and are able to"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p18#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 18, "snippet": "pepper from softbank robotics is a human - shaped robot which is termed as a humanoid companion which can interact with humans. it is able to understa"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p19#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 19, "snippet": "sophia from hanson robotics is a social humanoid robot which is incredibly human - like and can express emotions through > 50 facial expressions. duri"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p20#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 20, "snippet": "application of natural language processing, computer vision, shape recognition, object recognition, detection and tracking, block - chain technology t"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p21#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 21, "snippet": "3. smart devices in an iot apart from the voice assistants and robots, there are sos / devices that are present which make the task simpler for humans"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p22#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 22, "snippet": "smart lights by deako can be controlled remotely through smartphones and alexa or google assistant. they are connected via the internet and can receiv"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p23#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 23, "snippet": "4 industrial iot apart from being used inside smart homes, iot has a huge application area in the various industrial sectors. these solutions perform "}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p24#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 24, "snippet": "thus, the opportunities and potential of both ai and iot can advance when they are combined. as iot generates data, ml and bda carry the potential to "}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p25#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 25, "snippet": "iot – cps now that we have established a clear relationship between iot, cps, and the related terms around it, the ecosystem of these technologies mat"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p26#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 26, "snippet": "fig. 3 iot architecture tree unit 4 : ai in iot 26"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p27#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 27, "snippet": "any so can also have a limited data storage capability and limited data processing capability as well. for example, a smart watch signals to walk when"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p28#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 28, "snippet": "all of the data is not needed to be handled at one place or at one time. so, smaller relevant portions of the data are extracted and dealt with as and"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p29#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 29, "snippet": "1 smart objects to catch up with such a substantial concept, we will require at least millions ( or more ) of data generating sos. these will act like"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p30#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 30, "snippet": "a pe or thing can be represented in the cyber world by a de by its digital proxy ( dp ). dps can be viewed as users in cyber world, just like our soci"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p31#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 31, "snippet": "data generated by these sos need to be transmitted through wireless technologies, and the objects themselves should be clearly identiﬁable. all transm"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p32#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 32, "snippet": "the objective of being autonomous, making decisions, and taking actions would never be fulﬁlled without having proper data storage and processing unit"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p33#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 33, "snippet": "3 communication networks continuous analysis of big data over these platforms demands an efﬁcient and reliable network structure. virtualization of ne"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p34#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 34, "snippet": "some people believe that for long - distance operations, 5g networks can meet all the requirements of iot devices. these 5g networks will be faster an"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p35#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 35, "snippet": "in those cases, wi - fi connectivity is often an obvious choice for people who want wireless connectivity in a local area. typically, it has a data tr"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p36#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 36, "snippet": "fig. 4 example of real world to virtual world mapping unit 4 : ai in iot 36"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p37#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 37, "snippet": "4. security unit 4 : ai in iot 37"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p38#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 38, "snippet": "ai enabled iot - cps unit 4 : ai in iot 38"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p39#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 39, "snippet": "ai - enabled iot - cps is designed to enhance healthcare services for elderly individuals. the system aims to boost the life quality of elderly indivi"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p40#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 40, "snippet": "recent advancements in ai - enabled iot - cps for healthcare, focusing on disease detection and patient well - being enhancement. the integration of a"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p41#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 41, "snippet": "cognitive ai a cognitive computer or system learn at scale, reasons with purpose and interacts with humans naturally. rather than being explicitly pro"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p42#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 42, "snippet": "what is cognitive computing ( cc )? cognitive computing refers to individual technologies that perform specific tasks to facilitate human intelligence"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p43#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 43, "snippet": "also, you can refer to cognitive computing as : understanding and simulating reasoning understanding and simulating human behavior using cognitive com"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p44#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 44, "snippet": "how cognitive computing works? cognitive computing systems synthesize data from various information sources while weighing context and conflicting evi"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p45#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 45, "snippet": "cognitive ai example unit 4 : ai in iot 45"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p46#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 46, "snippet": "using computer systems to solve problems that are supposed to be done by humans require huge structured and unstructured data. with time, cognitive sy"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p47#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 47, "snippet": "key attributes adaptive : cognitive systems must be flexible enough to understand the changes in the information. also, the systems must be able to di"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p48#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 48, "snippet": "cognitive computing vs ai the technologies behind cognitive computing are similar to the technologies behind ai. these include machine learning, deep "}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p49#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 49, "snippet": "cc vs ai cognitive computing artificial intelligence cognitive computing focuses on mimicking human behavior and reasoning to solve complex problems. "}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p50#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 50, "snippet": "cognitive ai : use case ai is performing human tasks such as speech recognition, decision - making, etc. the future is based on ai technology so under"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p51#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 51, "snippet": "unit 4 : ai in iot 51"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p52#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 52, "snippet": "whereas, a cognitive assistant suggests potential career paths to the job seeker, besides furnishing the person with important details like additional"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p53#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 53, "snippet": "applications of cognitive ai smart iot : this includes connecting and optimizing devices, data and the iot. but assuming we get more sensors and devic"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p54#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 54, "snippet": "references naoufel boulila, guidelines for modeling cyber - physical systems – a three - layered architecture for cyber physical systems, siemens germ"}
{"id": "AAI/Unit 4_AI in IoT_rev1.pptx#p55#c1", "source": "AAI/Unit 4_AI in IoT_rev1.pptx", "page": 55, "snippet": "r. ferdousi, m. a. hossain and a. e. saddik, \" early - stage risk prediction of non - communicable disease using machine learning in health cps, \" in "}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p1#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 1, "snippet": "you said : what is xai? chatgpt said : chatgpt xai stands for explainable artificial intelligence. it refers to ai systems that are designed to provid"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p2#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 2, "snippet": "• logistic regression : similar to linear regression, but used for classification tasks. the coefficients can explain how each input variable influenc"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p3#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 3, "snippet": "• prototype - based explanations : these methods explain predictions by comparing them to representative examples ( prototypes ) from the training dat"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p4#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 4, "snippet": "2. regulatory compliance : • meeting legal requirements : many industries face stringent regulations that require transparency and accountability in d"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p5#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 5, "snippet": "• model improvement : by understanding how ai models make decisions, data scientists and engineers can more easily refine and optimize these models. x"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p6#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 6, "snippet": "• legal : contract analysis, compliance monitoring, risk assessments. conclusion : explainable ai ( xai ) is essential in business because it builds t"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p7#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 7, "snippet": "• use case : in a fraud detection system, a complex deep learning model might be used to flag fraudulent transactions. by training a decision tree as "}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p8#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 8, "snippet": "• use case : in an insurance claim approval system, if a claim is denied, a counterfactual explanation might show that if the \" damage amount \" had be"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p9#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 9, "snippet": "aspect ante hoc interpretability post hoc interpretability definition interpretability built directly into the model ( intrinsic ). the model is simpl"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p10#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 10, "snippet": "aspect ante hoc interpretability post hoc interpretability transparency transparent by nature ; easy to understand, explain, and audit. not inherently"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p11#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 11, "snippet": "• gams allow for non - linear relationships between features and the outcome, but these relationships are modeled in a way that remains interpretable."}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p12#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 12, "snippet": "chatgpt said : chatgpt stochastic and - or graph ( s - aog ) models are a type of structured model used to represent and interpret complex, hierarchic"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p13#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 13, "snippet": "3. probabilistic reasoning : • s - aogs include probability distributions at or - nodes, which capture the likelihood of choosing different alternativ"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p14#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 14, "snippet": "• inherent structure for interpretability : s - aogs are interpretable by design due to their compositional, hierarchical structure. every decision th"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p15#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 15, "snippet": "• for example, a rule - based or linear component may explain decisions based on simple patterns, while another part of the model may use deep learnin"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p16#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 16, "snippet": "between features. the wide part is inherently interpretable, while the deep part boosts accuracy by capturing complex, hierarchical relationships. • e"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p17#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 17, "snippet": "lime ( local interpretable model - agnostic explanations ) is a widely used post - hoc interpretability method. post - hoc methods, like lime, are app"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p18#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 18, "snippet": "3. model predictions for perturbed data : • the black - box model makes predictions for these perturbed instances. 4. fitting the surrogate model : • "}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p19#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 19, "snippet": "3. intuitive output : the explanations are often easy to interpret, with clear feature importance and direction of influence. limitations of lime : 1."}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p20#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 20, "snippet": "• like lime and shap, beta is model - agnostic, meaning it can be applied to any black - box model, such as deep learning models, random forests, and "}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p21#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 21, "snippet": "3. explanation : • the identified teaching set serves as the explanation. instead of providing feature importance or local approximations, beta explai"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p22#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 22, "snippet": "3. local explanations : beta provides local explanations, meaning it explains individual predictions rather than offering a global understanding of ho"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p23#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 23, "snippet": "steps in conducting post hoc lrp : 1. data preparation : clean and organize the dataset to ensure it ’ s suitable for analysis. 2. model specification"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p24#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 24, "snippet": "• the idea is to use these simpler models to gain insights into the behavior and decision - making processes of the more complex model. techniques for"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p25#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 25, "snippet": "• approximation accuracy : the simpler model may not perfectly capture the nuances of the black box model, leading to potential misinterpretations. • "}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p26#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 26, "snippet": "• lrp - αβ rule : a combination of two parameters ( αα and ββ ) to control how relevance is split among incoming and outgoing connections. • lrp - ε r"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p27#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 27, "snippet": "chatgpt explainable artificial intelligence ( xai ) is increasingly being integrated into various applications, including systems that provide driving"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p28#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 28, "snippet": "• interactive explanations : allow users to interactively explore routing decisions, such as dragging the route or selecting alternative paths, while "}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p29#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 29, "snippet": "2. surgical planning • preoperative analysis : ai can assist in evaluating patient data to recommend surgical approaches. explainable ai can clarify w"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p30#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 30, "snippet": "• augmented decision - making : by elucidating how ai reaches conclusions, healthcare professionals can learn to better integrate ai tools into their "}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p31#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 31, "snippet": "label = ' regression line ' ) plt. xlabel ( ' hours studied ' ) plt. ylabel ( ' scores ' ) plt. title ( ' linear regression example ' ) plt. legend ( "}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p32#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 32, "snippet": "loss = tf. keras. losses. sparsecategoricalcrossentropy ( from _ logits = true ), metrics = [ ' accuracy ' ] ) model. fit ( train _ images, train _ la"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p33#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 33, "snippet": "• analyzing training data : white box approaches enable a detailed examination of the training data, helping identify any biases present ( e. g., unde"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p34#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 34, "snippet": "• partial dependence plots : show the relationship between features and predictions, allowing users to see how changes in input values impact outcomes"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p35#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 35, "snippet": "• example : predicting house prices based on features like size, location, and number of bedrooms. • unsupervised learning : in this case, the model i"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p36#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 36, "snippet": "4. common algorithms • linear regression : used for predicting continuous outcomes based on linear relationships between features. • decision trees : "}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p37#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 37, "snippet": "below is a structured approach to creating an explainable ai solution, using python as the primary programming language and leveraging popular librari"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p38#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 38, "snippet": "• shap ( shapley additive explanations ) : provides a unified measure of feature importance and helps understand the impact of each feature on the mod"}
{"id": "AAI/AAI Unit1 chatgpt(Read in short).pdf#p39#c1", "source": "AAI/AAI Unit1 chatgpt(Read in short).pdf", "page": 39, "snippet": "conclusion building an explainable ai solution from scratch involves a structured approach, emphasizing not only model accuracy but also transparency "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p1#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 1, "snippet": "leonida gianfagna antonio di cecco explainable al with python % ) springer"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p2#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 2, "snippet": "leonida gianfagna and antonio di cecco explainable ai with python 1st ed. 2021"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p3#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 3, "snippet": "leonida gianfagna cyber guru, rome, italy antonio di cecco school of ai italia, pescara, italy isbn 978 - 3 - 030 - 68639 - 0e - isbn 978 - 3 - 030 - "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p4#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 4, "snippet": "this springer imprint is published by the registered company springer nature switzerland ag the registered company address is : gewerbestrasse 11, 633"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p5#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 5, "snippet": "contents 1 the landscape 1. 1 examples of what explainable ai is 1. 1. 1 learning phase 1. 1. 2 knowledge discovery 1. 1. 3 reliability and robustness"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p6#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 6, "snippet": "2. 1. 2 xai evaluation from “ human in the loop perspective ” 2. 2 how to make machine learning models explainable 2. 2. 1 intrinsic explanations 2. 2"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p7#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 7, "snippet": "4. 2. 3 properties of explanations 4. 3 the road to kernelshap 4. 3. 1 the shapley formula 4. 3. 2 how to calculate shapley values 4. 3. 3 local linea"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p8#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 8, "snippet": "5. 2. 3 rectiied activations ( and batch normalization ) 5. 2. 4 saliency maps 5. 3 opening deep networks 5. 3. 1 different layer explanation 5. 3. 2 "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p9#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 9, "snippet": "7 adversarial machine learning and explainability 7. 1 adversarial examples ( aes ) : crash course 7. 1. 1 hands - on adversarial examples 7. 2 doing "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p10#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 10, "snippet": "( 1 ) ( 2 ) © the author ( s ), under exclusive license to springer nature switzerland ag 2021 l. gianfagna, a. di cecco, explainable ai with python h"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p11#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 11, "snippet": "so, the root of explainable ai was at the very beginning of artiicial intelligence, albeit not in the current form as a speciic discipline. the key to"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p12#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 12, "snippet": "we ’ ll start with some examples to get into the context. in particular, we will go through three easy cases that will show different but fundamental "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p13#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 13, "snippet": "fig. 1. 1ml classiication of wolves and dogs ( singh 2017 ) after the training, the algorithm learned to distinguish the classes with remarkable accur"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p14#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 14, "snippet": "fig. 1. 2classiication mistake. ( a ) husky classiied as wolf. ( b ) explanation. ( ribeiro et al. 2016 ) so maybe giving a model the ability to expla"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p15#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 15, "snippet": "learned the bias. a good model must be fair, and fairness is also one of the goals of explainable ai. 1. 1. 3 reliability and robustness now let ’ s l"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p16#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 16, "snippet": "1. 1. 4 what have we learnt from the three examples as promised, let ’ s critically think about the three examples above to see how they introduce dif"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p17#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 17, "snippet": "explainable. we will deep dive their meaning in the following starting from sect. 1. 4 of this chapter. 1. 2 machine learning and xai without going th"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p18#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 18, "snippet": "for our purposes, we need to focus on “ without being explicitly programmed. ” in the old world of software, the solution to whatever problem was dema"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p19#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 19, "snippet": "before getting into details. there are three different main categories of machine learning systems based on the type of training that is needed ( fig."}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p20#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 20, "snippet": "deep neural networks ( dnns ) are the machine learning systems that are producing the most successful results and performance. given the categories ab"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p21#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 21, "snippet": "fig. 1. 7loan approval, good and bad cases with linear boundary but in a more complex model, we must face a trade - off problem. in fig. 1. 8 we may s"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p22#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 22, "snippet": "in linear models, you can easily say the effect of an increase or a decrease of a speciic feature that is not generally possible for nonlinear cases. "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p23#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 23, "snippet": "fig. 1. 9qualitative trend and relation of learning performance vs explainability the two sets of points represent the qualitative trends for today an"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p24#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 24, "snippet": "while they refer to slightly different aspects of the same concept to be clariied. 1. 3 the need for explainable ai the picture that is emerging from "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p25#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 25, "snippet": "information is a must in speciic cases ( legal or medical among the others ). model debugging : to guarantee reliability and robustness, the ml model "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p26#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 26, "snippet": "any signiicant impact. and this is pretty evident if we look at the ai adoption speed in the consumer market in which there is a large diffusion of re"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p27#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 27, "snippet": "humans 16 interface - interpretability a = mere a > : oe = piensa sae on aos am evaluate \\ black box > model. learn 123456 2354667 32223332 \" oe 25555"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p28#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 28, "snippet": "fig. 1. 10from world to humans through machine learning interpretability methods should bridge the gap between predictions or decisions generated by t"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p29#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 29, "snippet": "predictions, that is, what machine learning does, you are almost relying on correlation to ind patterns. but you are not going into the direction of b"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p30#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 30, "snippet": "to get explanations. interpretations are an element of an explanation but do not exhaust it. to get a an analogy from science, the same theory, for ex"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p31#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 31, "snippet": "fig. 1. 13an illustration of the error surface of machine learning model if we model the error as a mountain landscape, each choice of parameters make"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p32#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 32, "snippet": "boiling point, but the physics of the phase transition would not be explainable ( fig. 1. 14 ). fig. 1. 14water phase transition diagram with two diff"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p33#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 33, "snippet": "table 1. 1difference between interpretability and explainability in terms of the questions to answer for the two different scopes question interpretab"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p34#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 34, "snippet": "and try to present a framework for a standard approach to ai ( chap. 8 ) in which this distinction will rise again. 1. 5 making machine learning syste"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p35#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 35, "snippet": "fig. 1. 15machine learning pipeline with focus on xai, blocks inside the ellipse the main point of xai is to make sense of the output producing explan"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p36#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 36, "snippet": "fig. 1. 16different approaches to make a ml model explainable as per the picture below, the xai mental model is a low that, given a ml model, provides"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p37#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 37, "snippet": "fig. 1. 17different approaches to make a ml model explainable with focus on the feedback from the external to further improve xai figure 1. 17 shows t"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p38#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 38, "snippet": "of trading off we prefer to try to share a mindset, a way of thinking with practical methods instead of just a list of references to xai methods and t"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p39#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 39, "snippet": "fig. 1. 19visual map of the main concepts and topics across the book as discussed, there are different levels of possible explanations that could be r"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p40#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 40, "snippet": "aimed at fooling the ml models to change their predictions. the ultimate goal is to look at xai from different perspectives to successfully deal with "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p41#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 41, "snippet": "fig. 1. 20is knowing physics really necessary to play football? ( chaoskomori 2003 ) to make these arguments more practical, let ’ s suppose that we a"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p42#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 42, "snippet": "student should be able to solve given the required level of knowledge. but you would not care about “ how ” the student derived his answers. and this "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p43#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 43, "snippet": "model somehow ) to reach the needed level of interpretability and start to trust the model itself. also we will see how asking for a model to be inter"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p44#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 44, "snippet": "gilpin, l. h., bau, d., yuan, b. z., bajwa, a., specter, m., & kagal, l. ( 2018 ). explaining explanations : an overview of interpretability of machin"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p45#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 45, "snippet": "( 1 ) ( 2 ) © the author ( s ), under exclusive license to springer nature switzerland ag 2021 l. gianfagna, a. di cecco, explainable ai with python h"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p46#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 46, "snippet": "practical example in which the goal is to forecast sales of a product depending on the age of the customers. before jumping to a general presentation "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p47#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 47, "snippet": "fig. 2. 1the human in the loop improves the performance of an ai classiier taking part in the training process if the classiier is not conident enough"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p48#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 48, "snippet": "most powerful chess ai at the time. where the three weak computers had different recommendations, the humans may interact with the system to do furthe"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p49#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 49, "snippet": "fig. 2. 2the pareto principle suggests that an eficient ml system must have a 20 % of creative ( human ) effort humans have an active role not only as"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p50#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 50, "snippet": "as outlined by gilpin et al. ( 2018 ), an explanation can be assessed by two main features : its interpretability and its completeness. the main objec"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p51#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 51, "snippet": "fig. 2. 3categories of explainability based on the role played by the human in the evaluation application - grounded evaluation ( real humans – real t"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p52#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 52, "snippet": "set of models that have been validated by human - grounded experiments as explainable which are then used as a proxy for the real model. they are used"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p53#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 53, "snippet": "fig. 2. 4categories of explainability based on the role played by the human in the evaluation ; arrow indicates increasing costs and complexity 2. 2 h"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p54#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 54, "snippet": "fig. 2. 5purchases of smartphones depending on age of the customer looking at the diagram, it is evident how the purchases are scattered in the plane "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p55#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 55, "snippet": "let ’ s get back now to our example but now trying to predict the sales. figure 2. 6 shows the case of a model trying to predict the number of purchas"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p56#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 56, "snippet": "fig. 2. 7with nonlinear non - monotonic function, we lose a global easily explainable model in favor of an accuracy improvement as you may easily real"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p57#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 57, "snippet": "details ), the idea behind this example is just to show how improving performance in general may challenge explainability, but we will see in chap. 5 "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p58#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 58, "snippet": "fig. 2. 8categories of different xai techniques, du et al. ( 2019 ) all three schemes are about analyzing a deep neural network ( but the type of mode"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p59#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 59, "snippet": "linear regression is used to model the dependence of a target ( y ) from a set of features ( x 1 … x k ) with a linear relation : ( 2. 1 ) equation ( "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p60#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 60, "snippet": "to deepen the concepts, eq. ( 2. 2 ) solves two theoretical problems in machine learning : it is the simplest equation that maximizes the likelihood f"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p61#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 61, "snippet": "fig. 2. 9a decision tree applied to the titanic dataset. it is easy to explain how the model predicts the probabilities of survival the tree shows the"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p62#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 62, "snippet": "to the reader who is acquainted with machine learning, we remember decision trees do their splitting to achieve the maximum purity in the target varia"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p63#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 63, "snippet": "we already discussed from another perspective in sect. 2. 2 discussing figs. 2. 6 and 2. 7. ( yes, another time the myth seems to be conirmed about th"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p64#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 64, "snippet": "we cannot stress enough the importance of model - agnostic explanations from a practical point of view. suppose you are called to explain a model crea"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p65#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 65, "snippet": "between the different approaches and getting an intuition of the main concepts. at this point of our journey, we are still missing the practical skill"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p66#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 66, "snippet": "fig. 2. 10model to predict purchases. dashed lines with different slopes can be applied to explain locally the predictions of the model the nonlinear,"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p67#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 67, "snippet": "let ’ s try to be very clear about this point with our example. as we said, the scenario is that our marketing department has been asked to forecast t"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p68#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 68, "snippet": "fig. 2. 11a recap of explainations. we irst divide models in intrinsically explainable and black - box models. we then add the scope of explainations "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p69#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 69, "snippet": "concepts do not change, but we prefer to use our low ( fig. 2. 6 ) in the rest of the book in which the irst split is made between the ml models that "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p70#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 70, "snippet": "as we said in the previous sections regarding xai taxonomy, it is not currently possible to have a quantitative assessment of the xai methods and gene"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p71#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 71, "snippet": "important in terms of being a potential bottleneck to provide explanations in case of huge complexity. the group below refers to properties of individ"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p72#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 72, "snippet": "depending on the main goal : achieve a detailed description of the system, or privilege easy explanations for the audience. place xai methods in a pro"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p73#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 73, "snippet": "( 1 ) ( 2 ) © the author ( s ), under exclusive license to springer nature switzerland ag 2021l. gianfagna, a. di cecco, explainable ai with pythonhtt"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p74#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 74, "snippet": "fig. 3. 1xai low : intrinsic explainable models we will focus on the concepts so that people may translate the same lows frompython to other programmi"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p75#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 75, "snippet": "purely ml perspective but also from a more speciic xai angle. ( the loss function inthe case of linear regression is also called empirical risk. ) a s"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p76#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 76, "snippet": "fig. 3. 3smooth loss function in a generic parameter space finding the minimum of loss in the parameters is equivalent to choosing the bestmodel, that"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p77#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 77, "snippet": "for example, if locally the loss is increasing in space parameter, we decrease theparameter value. from the courses on machine learning, we know gd is"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p78#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 78, "snippet": "scale of 0 – 10, depending on some speciic set of features. suppose a wine producerheard about the “ miracles ” that machine learning may perform to i"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p79#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 79, "snippet": "as we said, we will use linear regression to build our ml model and getpredictions on wine quality. let ’ s repeat very quickly how linear regression "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p80#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 80, "snippet": "df = pd. read _ csv ( ' winequality - red. csv ' ) x = df. iloc [ :, : - 1 ]. values y = df. iloc [ :, - 1 ]. values x _ train, x _ test, y _ train, y"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p81#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 81, "snippet": "with minimum effort and amount of code, you as the data scientist are already inthe position of providing feedback to the wine producer. he wanted to "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p82#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 82, "snippet": "looking at the table values, we see how volatile acidity and alcohol are thefeatures that are more correlated with quality ( negatively for volatile a"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p83#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 83, "snippet": "import seaborn as sns sns. heatmap ( df. corr ( ), annot = true, linewidths =. 5, ax = ax, cmap = \" twilight \" ) plt. show ( ) looking at the picture,"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p84#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 84, "snippet": "the usual method to overcome multicollinearity is to exclude features highlycorrelated or to whiten the features via principal component analysis. we "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p85#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 85, "snippet": "the idea is simple but powerful. in lasso regularization, we add a positive term inl1 norm to force the weights associated with less signiicant featur"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p86#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 86, "snippet": "for example with a choice of alpha = 0. 045, we ind and show what are the irstsix features by importance, and in fact “ density ” has been found as on"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p87#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 87, "snippet": "fig. 3. 7linear regression to classify iris lower type based on sepal length only ) the threshold is the limit that we set to 0. 5 as to get the proba"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p88#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 88, "snippet": "reason why we need to switch to logistic regression we already mentioned in theprevious chapter. before jumping to the details of our lower classiicat"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p89#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 89, "snippet": "as we see the formula is back to linear but with log terms. the term is named log - odds or logit where the odds mean the ratio of probability of the "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p90#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 90, "snippet": "species _ map = { 0 : ' setosa ', 1 : ' versicolor ', 2 : ' virginica ' } df [ ' species _ names ' ] = df [ ' species _ id ' ]. map ( species _ map ) "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p91#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 91, "snippet": "[ ‘ versicolor ’ ‘ setosa ’ ‘ virginica ’ ‘ versicolor ’ ‘ versicolor ’ ‘ setosa ’ ‘ versicolor ’ ‘ virginica ’ ‘ versicolor ’ ‘ versicolor ’ ‘ virgin"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p92#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 92, "snippet": "fig. 3. 10 iris classiication based on sepal _ length and sepal _ width features marker _ map = [ ' o ', ' s ', ' ^ ' ] unique = np. unique ( df [ ' s"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p93#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 93, "snippet": "fig. 3. 11 iris classiication with sepal length and petal width instead of sepal width produced a better split y = np. array ( y ) marker _ map = [ ' "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p94#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 94, "snippet": "w, b output : ( array ( [ [ 1. 3983599, 3. 91315269 ] ] ), array ( [ - 10. 48150545 ] ) ) what do these numbers mean? they are the coeficient of the l"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p95#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 95, "snippet": "equations ( 3. 13 ) and ( 3. 14 ) express how the probability of iris being setosa ornot changes for an increment of 1 cm in sepal length and petal wi"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p96#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 96, "snippet": "as shown in fig. 3. 10, we may point to sepal length and petal width as thefeatures that are the most important ones for our classiication problem. th"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p97#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 97, "snippet": "learn library. cart was introduced by breiman in 1984 and is the irst “ universal ” algorithm in the sense that it can accomplish both classiication a"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p98#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 98, "snippet": "we know that for a perfectly classiied item, impurity would be zero. as wealready said in the previous chapter, a node is 100 % impure when it is spli"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p99#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 99, "snippet": "in the case of regression, we build regression trees, and the impurity is simplyvariance. to train a regression tree is to ind in each node the approp"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p100#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 100, "snippet": "using the shufled dataset. the predictions are expected to worsen because of theshufling ( dataset has been hacked! ). we repeat the shufling for each"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p101#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 101, "snippet": "and split features and target columns : # split the data into independent ' x ' and dependent ' y ' variables x _ train = titanic. iloc [ :, 1 : 4 ]. "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p102#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 102, "snippet": "fig. 3. 15decision tree to predict survival rates based on the different features we have two ways of assessing feature importance in dt. one is ranki"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p103#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 103, "snippet": "look at the decision tree graph. each node ( rectangle ) contains both survived andnot - survived passengers. the value array contains the number of s"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p104#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 104, "snippet": "modiication even contrastive examples. with contrastive explanations, we createdescriptions based on the missing abnormalities. we can look to classii"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p105#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 105, "snippet": "z = trained _ model. predict ( np. c _ [ xx. ravel ( ), yy. ravel ( ) ] ) kk = np. c _ [ xx. ravel ( ), yy. ravel ( ) ] # put the result into a color "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p106#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 106, "snippet": "we close with the usual properties ( table 3. 12 ) : table 3. 12explanations ’ properties property assessment completeness full completeness achieved "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p107#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 107, "snippet": "uci. ( 2009 ). wine quality data set. available at https : / / archive. ics. uci. edu / ml / datasets / wine + quality. waskom, m. ( 2014 ). seaborn d"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p108#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 108, "snippet": "( 1 ) ( 2 ) © the author ( s ), under exclusive license to springer nature switzerland ag 2021 l. gianfagna, a. di cecco, explainable ai with python h"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p109#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 109, "snippet": "in that case, we relied on the “ intrinsic explainability ” of the decision tree, but we saw how permutation importance produced enhanced interpretabi"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p110#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 110, "snippet": "taxi cab company to provide to customers explanations on the fares ( predicted by a boosted tree ml model ) they will pay in real time. we will use th"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p111#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 111, "snippet": "import numpy as np import pandas as pd from sklearn. model _ selection import train _ test _ split from sklearn. ensemble import randomforestclassifie"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p112#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 112, "snippet": "date team opponentgoal scored ball possession % attemptson - target off - target man of the match 15 - 06 - 2018 iran morocco 1 36 8 2 5 yes as you ma"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p113#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 113, "snippet": "answer the question “ what are the most important features of your model? ” with permutation importance method, and the same low can be adopted for an"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p114#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 114, "snippet": "let ’ s explain the table. the features are ranked by their relative importance, so the irst and most important result is that we may directly answer "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p115#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 115, "snippet": "at the bottom of the ranking, we have some negative values. a negative value may sound strange, but it simply states that the model without those feat"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p116#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 116, "snippet": "fig. 4. 3permutation importance output on the training set, no anomalous negative values here the table conirms our idea : this time we don ’ t have a"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p117#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 117, "snippet": "goal scored may change the predictions. is there any threshold on goal scored to increase the probability of having the player of the match prize? per"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p118#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 118, "snippet": "with this bunch of lines, we just select the feature we want to analyze ( goal scored ) and pass the info to pdp library to do the job ; here are the "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p119#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 119, "snippet": "fig. 4. 5partial dependence plot diagram that shows how “ distance covered ” inluences the prediction feature _ to _ plot = ‘ distance covered ( kms )"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p120#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 120, "snippet": "too much decreases the probability for the “ player of the match ” award that was not evident from permutation importance analysis only. remember that"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p121#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 121, "snippet": "fig. 4. 6pdp diagram that shows the interaction of the two main features and their impact on the prediction features _ to _ plot = [ ' goal scored ', "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p122#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 122, "snippet": "some new behavior emerges from this diagram compared with the previous two in which we have just one feature per time. the maximum increase of probabi"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p123#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 123, "snippet": "property assessment translucency as any intrinsic explainable model, we can look at the internals. weights are used to provide explanations but not so"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p124#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 124, "snippet": "the functional relationship of these features with the prediction, but we are not able to answer the direct question : considering the features in the"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p125#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 125, "snippet": "players. the creators of modern game theory were mathematicians john von neumann and john nash and economist oskar morgenstern. cooperative game theor"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p126#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 126, "snippet": "the xai method is called shap – acronym for shapley additive explanations – and provides explanations of a single prediction through a linear combinat"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p127#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 127, "snippet": "with these three lines of code, we have already produced the shap values that can be used to explain the speciic prediction for the match, but they wo"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p128#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 128, "snippet": "between these features and the prediction. we had no chance of getting into a speciic prediction to answer a why question on a speciic match. with sha"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p129#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 129, "snippet": "fig. 4. 8shap diagram that shows the features ’ ranking and the related impact on the match prediction ( becker 2020 ) you see on the left the list of"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p130#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 130, "snippet": "let ’ s summarize, as usual, the properties of the explanations we provided ( table 4. 6 ) : table 4. 6explanation property assessment for shap proper"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p131#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 131, "snippet": "as we have already said, shapley values have a deep foundation in game theory – a theoretical base that many other explanation methods lack. in the or"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p132#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 132, "snippet": "usually, we use some data distribution, or we give to the method a background dataset to sample from randomly. the number of possible subsets of featu"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p133#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 133, "snippet": "ind a surrogate linear model repeatedly calling the trained model f. say f is a classiier : the value f ( x ) is the probability of a class for the in"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p134#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 134, "snippet": "the new term ω ( g ) is a lasso regularization to have a sparse representation. we can change ω ( g ) to reduce the dimension of the explanation to sa"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p135#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 135, "snippet": "we conclude by showing remarking one advantage of shap. shapley values are all of the same dimensions ( say dollars ) even if the corresponding featur"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p136#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 136, "snippet": "# a we will use sklearn now we ilter outliers and train a gradient boosting model data = data. query ( ' pickup _ latitude > 40. 7 and pickup _ latitu"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p137#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 137, "snippet": "0. 6872889587581943 train 0. 4777749929850299 test not a good model because the r2 score is pretty low, and we can say it ’ s even overitting because "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p138#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 138, "snippet": "as we have already seen, permutation importance shows to us the features that have a major impact on the loss function as in the y axis of the igure. "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p139#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 139, "snippet": "plot _ partial _ dependence ( reg, x _ test, [ ( x _ test. columns [ 0 ], x _ test. columns [ 3 ] ) ], n _ jobs = 3, grid _ resolution = 20, ax = ax )"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p140#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 140, "snippet": "reduce the example set to only 10 meaningful centroids. nonetheless, model creation took 115. 090s on our machine. as introduced at the beginning of t"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p141#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 141, "snippet": "fig. 4. 14partial dependence plot using shap # variable importance - like plot. shap. summary _ plot ( shap _ values, x _ test, plot _ type = \" bar \" "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p142#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 142, "snippet": "we see instead a deinite pattern with an intersection showing us a relevant interaction. what the picture roughly shows is that when dropoff _ latitud"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p143#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 143, "snippet": "pd. dataframe ( shap _ values, columns = x _ test. columns ) # a using treeshap, not kernelshap. remember reg is a lgbm model that is a tree - based m"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p144#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 144, "snippet": "fig. 4. 16shap summary plot for cab scenario # each plot represents one data row, with shap value for each variable, # along with red - blue as the ma"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p145#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 145, "snippet": "fig. 4. 17packed shap force plots or grouping shap explanation using the similarity between the explanations ( fig. 4. 18 ). fig. 4. 18shap diagram : "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p146#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 146, "snippet": "the method of gerber, albeit with its limits, clariies what we truly ind in shap in contrast to what we would expect to ind in shap. if we return to f"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p147#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 147, "snippet": "fig. 4. 19the adult census database – uci machine learning repository ( uci 1996 ) – shap documentation in the picture, we have both the explanation o"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p148#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 148, "snippet": "we have already seen this effect in the section on permutation importance ; the original model has learned to use other features than gender to predic"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p149#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 149, "snippet": "kaggle. ( 2020 ). new york taxi fare prediction. available at https : / / www. kaggle. com / c / new - york - city - taxi - fare - prediction. lundber"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p150#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 150, "snippet": "( 1 ) ( 2 ) © the author ( s ), under exclusive license to springer nature switzerland ag 2021 l. gianfagna, a. di cecco, explainable ai with python h"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p151#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 151, "snippet": "in this chapter, we will talk about xai methods for deep learning models. the explanation of deep learning models is a matter of active research, so w"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p152#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 152, "snippet": "internal functioning of deep learning models. only in the following sections we will cover gradient - based methods. 5. 1. 1 adversarial features it i"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p153#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 153, "snippet": "dogs are usually stationary, and cats are always on the move, so we will have to use different exposure times. also, dogs and cats have different size"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p154#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 154, "snippet": "so, for example, complex models of high capacity in the sense of vapnik - chervonenkis dimension of vapnik and chervonenkis ( 1968 ) with many paramet"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p155#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 155, "snippet": "the black box could learn to distinguish the two fruits from the shape, from the texture, or, for example, from the stem. the fruit petiole is an abso"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p156#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 156, "snippet": "fig. 5. 4the occlusion idea as an augmentation technique : random gray rectangles force the model to rely more on robust features such as skin ’ s tex"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p157#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 157, "snippet": "this is the most classic implementation of the algorithm, which obviously will take longer the smaller the patch. a slightly smarter solution is to ap"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p158#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 158, "snippet": "fig. 5. 5original image image _ path = \". / dog - and - cat - cover. jpg \" img = tf. keras. preprocessing. image. load _ img ( image _ path, target _ "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p159#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 159, "snippet": "this is the original image and following the explanations for the tabby cat and common dog classes, respectively ( fig. 5. 6 ). fig. 5. 6occlusions us"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p160#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 160, "snippet": "in this section, we will make a brief review of what neural networks are and how they work. the inner working will introduce us to differential method"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p161#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 161, "snippet": "each computational node is typically a linear combination of the inputs then passed through a nonlinear activation function, typically a relu function"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p162#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 162, "snippet": "in practice, for a large class of nn, the search for weights ( optimization ) through optimizers such as sgd ( stochastic gradient descent ) and adam "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p163#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 163, "snippet": "( 5. 1 ) for suitable weight matrices w 1 and w 2. in analogy with the logistic regression, the choice activation function σ was historically a sigmoi"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p164#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 164, "snippet": "property bengio ’ s conjecture. you can read a proof in mhaskar et al. ( 2019 ). of course, we are talking about ideal networks, but, from a theoretic"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p165#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 165, "snippet": "the relu function is always positive and for positive numbers has a derivative exactly of one preserving the lux. so using the relu activation, we can"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p166#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 166, "snippet": "we have already dealt with an example of saliency map with occlusion analysis, now armed with the ability to use the internal workings of a network, w"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p167#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 167, "snippet": "several partial solutions to this problem have been created, and each of them uses brilliant and powerful ideas ( we list a few ). 5. 3 opening deep n"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p168#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 168, "snippet": "convolutional layer to decrease the size of the image and reduce the parameters so as to reduce overitting. the global average pooling layer works in "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p169#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 169, "snippet": "from the convolutional layer, and then apply a relu function to regularize it ( fig. 5. 10 ). fig. 5. 10grad - cam schema : visual explanations from d"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p170#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 170, "snippet": "5. 3. 3 deepshap / deeplift where do the gradient problems come from? we have already mentioned that using the relu as an activation function, the neu"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p171#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 171, "snippet": "import keras from keras. datasets import mnist from keras. models import sequential from keras. layers import dense, dropout, flatten from keras. laye"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p172#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 172, "snippet": "model = sequential ( ) model. add ( conv2d ( 32, kernel _ size = ( 3, 3 ), activation = \" relu \", input _ shape = input _ shape ) ) model. add ( conv2"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p173#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 173, "snippet": "#... or pass tensors directly # e = shap. deepexplainer ( ( model. layers [ 0 ]. input, model. layers [ - 1 ]. output ), background ) shap _ values = "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p174#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 174, "snippet": "looking within the image does not tell the user what it is doing with that part of the image. ” we can say salience tells us what the network sees, no"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p175#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 175, "snippet": "fig. 5. 12the data distributions in the latent space. ( a ) the data are not mean centered ; ( b ) the batch normalized but not decorrelated ; ( c ) t"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p176#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 176, "snippet": "features along the dimensions of the largest variance. t - sne instead is a nonlinear method ; it leverages kullback - leibler divergence so that the "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p177#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 177, "snippet": "color = plt. cm. set1 ( y [ i ] / 10. ), fontdict = { ' weight ' : ' bold ', ' size ' : 9 } ) plt. xticks ( [ ] ), plt. yticks ( [ ] ) if title is not"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p178#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 178, "snippet": "fig. 5. 14alexnet architecture. a succession of convolutional ilters and max pooling reductions to other convolutional ilters and at the end of the ne"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p179#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 179, "snippet": "fig. 5. 15an atlas of images using the dimensional reduction technique ( karpathy 2014 ) we will say that, by using the dimensional reduction algorith"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p180#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 180, "snippet": "effect of some adversarial feature rather than an actual semantic division. to know what the network really thinks, we need a more reined tool. 5. 5. "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p181#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 181, "snippet": "fig. 5. 16one million images are given to the network, one activation function per image. activations are dimensionally reduced to two dimensions. sim"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p182#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 182, "snippet": "fig. 5. 17is it a frying pan or a wok? these are the most excited ilters at the sight of frying pan and wok now we can see the activation atlas shows "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p183#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 183, "snippet": "fig. 5. 18we have a perfect noodle discriminator perhaps we have found a counterfactual feature, and in fact, by overlaying a sticker with noodles on "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p184#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 184, "snippet": "we introduced neural networks by presenting their structure, equations, and convergence problems. we answered the question about the need for deep neu"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p185#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 185, "snippet": "1 convolutional neural networks. communications of the acm, 60 ( 6 ), 84 – 90. [ crossref ] mhaskar, h., liao, q., & poggio, t. ( 2019 ). learning fun"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p186#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 186, "snippet": "( 1 ) ( 2 ) © the author ( s ), under exclusive license to springer nature switzerland ag 2021 l. gianfagna, a. di cecco, explainable ai with python h"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p187#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 187, "snippet": "would have changed if some features ( or values ) would not have occurred. explainability is a theory that deals also with unobserved facts toward a g"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p188#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 188, "snippet": "approximated model based on these variables, and test the model with experiments. the iterative process is ( 1 ) make hypothesis, ( 2 ) build a model,"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p189#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 189, "snippet": "fig. 6. 1damped oscillation in this case, we don ’ t have the usual input and output features like in the previous examples ( e. g., we predicted and "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p190#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 190, "snippet": "feature value f2 output2 f3 output3 the most common classes of features that are created from the data series are : date time features : these are the"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p191#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 191, "snippet": "but having the time series adapted for a supervised learning is not enough for our goals. at this point we have the possibility to use the full superv"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p192#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 192, "snippet": "6. 2 ladder of causation in chap. 1 we used table 1. 1 to distinguish between interpretability and explainability ; we put it here again for simplicit"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p193#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 193, "snippet": "3. counterfactuals activity : questions : examples : imagining, retrospection, understanding, what if 1 had done...? why? ( was it x that caused y? wh"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p194#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 194, "snippet": "fig. 6. 2ladder of causation ( pearl & makenzie 2019 ) there are three different types of cognitive abilities that are needed to climb the ladder of c"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p195#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 195, "snippet": "the rat always goes from point a to point b. some readers may be surprised to see that i have placed present - day learning machines squarely on rung "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p196#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 196, "snippet": "despite the wording that is not so common, intervention is something that belongs to our daily lives. every time we decide to take a medicine for head"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p197#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 197, "snippet": "rung three is the proper level of causal modeling that is needed to do science. and from our point of view, this is explainability as a stronger versi"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p198#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 198, "snippet": "change ( fig. 6. 4 ). fig. 6. 3causal diagram for the iring squad example ( pearl & makenzie 2019 ) fig. 6. 4case of intervention, the link between c "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p199#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 199, "snippet": "the intervention removed the link between co and a ; a is true without co being necessarily true ; prisoner is dead whatever co is because a ired. and"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p200#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 200, "snippet": "the prisoner would have died also in the imaginary world. 6. 3 discovering physics concepts with ml and xai based on what we discussed in the previous"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p201#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 201, "snippet": "fig. 6. 6a generic feedforward neural network there is an input layer that transmits the values to internal layers that do the computation to produce "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p202#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 202, "snippet": "fig. 6. 7fully connected neural network it is a fully connected model of nn, and it is pretty obvious that if our goal is just to reproduce the output"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p203#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 203, "snippet": "fig. 6. 8trivial neural network topology but imagine now to reduce the number of nodes in the hidden layers ( fig. 6. 9 ) ; in this way the autoencode"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p204#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 204, "snippet": "fig. 6. 9autoencoder topology the internal layers have a reduced number of nodes, and this makes the autoencoder to represent the information in a com"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p205#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 205, "snippet": "fig. 6. 10autoencoder process fig. 6. 11number recognition with autoencoder i can guess your question at this point : but what is the relation of this"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p206#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 206, "snippet": "ind the most “ compact ” representation of the physical system, which is to ind the relevant physical variables to have a complete model of the damped"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p207#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 207, "snippet": "the system. while ( 1 ) can be achieved via the standard approach to time series, we will rely on variational autoencoders we discussed to tackle poin"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p208#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 208, "snippet": "physical system ) at speciic times based on the learned representation. as we said, the representation is called latent representation, and our main f"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p209#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 209, "snippet": "question = tprime answer = pendulum ( tprime, a0, delta0, k, b, m ) if answer = = none : continue x = np. linspace ( 0, tmax, 50 ) t _ arr = np. linsp"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p210#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 210, "snippet": "fig. 6. 13damped pendulum ( a ) real evolution compared to the predicted trajectory by scinet ( b ) activation plot, representation learned by scinet "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p211#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 211, "snippet": "this is an “ easy ” case that shows the general ideas about how to get knowledge discovery with ml and xai. as properly commented by iten et al. ( 201"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p212#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 212, "snippet": "fig. 6. 15three - tier explainability ( karim et al. 2018 ) looking at fig. 6. 15, we see the low from data to scientiic knowledge : 1. statistical mo"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p213#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 213, "snippet": "next section, we will see some options that are emerging to effectively use ml in scientiic discovery. 6. 4 science in the age of ml and xai the goal "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p214#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 214, "snippet": "reconstruct an event can be taken as an anomaly – a new class of event that we should study to look for new physics. we are making the things too easy"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p215#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 215, "snippet": "3. the hierarchical structure of any physical system may play a role also on nn side. elementary particles build atoms that build molecules up to plan"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p216#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 216, "snippet": "guest, d., cranmer, k., & whiteson, d. ( 2018 ). deep learning and its application to lhc physics. https : / / doi. org / 10. 1146 / annurev - nucl - "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p217#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 217, "snippet": "( 1 ) ( 2 ) © the author ( s ), under exclusive license to springer nature switzerland ag 2021 l. gianfagna, a. di cecco, explainable ai with python h"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p218#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 218, "snippet": "fig. 7. 1comparing two pictures of a panda. ( goodfellow et al. 2014 ) do you see any difference between these two pandas? i bet the answer is no ; we"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p219#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 219, "snippet": "but before exploring the relationship between ae and xai, we start with a crash course on adversarial machine learning to set the foundations. 7. 1 ad"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p220#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 220, "snippet": "fig. 7. 2left columns are original images. in the middle we have the perturbation ; on the right there are the hacked images incorrectly classiied. ( "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p221#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 221, "snippet": "( 7. 1 ) this is an optimization problem that is not easy to solve. the irst approach was to use the so - called l - bfgs algorithm ( i. e., a variati"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p222#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 222, "snippet": "craft ae, but it is a general optimization algorithm, and it doesn ’ t shed light on the phenomenon. all these points were prerequisites to answer the"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p223#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 223, "snippet": "generalized so easily and that different architectures of nns may be vulnerable to the same aes was a kind of surprise for the ml researchers. the ini"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p224#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 224, "snippet": "figure 7. 4 shows this behavior in which choosing the right vector in the orthogonal direction to the decision boundary quickly takes the model out of"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p225#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 225, "snippet": "it helps a lot to get the basics of ae ( fig. 7. 5 ). fig. 7. 5loss function in one dimension let ’ s consider the usual gradient descent in one dimen"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p226#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 226, "snippet": "we keep ixed the parameters of the model and differentiate with respect to the speciic input x in the other direction if compared to the gradient desc"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p227#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 227, "snippet": "power is based on the intrinsic linearity of the nn that is pretty shocking for every ml student. we say surprising because the basic theory about dee"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p228#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 228, "snippet": "fig. 7. 8logistic and tanh activation functions both logistic and tanh ( bipolar ) activation functions exhibit a saturation behavior that is not in r"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p229#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 229, "snippet": "fundamental piece of the puzzle : what we did so far assumes that we have access to the neural network models so the ae we talked about would not appl"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p230#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 230, "snippet": "fig. 7. 9left images are the original images with proper labels ; central image is the universal perturbation ; on the right there are the misclassiie"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p231#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 231, "snippet": "vgg - f caffenetgooglenetvgg - 16vgg - 19resnet - 152 caffenet 74. 00 % 93. 30 % 47. 70 % 39. 90 % 39. 90 % 48. 00 % googlenet46. 20 % 43. 80 % 78. 90"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p232#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 232, "snippet": "we now use fgsm to tweak this input ; remember that the idea behind fgsm is to have small changes in the input to make the overall image ( or whatever"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p233#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 233, "snippet": "from the imagenet database. it is used to classify images into more than 1000 object categories spanning from animals to pencils. in the following, we"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p234#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 234, "snippet": "# apply attack on source image attack = foolbox. attacks. fgsm ( fmodel ) adversarial = attack ( image, label ) adversarial here is the image that has"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p235#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 235, "snippet": "into these details will allow understanding what the root of this relation between xai and ae is. back to our picture of the xai low ( fig. 7. 11 ). f"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p236#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 236, "snippet": "of example - based explanations : the prototypes and counterfactual explanations. we will see how ae can be considered a speciic case of counterfactua"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p237#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 237, "snippet": "we said that differently from prototypes, counterfactual instances do not exist in the dataset. this should be a kind of deja vu for the reader becaus"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p238#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 238, "snippet": "p - contrast question into two parts : why p? why not q? we get shapley values for p and q classes and use those that work against the classiication o"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p239#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 239, "snippet": "7. 3 defending against adversarial attacks with xai keeping in mind what we learned about ae, the obvious question that emerges is how to defend again"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p240#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 240, "snippet": "using xai methods to defend from ae is an additional emerging approach that cannot be classiied in these four main families of defenses. the backgroun"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p241#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 241, "snippet": "figure 7. 12 adapted from the work of fidel et al. ( 2020 ) clearly shows the proposed solution. on the left and right parts of the igures are normal "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p242#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 242, "snippet": "the standard cars have ive evident and strong rows, three rows on the top, one in the middle, and two on the bottom of the shap diagram. the cats on t"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p243#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 243, "snippet": "robustness of the model against ae helps xai, and xai helps defending against ae. this chapter is very rich in content and essential ideas that are no"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p244#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 244, "snippet": "molnar, c. ( 2019 ). interpretable machine learning. a guide for making black box models explainable. available at https : / / christophm. github. io "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p245#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 245, "snippet": "( 1 ) ( 2 ) © the author ( s ), under exclusive license to springer nature switzerland ag 2021 l. gianfagna, a. di cecco, explainable ai with python h"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p246#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 246, "snippet": "also, we deeply discuss a real case scenario in which we show how also xai methods may be fooled to be aware of the risks of any easy approach to gdpr"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p247#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 247, "snippet": "own teacher. the dnn knows nothing about go and is not trained on an existing huge dataset of go matches but starts playing against itself combining t"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p248#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 248, "snippet": "possible approach following shap to generate counterfactuals. this would be very useful in case of alphago because it would represent a way to generat"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p249#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 249, "snippet": "( a ) is necessary for entering into, or performance of, a contract between the data subject and a data controller ; ( b ) is authorized by union or m"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p250#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 250, "snippet": "from a legal point of view, the situation is far from being clear ( wu 2017 ) : the right to explanations is not explicitly mentioned ; gdpr only mand"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p251#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 251, "snippet": "this approach is tested by the authors in a case study in which the objective is to predict the risk of default for a loan. it is shown how, following"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p252#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 252, "snippet": "the core of the solution is to understand how the adversarial classiier may be able to distinguish the real inputs ( to answer with bias ) from the pr"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p253#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 253, "snippet": "fig. 8. 2pca applied to original compas dataset ( blue points ) and lime perturbations ( red points ) : different clusters of blue and red points natu"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p254#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 254, "snippet": "in order to distinguish the points coming from dist from the perturbations and relying on our intuition ( fig. 8. 1 ), a dataset of perturbed points ("}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p255#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 255, "snippet": "fig. 8. 3feature importance for the biased classiier vs our adversarial classiier ( middle and right columns ) that exhibits an unbiased behavior for "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p256#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 256, "snippet": "and tredan ( 2019 ) who claimed to have the proof of the impossibility for online services to provide trusted explanations. and as you can easily gues"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p257#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 257, "snippet": "and we close just in time with the last section with some thoughts on gai, xai, and quantum mechanics ( yes theoretical physicists like us need to tal"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p258#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 258, "snippet": "chinese and answers the same questions. would we say that this artiicial intelligence really understands chinese? while we are writing, openai team re"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p259#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 259, "snippet": "but for our purposes and from perspective of xai, we ask a different question : “ should we enforce explainability on agents like gpt - 3? ” or better"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p260#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 260, "snippet": "we attempted different approaches to deal with this state of things, from hidden variables to many world interpretations ; the objective is to avoid t"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p261#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 261, "snippet": "type of interactions in which everything depends on the relations between the entities and there is nothing that is existent by itself in a ixed form."}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p262#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 262, "snippet": "[ crossref ]"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p263#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 263, "snippet": "appendix a “ f. a. s. t. xai certiication ” the purpose of this checklist is to provide a practical guidance, based on the contents of this book, and "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p264#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 264, "snippet": "if the “ surrogate ” is not available, certiication is assumed to be “ light ” 5. f. a. s. t. methodology key aspects – [ ] identify all possible fair"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p265#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 265, "snippet": "e explainable artiicial intelligence ( xai ) f f. a. s. t. g general data protection regulation ( gdpr ) h how to defend against ae human in the loop "}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p266#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 266, "snippet": "partial dependence plot ( pdp ) permutation importance post hoc explainability properties of explanations r reinforcement learning s science with ml a"}
{"id": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf#p267#c1", "source": "AAI/Book dokumen.pub_explainable-ai-using-python- (2).pdf", "page": 267, "snippet": "leonida gianfagna antonio di cecco explainable al with python"}
{"id": "AAI/pleplp_featureimport (1) (1).pptx#p1#c1", "source": "AAI/pleplp_featureimport (1) (1).pptx", "page": 1, "snippet": "summary of current plp feature importance plp currently uses variable importance in scikit - learn or coefficients – this is not great and may use dif"}
{"id": "AAI/pleplp_featureimport (1) (1).pptx#p2#c1", "source": "AAI/pleplp_featureimport (1) (1).pptx", "page": 2, "snippet": "current feature importance not based on model we also use basic measure of association between predictor and outcome : standardized mean difference th"}
{"id": "AAI/pleplp_featureimport (1) (1).pptx#p3#c1", "source": "AAI/pleplp_featureimport (1) (1).pptx", "page": 3, "snippet": "common feature importance potential other methods commonly used : permutation feature importance ( based on the decrease in model performance ) shap ("}
{"id": "AAI/pleplp_featureimport (1) (1).pptx#p4#c1", "source": "AAI/pleplp_featureimport (1) (1).pptx", "page": 4, "snippet": "permutation feature importance ( pfi ) first, a model is fit on the dataset, such as a model that does not support native feature importance scores. t"}
{"id": "AAI/pleplp_featureimport (1) (1).pptx#p5#c1", "source": "AAI/pleplp_featureimport (1) (1).pptx", "page": 5, "snippet": "illustration feature 1 feature 2 feature 3 feature 4 feature 5 outcome 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 feature 1 feature 2"}
{"id": "AAI/pleplp_featureimport (1) (1).pptx#p6#c1", "source": "AAI/pleplp_featureimport (1) (1).pptx", "page": 6, "snippet": "pfi example plot"}
{"id": "AAI/pleplp_featureimport (1) (1).pptx#p7#c1", "source": "AAI/pleplp_featureimport (1) (1).pptx", "page": 7, "snippet": "pfi pros / cons pros cons fairly quick – just permute data per predictor then apply model and calculate auc ( or other metric ) decrease model specifi"}
{"id": "AAI/pleplp_featureimport (1) (1).pptx#p8#c1", "source": "AAI/pleplp_featureimport (1) (1).pptx", "page": 8, "snippet": "shap ( shapley additive explanations ) based on game theory looks at the contribution of a model ’ s feature for each patient ( local interpretability"}
{"id": "AAI/pleplp_featureimport (1) (1).pptx#p9#c1", "source": "AAI/pleplp_featureimport (1) (1).pptx", "page": 9, "snippet": "example shap example from internet – the nice thing about shap is that you get the impact of the feature per patient so we can plot distributions of i"}
{"id": "AAI/pleplp_featureimport (1) (1).pptx#p10#c1", "source": "AAI/pleplp_featureimport (1) (1).pptx", "page": 10, "snippet": "shap pros / cons pros cons local importance model specific – with our data correlated variables may not be selected due to the other variable being pi"}
{"id": "AAI/pleplp_featureimport (1) (1).pptx#p11#c1", "source": "AAI/pleplp_featureimport (1) (1).pptx", "page": 11, "snippet": "other methods partial importance ( pi ) individual conditional importance ( ici ) plots partial dependence ( pd ) individual conditional expectation ("}
{"id": "AAI/pleplp_featureimport (1) (1).pptx#p12#c1", "source": "AAI/pleplp_featureimport (1) (1).pptx", "page": 12, "snippet": "current work i ’ ve started to add pfi – i ’ m doing in in parallel to speed things up https : / / github. com / ohdsi / patientlevelprediction / blob"}
{"id": "AAI/pleplp_featureimport (1) (1).pptx#p13#c1", "source": "AAI/pleplp_featureimport (1) (1).pptx", "page": 13, "snippet": "10 min group discussion 1. should we focus on feature importance – what are the use cases in ple and plp? 2. any other useful methods we should consid"}
{"id": "AAI/pleplp_featureimport (1) (1).pptx#p14#c1", "source": "AAI/pleplp_featureimport (1) (1).pptx", "page": 14, "snippet": "10 min research collaboration ideas shall we do an ohdsi network study into feature importance? what things are important to assess? ( speed, interpre"}
{"id": "AAI/pleplp_featureimport (1) (1).pptx#p15#c1", "source": "AAI/pleplp_featureimport (1) (1).pptx", "page": 15, "snippet": "if we have time … any ideas to improve our clinical application publication chance?"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p1#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 1, "snippet": "module 3 ai for robotics 1"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p2#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 2, "snippet": "robotic perception robots ’ ability to interact with their surroundings is an essential capability, especially in unstructured human - inhabited envir"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p3#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 3, "snippet": "in robotics, perception is understood as a system that endows the robot with the ability to perceive, comprehend, and reason about the surrounding env"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p4#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 4, "snippet": "robotic perception system 4"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p5#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 5, "snippet": "why robotic perception is crucial? robotics perception is crucial for a robot to make decisions, plan and operate in real world environments, by means"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p6#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 6, "snippet": "nowadays, most of robotic perception systems use machine learning ( ml ) techniques, ranging from classical to deep - learning approaches machine lear"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p7#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 7, "snippet": "in multiple - sensors perception, either the same modality or multimodal, an efficient approach is usually necessary to combine and process data from "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p8#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 8, "snippet": "however, in the majority of applications, the primary role of environment mapping is to model data from exteroceptive sensors, mounted onboard the rob"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p9#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 9, "snippet": "moreover, the sensors used are different depending on the environment, and therefore, the sensory data to be processed by a perception system will not"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p10#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 10, "snippet": "localisation & navigation self - localisation : global sensors odometry markers goal : understand how robots know where they are and how they get to n"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p11#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 11, "snippet": "global sensors satellite global position sensors ( gps ) outdoor ok – c. 10m accuracy military differential gps < 1m accuracy near buildings – too man"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p12#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 12, "snippet": "odometry odometry : position measurement by distance travelled know current position know how much wheels rotate ( e. g. current * time ) new position"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p13#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 13, "snippet": "khepera odometry non - holonomic – must rotate about central vertical axis by wheel rotation counts l = - r wheel geometry : 13"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p14#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 14, "snippet": "computing khepara position n = 600 encoder pulses / full wheel rotation l & r encoder pulses commanded ( or speed & time ) wheel radius left / right w"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p15#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 15, "snippet": "navigating with beacons dead reckoning : wheel slip means increasing error periodically observe markers to recalculate position classical style : sens"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p16#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 16, "snippet": "2d beacon observation observe direction to 3 beacons beacons have known position angle and between pairs of beacons locate self by triangulation need "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p17#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 17, "snippet": "beacons with range assume can measure distances ( u, v ) to 2 beacons ( a, b ) as well as bearing ( e. g. with a range sensor ) 17"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p18#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 18, "snippet": "beacons cont. need : lots of beacons map of beacon location easily identifiable beacons common alternative : use existing scene features : doorways, c"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p19#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 19, "snippet": "navigation i planning how to get to destination keeping track of current position classical robotics approach sense : localisation ( previous ) plan :"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p20#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 20, "snippet": "path planning i get route from current point to destination that avoids obstacles assume a world map, with observable features in known positions ( e."}
{"id": "AAI/Module 3 AI for Robotics.pptx#p21#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 21, "snippet": "avoiding obstacles simplest approach for convex robots is to enlarge environment by size of robot 21"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p22#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 22, "snippet": "route planning i graph search : nodes : floor regions + centre of mass arcs : connectivity & straight line distance between lines of sight here graph "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p23#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 23, "snippet": "route planning ii more realistic graph 23"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p24#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 24, "snippet": "route planning – potential fields give a scene free space map, compute 2 fields : f ( x, y ) : distance from nearest obstacle ( eg. corridor wall ) g "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p25#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 25, "snippet": "potential fields ii define h ( x, y ) = f ( x, y ) - k * g ( x, y ) move in direction of maximum gradient : : force vector from walls : force vector t"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p26#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 26, "snippet": "mars rovers 2 rovers : spirit & opportunity opportunity : 1350 sols travel, 11. 5 km ( design : 90 sols ) humans : route goals robot : route following"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p27#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 27, "snippet": "reactive navigation i classical robot control paradigm 27"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p28#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 28, "snippet": "reactive navigation ii reactive : responds immediately to sensor data mit ’ s rodney brooks subsumption architecture hierarchy of parallel behaviours "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p29#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 29, "snippet": "localisation & navigation summary global features / beacons allow direct position feedback 2. odometry : position based on estimated motion 3. path pl"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p30#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 30, "snippet": "30 what is robot mapping? robot – a device, that moves through the environment mapping – modeling the environment"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p31#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 31, "snippet": "related terms state estimation localization mapping slam navigation motion planning 31"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p32#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 32, "snippet": "32 what is slam? computing the robot ’ s poses and the map of the environment at the same time localization : estimating the robot ’ s location mappin"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p33#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 33, "snippet": "33 what is slam? computing the robot ’ s poses and the map of the environment at the same time localization : estimating the robot ’ s location mappin"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p34#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 34, "snippet": "localization example estimate the robot ’ s poses given landmarks 34"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p35#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 35, "snippet": "mapping example estimate the landmarks given the robot ’ s poses 35"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p36#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 36, "snippet": "slam example estimate the robot ’ s poses and the landmarks at the same time 36"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p37#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 37, "snippet": "the slam problem slam is a chicken - or - egg problem : → a map is needed for localization and → a pose estimate is needed for mapping map localize 37"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p38#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 38, "snippet": "slam is relevant it is considered a fundamental problem for truly autonomous robots slam is the basis for most navigation systems map autonomous navig"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p39#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 39, "snippet": "10 slam applications slam is central to a range of indoor, outdoor, air and underwater applications for both manned and autonomous vehicles. examples "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p40#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 40, "snippet": "11 slam applications indoors space undersea underground courtesy of evolution robotics, h. durrant - whyte, nasa, s. thrun 40"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p41#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 41, "snippet": "definition of the slam problem given the robot ’ s controls observations wanted map of the environment path of the robot 41"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p42#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 42, "snippet": "how ai is changing iot artificial intelligence unlocks the true potential of iot by enabling networks and devices to learn from past decisions, predic"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p43#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 43, "snippet": "consumers, businesses, economies, and industries that adopt and invest in aiot can leverage its power and gain competitive advantages iot collects the"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p44#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 44, "snippet": "why iot needs ai iot allows devices to communicate with each other and act on those insights these devices are only as good as the data they provide. "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p45#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 45, "snippet": "no matter the size and sophistication of the communications network, the sheer volume of data collected by iot devices leads to latency and congestion"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p46#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 46, "snippet": "in security, biometrics are often used to restrict or allow access to specific areas without rapid data processing, there could be delays that impact "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p47#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 47, "snippet": "benefits of aiot every day, iot devices generate around one billion gigabytes of data by 2025, the projection for iot - connected devices globally is "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p48#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 48, "snippet": "with ai, iot networks and devices can learn from past decisions, predict future activity, and continuously improve performance and decision - making c"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p49#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 49, "snippet": "avoiding downtime some industries are hampered by downtime, such as the offshore oil and gas industry unexpected equipment breakdown can cost a fortun"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p50#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 50, "snippet": "increasing operational efficiency ai processes the huge volumes of data coming into iot devices and detects underlying patterns much more efficiently "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p51#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 51, "snippet": "enabling new and improved products and services natural language processing is constantly improving, allowing devices and humans to communicate more e"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p52#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 52, "snippet": "aiot is already revolutionizing many industries, including manufacturing, automotive, and retail here are some common applications for aiot in differe"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p53#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 53, "snippet": "challenges related to the use of intelligent drones in humanitarian action in 2014, ocha ( office for the coordination of humanitarian affairs ) highl"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p54#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 54, "snippet": "to help answer this question, the international community has developed policies, guidelines and principles to guarantee the responsible implementatio"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p55#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 55, "snippet": "consequently, the uav code of conduct is currently being updated by uaviators and the harvard humanitarian initiative similarly, based on a compilatio"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p56#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 56, "snippet": "operational challenges data management while drones are becoming increasingly affordable and autonomous, the difficulty lies in exploiting the data co"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p57#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 57, "snippet": "this storage capacity can be so limited that drones equipped with onboard processing for navigation and collision avoidance capacity diminish the capa"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p58#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 58, "snippet": "“ the big issue when we are talking of ai is, as we collect huge amounts of data, we need a big size for storage and then we need another platform to "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p59#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 59, "snippet": "the challenge related to analyzing data has been partly overcome as computer vision algorithms can now quickly analyze a large amount of aerial data h"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p60#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 60, "snippet": "secondly, using ai models for analyzing aerial data can be challenging if humanitarians do not have the necessary expertise thus, the technological so"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p61#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 61, "snippet": "humanitarians are distracted by different components such as the drone itself ; the accessories ; how to capture imagery ; and how to analyze and proc"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p62#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 62, "snippet": "ethical issues “ challenges of autonomous uavs are mainly ethical, not operational. ” - regina surber responsible partnership the military is the main"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p63#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 63, "snippet": "thus, the ethical issues raised by d. gilman and m. easton in 2014 on the association, even indirect, of humanitarian actors with defense companies ar"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p64#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 64, "snippet": "data collection and privacy the issues regarding the protection of privacy and the ethical collection and use of data are highly significant when talk"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p65#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 65, "snippet": "as uavs involve data collection, analysis, creation and storage, they create property to whose security is difficult to ensure in that sense, it may c"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p66#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 66, "snippet": "this principle requires that the data collected by humanitarian uavs “ will never be used for informing anything other than the humanitarian situation"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p67#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 67, "snippet": "the risk of mass surveillance with drones has often been highlighted as the main ethical issue when shared with governments, data can be used for dome"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p68#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 68, "snippet": "while these images have proven to be useful for humanitarian purposes, they give information on a population displaced by genocide issuing security th"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p69#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 69, "snippet": "using drones to collect human data such as emotional or biological data raises issues on the potential violation of human dignity, especially when thi"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p70#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 70, "snippet": "as humanitarians have “ to ensure data privacy and security before, during, and after the implementation of a hia ” ( hhi signal code, obligation n°6 "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p71#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 71, "snippet": "but, ensuring data safety is hard as data is intangible and easily manipulated or stolen “ more importantly, whether or not we want this to be ensured"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p72#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 72, "snippet": "uncertainty on risks related to new technology there is a need to take advantage of emerging technologies in humanitarian action while recognizing the"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p73#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 73, "snippet": "to guarantee an ethical development of ai technology, “ investments in ai should be accompanied by funding for research on ensuring its beneficial use"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p74#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 74, "snippet": "drone only relying on computer vision algorithms to fly autonomously, is highly dangerous indeed, there is a risk that ai algorithms learn human preju"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p75#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 75, "snippet": "it is extremely important to have proper training data sets to leverage risks against biased data in that sense, unosat developed an archive of traini"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p76#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 76, "snippet": "autonomous decision - making an agent is autonomous when it has a goal, works toward it by analyzing a vast amount of data from its environment, makes"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p77#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 77, "snippet": "also, when drones are replacing humanitarians in missions where human - human interactions is key – for example when they are used for inspection of d"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p78#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 78, "snippet": "as autonomous drones are not developed by humanitarians, they don ’ t have the ownership nor control over this technology the question is whether it r"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p79#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 79, "snippet": "as full autonomy is reached when humans are out of the loop, creating autonomous technology means that “ humans take a conscious decision to create in"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p80#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 80, "snippet": "impact on humanitarian action in emergency response automated detection can be essential for humanitarians in disaster response and disaster risk mana"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p81#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 81, "snippet": "by getting to the damage site first, uavs can provide valuable data and better ensure the safety of first responders it can access areas that are not "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p82#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 82, "snippet": "after a disaster such as a hurricane, a flood, an earthquake or a wildfire, ai models applied to drone imagery can provide quick and accurate damage a"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p83#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 83, "snippet": "drones integrated in disaster emergency response plans “ we are seeing departments starting uav programs all over the world. up until last year, it wa"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p84#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 84, "snippet": "their use has significantly increased, and they are now used by first responders all over the country to respond to a wide range of disasters such as "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p85#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 85, "snippet": "for example, aerial application software has been used in recovery efforts in several hurricane emergency responses late 2016, after hurricane matthew"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p86#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 86, "snippet": "live ai for situation monitoring the use of drones to obtain real - time information, such as live streamed video, presents additional opportunities f"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p87#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 87, "snippet": "if a shark is detected, sharkspotter provides both a visual indication on the computer screen and an audible alert to the operator rescuers are warned"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p88#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 88, "snippet": "live ai for real time damage assessment the verizon 5g first responder lab aims to build solutions enabled by 5g technology for first responder commun"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p89#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 89, "snippet": "this was implemented in china in response to a forest fire in early april 2019 drones were deployed in the affected zone of sichuan province where foo"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p90#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 90, "snippet": "ai onboard edge computing makes ai analysis onboard drones possible devices equipped with processors and sensors can process data locally as it is cap"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p91#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 91, "snippet": "onboard ai for autonomous flight today, most of the uavs used in humanitarian response are partly autonomous as flight parameters can be set : over wh"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p92#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 92, "snippet": "however, drones need to fly in visual line of sight so the drone pilot can ensure that the device does not collide with something present in its envir"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p93#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 93, "snippet": "this is an entry - level option to add advanced ai to embedded products similarly, intel has designed a new vision processing unit that could be used "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p94#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 94, "snippet": "a system is autonomous if it can perceive, analyse, communicate, plan, make decisions and act, in order to achieve objectives assigned by a human or b"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p95#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 95, "snippet": "onboard ai is needed to make moment - to - moment decisions about how to respond to the data perceived by sensors an autonomous system must be able to"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p96#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 96, "snippet": "indeed, onboard computers will get more powerful, new algorithms will improve flight paths programing and image processing so that decision - making p"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p97#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 97, "snippet": "such abilities could be useful during humanitarian missions for things such as advocacy, remote support, extra sensors ( e. g. heat sensors in search "}
{"id": "AAI/Module 3 AI for Robotics.pptx#p98#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 98, "snippet": "when flying in daylight and good weather conditions, the system is highly capable of following an individual and circumventing obstacles but, such con"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p99#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 99, "snippet": "the drone used iris automation ’ s detect - and - avoid system to complete more than 150 miles of power line inspections this special authorization pa"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p100#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 100, "snippet": "it can also retrieve videos and images from areas that are far to reach or are insecure to be useful, drones need to fly far, fast and bvlos they need"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p101#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 101, "snippet": "onboard ai for feature detection integrating an ai model for feature detection into a drone represents a valuable opportunity for the humanitarian sec"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p102#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 102, "snippet": "in warehouses, drones are used to navigate through the building and automatically do inventory checks customized models have also been created specifi"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p103#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 103, "snippet": "leveraging edge computing and having an ai model onboard is valuable as the drone can transmit results to the ground controller in a search and rescue"}
{"id": "AAI/Module 3 AI for Robotics.pptx#p104#c1", "source": "AAI/Module 3 AI for Robotics.pptx", "page": 104, "snippet": "if the cameras on the drone “ sees ” a shape that matches, it would instantly report sightings of potential missing people with a certain level of con"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p1#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 1, "snippet": "code of ethics in research methodology chatgpt said : chatgpt the code of ethics in research methodology refers to the guidelines and principles that "}
{"id": "RM/RM Unit 1 chatgpt.pdf#p2#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 2, "snippet": "7. non - discrimination • research must be conducted without bias, ensuring that participants are treated fairly regardless of race, gender, religion,"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p3#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 3, "snippet": "o protect public safety in all professional decisions and actions. o disclose any factors that might endanger the public or the environment. 2. avoid "}
{"id": "RM/RM Unit 1 chatgpt.pdf#p4#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 4, "snippet": "o be truthful and respectful in all professional interactions. 10. assist colleagues and co - workers in their professional development : o support ot"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p5#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 5, "snippet": "• 1. 04. be transparent about any limitations or weaknesses in the software that could result in harm. • 1. 05. encourage public understanding of comp"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p6#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 6, "snippet": "• 4. 04. ensure that technical decisions align with ethical practices and uphold public safety. 5. management software engineering managers and leader"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p7#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 7, "snippet": "software engineers shall participate in lifelong learning regarding the practice of their profession and shall promote an ethical approach to the prac"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p8#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 8, "snippet": "• they must continually strive to improve their technical skills and stay updated with advances in the field. • engineers should avoid undertaking tas"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p9#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 9, "snippet": "• engineers shall uphold the dignity of the profession and conduct themselves with honesty, fairness, and respect for others. • they should foster an "}
{"id": "RM/RM Unit 1 chatgpt.pdf#p10#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 10, "snippet": "4o you said : environment aspect chatgpt said : chatgpt the environmental aspect in engineering refers to the consideration of the natural environment"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p11#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 11, "snippet": "o engineers should conduct environmental impact assessments ( eias ) to understand how a project might affect biodiversity and take steps to reduce ha"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p12#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 12, "snippet": "o regulations such as air quality standards, water quality laws, and hazardous materials management ensure that engineering projects do not harm the e"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p13#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 13, "snippet": "you said : sustainable outcome aspect of code of ethics chatgpt said : chatgpt the sustainable outcome aspect of a code of ethics refers to the ethica"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p14#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 14, "snippet": "• engineers and professionals are responsible for the ethical management of resources, ensuring that natural, financial, and human resources are used "}
{"id": "RM/RM Unit 1 chatgpt.pdf#p15#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 15, "snippet": "examples of sustainable outcomes in professional ethics • green building design : architects and engineers designing energy - efficient buildings usin"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p16#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 16, "snippet": "the environmental aspect of a code of ethics emphasizes the responsibility of professionals, especially in fields like engineering, science, and busin"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p17#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 17, "snippet": "• transparent communication about environmental risks and outcomes is essential, ensuring that all stakeholders are aware of potential environmental c"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p18#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 18, "snippet": "importance of the environmental aspect in a code of ethics 1. long - term sustainability : considering environmental aspects ensures that projects and"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p19#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 19, "snippet": "• employees have a duty to demonstrate loyalty and commitment to their employer, acting in the best interests of the organization. • loyalty involves "}
{"id": "RM/RM Unit 1 chatgpt.pdf#p20#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 20, "snippet": "• employees should not engage in actions that reflect poorly on their professional abilities, such as misrepresenting qualifications or failing to com"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p21#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 21, "snippet": "• transparency is key in building trust within the organization, as it helps ensure that the employer is aware of any issues that might affect operati"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p22#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 22, "snippet": "4o you said : general public and nation aspect of code of ethics chatgpt said : chatgpt the general public and nation aspect of a code of ethics focus"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p23#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 23, "snippet": "• professionals must not participate in illegal activities, even if pressured to do so by employers or clients, and they should report violations that"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p24#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 24, "snippet": "• they should support efforts that enhance national prosperity and sustainability, ensuring that their work contributes positively to long - term nati"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p25#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 25, "snippet": "• social equity : a healthcare professional advocates for access to affordable healthcare for underserved communities, ensuring that everyone, regardl"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p26#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 26, "snippet": "• lessons learned : the importance of comprehensive risk assessment, open communication channels, and considering environmental factors in engineering"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p27#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 27, "snippet": "5. bridge collapse in kolkata, india ( 2016 ) • overview : an under - construction bridge in kolkata collapsed, killing at least 26 people and injurin"}
{"id": "RM/RM Unit 1 chatgpt.pdf#p28#c1", "source": "RM/RM Unit 1 chatgpt.pdf", "page": 28, "snippet": "6. public trust : transparent research methodologies and adherence to ethical standards build public confidence in engineering practices and infrastru"}
{"id": "RM/updated_iit_survey.pptx#p1#c1", "source": "RM/updated_iit_survey.pptx", "page": 1, "snippet": "iit bombay # › iste workshop introduction to research methodologies sahana murthy iit bombay june 25 – july 4, 2012 tips on literature review"}
{"id": "RM/updated_iit_survey.pptx#p2#c1", "source": "RM/updated_iit_survey.pptx", "page": 2, "snippet": "iit bombay # › what is a literature review summary of related work descriptive and evaluative analytical in nature"}
{"id": "RM/updated_iit_survey.pptx#p3#c1", "source": "RM/updated_iit_survey.pptx", "page": 3, "snippet": "iit bombay # › why literature review need to give credit to others ’ work giving credit is the “ law ”! gives background information to your work situ"}
{"id": "RM/updated_iit_survey.pptx#p4#c1", "source": "RM/updated_iit_survey.pptx", "page": 4, "snippet": "iit bombay # › when should i do the literature review at the beginning, when you start looking for a problem"}
{"id": "RM/updated_iit_survey.pptx#p5#c1", "source": "RM/updated_iit_survey.pptx", "page": 5, "snippet": "iit bombay # › when should i do the literature review at the beginning, when you start looking for a problem in the middle, once your work is under wa"}
{"id": "RM/updated_iit_survey.pptx#p6#c1", "source": "RM/updated_iit_survey.pptx", "page": 6, "snippet": "iit bombay # › when should i do the literature review at the beginning, when you start looking for a problem in the middle, once your work is under wa"}
{"id": "RM/updated_iit_survey.pptx#p7#c1", "source": "RM/updated_iit_survey.pptx", "page": 7, "snippet": "iit bombay # › when should i do the literature review at the beginning, when you start looking for a problem in the middle, once your work is under wa"}
{"id": "RM/updated_iit_survey.pptx#p8#c1", "source": "RM/updated_iit_survey.pptx", "page": 8, "snippet": "iit bombay # › what type of papers should i look for? engineering / scientific research papers published research papers in “ established ” journals, "}
{"id": "RM/updated_iit_survey.pptx#p9#c1", "source": "RM/updated_iit_survey.pptx", "page": 9, "snippet": "iit bombay # › what type of papers should i look for? engineering / scientific research papers published research papers in “ established ” journals, "}
{"id": "RM/updated_iit_survey.pptx#p10#c1", "source": "RM/updated_iit_survey.pptx", "page": 10, "snippet": "iit bombay # › where should i look for papers? databases and indexes inspec web of science ieee xplore compendex scifinder scopus check your field jou"}
{"id": "RM/updated_iit_survey.pptx#p11#c1", "source": "RM/updated_iit_survey.pptx", "page": 11, "snippet": "iit bombay # › where should i look for papers? what about google?"}
{"id": "RM/updated_iit_survey.pptx#p12#c1", "source": "RM/updated_iit_survey.pptx", "page": 12, "snippet": "iit bombay # › where should i look for papers? what about google? google scholar search more appropriate than simple google search. also try citeseer"}
{"id": "RM/updated_iit_survey.pptx#p13#c1", "source": "RM/updated_iit_survey.pptx", "page": 13, "snippet": "iit bombay # › what information should i track bibliographic information title authors year of publication source – journal / proceedings / book name "}
{"id": "RM/updated_iit_survey.pptx#p14#c1", "source": "RM/updated_iit_survey.pptx", "page": 14, "snippet": "iit bombay # › how should i select papers to read? some tips ask your guide survey / review articles are a good starting point try to identify seminal"}
{"id": "RM/updated_iit_survey.pptx#p15#c1", "source": "RM/updated_iit_survey.pptx", "page": 15, "snippet": "iit bombay # › what to do if i find too few sources discuss with expert, guide look for related topics, synonyms broaden scope of topic a little, then"}
{"id": "RM/updated_iit_survey.pptx#p16#c1", "source": "RM/updated_iit_survey.pptx", "page": 16, "snippet": "iit bombay # › what to do if i find too many sources plan to scope or scale down your search restrict by year ( recent ) restrict by source ( only two"}
{"id": "RM/updated_iit_survey.pptx#p17#c1", "source": "RM/updated_iit_survey.pptx", "page": 17, "snippet": "iit bombay # › how to report literature search"}
{"id": "RM/updated_iit_survey.pptx#p18#c1", "source": "RM/updated_iit_survey.pptx", "page": 18, "snippet": "iit bombay # › according to paper1, clickers were found to be improve student motivation and attendance. in paper2, authors used clickers in a cs prog"}
{"id": "RM/updated_iit_survey.pptx#p19#c1", "source": "RM/updated_iit_survey.pptx", "page": 19, "snippet": "iit bombay # › how not to write the literature review / related work section according to paper1, clickers were found to be improve student motivation"}
{"id": "RM/updated_iit_survey.pptx#p20#c1", "source": "RM/updated_iit_survey.pptx", "page": 20, "snippet": "iit bombay # › how not to write the literature review / related work section a literature review is not : a descriptive list of papers a summary of on"}
{"id": "RM/updated_iit_survey.pptx#p21#c1", "source": "RM/updated_iit_survey.pptx", "page": 21, "snippet": "iit bombay # › how should i write the literature review / related work section identify themes, factors, or variables relevant to your problem. one wa"}
{"id": "RM/updated_iit_survey.pptx#p22#c1", "source": "RM/updated_iit_survey.pptx", "page": 22, "snippet": "iit bombay # › how should i write the literature review / related work section analyze on the basis of categories, strengths & weaknesses"}
{"id": "RM/updated_iit_survey.pptx#p23#c1", "source": "RM/updated_iit_survey.pptx", "page": 23, "snippet": "iit bombay # › how should i write the literature review / related work section analyze on the basis of categories, strengths & weaknesses 1 ) clickers"}
{"id": "RM/updated_iit_survey.pptx#p24#c1", "source": "RM/updated_iit_survey.pptx", "page": 24, "snippet": "iit bombay # › how should i write the literature review / related work section provide synthesis of the reviewed literature. what are existing solutio"}
{"id": "RM/updated_iit_survey.pptx#p25#c1", "source": "RM/updated_iit_survey.pptx", "page": 25, "snippet": "iit bombay # › how should i write the literature review / related work section identify the gaps in existing work. in most current solutions aimed at "}
{"id": "RM/updated_iit_survey.pptx#p26#c1", "source": "RM/updated_iit_survey.pptx", "page": 26, "snippet": "iit bombay # › how should i write the literature review / related work section identify the gaps in existing work. in most current solutions aimed at "}
{"id": "RM/updated_iit_survey.pptx#p27#c1", "source": "RM/updated_iit_survey.pptx", "page": 27, "snippet": "iit bombay # › points to note be aware of disciplinary conventions"}
{"id": "RM/updated_iit_survey.pptx#p28#c1", "source": "RM/updated_iit_survey.pptx", "page": 28, "snippet": "iit bombay # › points to note you have to write about what has already been written, but write something original."}
{"id": "RM/book 1 (1).pdf#p1#c1", "source": "RM/book 1 (1).pdf", "page": 1, "snippet": "me research methods for engineers | david v. thiel restricted south asia edition this edition is licensed for sale in india, bangladesh, bhutan, maldi"}
{"id": "RM/book 1 (1).pdf#p2#c1", "source": "RM/book 1 (1).pdf", "page": 2, "snippet": "a research methods for engineers learn how to plan for success with this hands - on guide to c ducting high - quality engineering research. plan and i"}
{"id": "RM/book 1 (1).pdf#p3#c1", "source": "RM/book 1 (1).pdf", "page": 3, "snippet": "research — methods for engineers david v. thiel. griffith university, australia i ea eter"}
{"id": "RM/book 1 (1).pdf#p4#c1", "source": "RM/book 1 (1).pdf", "page": 4, "snippet": "cambridge university press 4843 / 24, 2nd floor, ansari road, daryaganj, delhi - 110002, india cambridge university press is part of the university of"}
{"id": "RM/book 1 (1).pdf#p5#c1", "source": "RM/book 1 (1).pdf", "page": 5, "snippet": "i contents developing a research plan 3. 1 research proposals 3. 2 finding a suitable research question 3. 3 the elements of a research proposal 3. 4 "}
{"id": "RM/book 1 (1).pdf#p6#c1", "source": "RM/book 1 (1).pdf", "page": 6, "snippet": "preface this book is unashamedly idealistic. it aims first to convey to engineers and engineering undergraduates interested in conduct - ing research "}
{"id": "RM/book 1 (1).pdf#p7#c1", "source": "RM/book 1 (1).pdf", "page": 7, "snippet": "preface the course required the students to submit three written assign - ments using engineering journal format ( a template was provided ) and appro"}
{"id": "RM/book 1 (1).pdf#p8#c1", "source": "RM/book 1 (1).pdf", "page": 8, "snippet": "an introductory note for instructors this course was run at master ’ s degree level at griffith university for engineering graduates. for convenience,"}
{"id": "RM/book 1 (1).pdf#p9#c1", "source": "RM/book 1 (1).pdf", "page": 9, "snippet": "| introduction to engineering research"}
{"id": "RM/book 1 (1).pdf#p10#c1", "source": "RM/book 1 (1).pdf", "page": 10, "snippet": "reqtarla meimuus fur eroineers 1. 1 why engineering research? the disciplines of engineering are all described as the applica - tion of science to rea"}
{"id": "RM/book 1 (1).pdf#p11#c1", "source": "RM/book 1 (1).pdf", "page": 11, "snippet": "4 research methods for engineers a more detailed explanation of these issues is provided in the following chapters. ; clearly scientific research and "}
{"id": "RM/book 1 (1).pdf#p12#c1", "source": "RM/book 1 (1).pdf", "page": 12, "snippet": "6 research methods for engineers similarly, a search of previous publications and patents does not constitute research. thus, when a primary school ch"}
{"id": "RM/book 1 (1).pdf#p13#c1", "source": "RM/book 1 (1).pdf", "page": 13, "snippet": "8 research methods for engineers nal research opport introduction to engineering research 9 research questions a common method of focusing on a resear"}
{"id": "RM/book 1 (1).pdf#p14#c1", "source": "RM/book 1 (1).pdf", "page": 14, "snippet": "10 research methods for engineers what? example 1. 6 research question ‘ what? ’ how? example 1. 7 research question ‘ how? ’ when? example 1. 8 resea"}
{"id": "RM/book 1 (1).pdf#p15#c1", "source": "RM/book 1 (1).pdf", "page": 15, "snippet": "12 research methods for engineers 1. 4 engineering ethics as engineers are involved in all major infrastructure projects ( dams, bridges, roads, railw"}
{"id": "RM/book 1 (1).pdf#p16#c1", "source": "RM/book 1 (1).pdf", "page": 16, "snippet": "14 research methods for engineers also prohibit engineers from practising outside their engineering discipline. while most engineering codes of ethics"}
{"id": "RM/book 1 (1).pdf#p17#c1", "source": "RM/book 1 (1).pdf", "page": 17, "snippet": "16 research methods for engineers introduction to engineering research 17 when the research involves the use of humans or animals, ‘ the participants "}
{"id": "RM/book 1 (1).pdf#p18#c1", "source": "RM/book 1 (1).pdf", "page": 18, "snippet": "18 i research methods for engineers what constitutes conclusive proof? research is designed to create new knowledge. this new knowl - edge needs to be"}
{"id": "RM/book 1 (1).pdf#p19#c1", "source": "RM/book 1 (1).pdf", "page": 19, "snippet": "20 research methods for engineers troduction to engineering research in mathematics, the concept of upper and lower limits can be applied to gain info"}
{"id": "RM/book 1 (1).pdf#p20#c1", "source": "RM/book 1 (1).pdf", "page": 20, "snippet": "22 research methods for engineers the process of research training is part of every engineering undergraduate degree programme. every laboratory exper"}
{"id": "RM/book 1 (1).pdf#p21#c1", "source": "RM/book 1 (1).pdf", "page": 21, "snippet": "24 research methods for engineers 1. 1 use an academic web search to locate a journal paper which describes a design outcome in your field of interest"}
{"id": "RM/book 1 (1).pdf#p22#c1", "source": "RM/book 1 (1).pdf", "page": 22, "snippet": "26 research methods for engineers [ 10 ] american society of civil engineers, ‘ code of ethics, fundamental principles ’, http : / / www. asce. org / "}
{"id": "RM/book 1 (1).pdf#p23#c1", "source": "RM/book 1 (1).pdf", "page": 23, "snippet": "28 evd * of this is now stored research methods for engineers archival literature the world ’ s total knowledge in the fields of science and engineer "}
{"id": "RM/book 1 (1).pdf#p24#c1", "source": "RM/book 1 (1).pdf", "page": 24, "snippet": "30 research methods for engineers 2. 2 why should engineers be ethical? there are many answers to this question ; some are carrots ( = incentives ) an"}
{"id": "RM/book 1 (1).pdf#p25#c1", "source": "RM/book 1 (1).pdf", "page": 25, "snippet": "32 research methods for engineers incorrect observations, experimental errors, misinterpretation of data, etc. should a person accept and copy informa"}
{"id": "RM/book 1 (1).pdf#p26#c1", "source": "RM/book 1 (1).pdf", "page": 26, "snippet": "34 research methods for engineers 2. 3 types of publications whatever you say may fade away whatever you write might come back and bite while the spok"}
{"id": "RM/book 1 (1).pdf#p27#c1", "source": "RM/book 1 (1).pdf", "page": 27, "snippet": "p1padiyim sjuawasiy pl4awojuy sa ] 2e sadeds sauizebe apes sasayy s } uaqeg spuepuejs syoog and their characte : istics. a n es tha scnp is va tick in"}
{"id": "RM/book 1 (1).pdf#p28#c1", "source": "RM/book 1 (1).pdf", "page": 28, "snippet": "38 research methods for engineers the wish that the time between submission and publication be as short as possible. a period of six weeks for the ret"}
{"id": "RM/book 1 (1).pdf#p29#c1", "source": "RM/book 1 (1).pdf", "page": 29, "snippet": "40 research methods for engineers front pages of each issue. while most journal names are unique ( world - wide ), it is important for authors to use "}
{"id": "RM/book 1 (1).pdf#p30#c1", "source": "RM/book 1 (1).pdf", "page": 30, "snippet": "42 research methods for engineers 2 : 3ke when submitting a paper for review, the authors are required to state that their article ( in full or in par"}
{"id": "RM/book 1 (1).pdf#p31#c1", "source": "RM/book 1 (1).pdf", "page": 31, "snippet": "44 research methods for engineers compared to the journal review and publication process. com - monly conference papers are submitted six months befor"}
{"id": "RM/book 1 (1).pdf#p32#c1", "source": "RM/book 1 (1).pdf", "page": 32, "snippet": "46 research methods for engineers = textbooks ; ® research books ( monographs ) ; and ® reference books. textbooks are used in undergraduate and postg"}
{"id": "RM/book 1 (1).pdf#p33#c1", "source": "RM/book 1 (1).pdf", "page": 33, "snippet": "48 2i3. 6 research methods for engineers examples of standards it is wise for a research team to use measurement tech - niques and numerical computati"}
{"id": "RM/book 1 (1).pdf#p34#c1", "source": "RM/book 1 (1).pdf", "page": 34, "snippet": "50 research methods for engineers international patents treaty between nations ensures that there is some uniformity between the national patent offic"}
{"id": "RM/book 1 (1).pdf#p35#c1", "source": "RM/book 1 (1).pdf", "page": 35, "snippet": "52 2. 5. 6 2. 3. 9 research methods for engineers relevant thesis. researchers need to remain cautious about the accuracy of the material in theses. t"}
{"id": "RM/book 1 (1).pdf#p36#c1", "source": "RM/book 1 (1).pdf", "page": 36, "snippet": "54 research methods for engineers pee et em et he et reg if te lial freda hild z | le ge ‘ a3 i ii! ily ay a ee = hil i : tpe. hl hae ain | a a : ~ hu"}
{"id": "RM/book 1 (1).pdf#p37#c1", "source": "RM/book 1 (1).pdf", "page": 37, "snippet": "56 research methods for engineers 2. 3, 13 engineering and scientific papers should not include references to wikipedia directly because it is subject"}
{"id": "RM/book 1 (1).pdf#p38#c1", "source": "RM/book 1 (1).pdf", "page": 38, "snippet": "28 roqearlh mein fur eruaneerd paper. however, citation counts do constitute a measure of interest in a particular paper. it should be noted that most"}
{"id": "RM/book 1 (1).pdf#p39#c1", "source": "RM/book 1 (1).pdf", "page": 39, "snippet": "60 research methods for engineers 2. 5 literature review it is mandatory that all research is grounded in the previous sci - entific and engineering k"}
{"id": "RM/book 1 (1).pdf#p40#c1", "source": "RM/book 1 (1).pdf", "page": 40, "snippet": "62 research methods for engineers journal has an individual reference style, so many of the citations must be reformatted to fit the required style.. "}
{"id": "RM/book 1 (1).pdf#p41#c1", "source": "RM/book 1 (1).pdf", "page": 41, "snippet": "64 research methods for engineers 2. 6 keywords keywords are a critical part of the publication process if a pub - lication is to be found by the inte"}
{"id": "RM/book 1 (1).pdf#p42#c1", "source": "RM/book 1 (1).pdf", "page": 42, "snippet": "66 research methods for engineers 2. 7 publication cost as the publication of research papers has become mandatory for must academics around the world"}
{"id": "RM/book 1 (1).pdf#p43#c1", "source": "RM/book 1 (1).pdf", "page": 43, "snippet": "68 research methods for engineers 2. 7. 2 conference registration fee which defrays publishing costs. with many conferences, a person who attends the "}
{"id": "RM/book 1 (1).pdf#p44#c1", "source": "RM/book 1 (1).pdf", "page": 44, "snippet": "70 21 2. 2 2. 3 2. 4 research methods for engineers panied nai aon oi baal sr da a dey exercises citation comparison : consider a journal article in y"}
{"id": "RM/book 1 (1).pdf#p45#c1", "source": "RM/book 1 (1).pdf", "page": 45, "snippet": "72 research methods for engineers [ 1 ] newton, l., the mathematical principles of natural philosophy, trans. a. motte, london : middle - temple gate,"}
{"id": "RM/book 1 (1).pdf#p46#c1", "source": "RM/book 1 (1).pdf", "page": 46, "snippet": "ng a research plan 75 74 research methods for engineers research team is presented in the best possible light, i the expertise necessary to achieve th"}
{"id": "RM/book 1 (1).pdf#p47#c1", "source": "RM/book 1 (1).pdf", "page": 47, "snippet": "76 research methods for engineers 3. 2 finding a suitable research question one challenge in undertaking research is to find a place to start. where c"}
{"id": "RM/book 1 (1).pdf#p48#c1", "source": "RM/book 1 (1).pdf", "page": 48, "snippet": "78 research methods for engineers in order to develop this idea, the team needs to study and understand the theory, experimental methods, data analysi"}
{"id": "RM/book 1 (1).pdf#p49#c1", "source": "RM/book 1 (1).pdf", "page": 49, "snippet": "78 research methods for engineers in order to develop this idea, the team needs to study and understand the theory, experimental methods, data analysi"}
{"id": "RM/book 1 (1).pdf#p50#c1", "source": "RM/book 1 (1).pdf", "page": 50, "snippet": "80 research methods for engineers project title research team members ( qualifications & experience ) affiliations & institutions contact details proj"}
{"id": "RM/book 1 (1).pdf#p51#c1", "source": "RM/book 1 (1).pdf", "page": 51, "snippet": "82 research methods for engineers 3. 333 will make a substantial contribution to the project. at the plan - ning stages it is important to make it cle"}
{"id": "RM/book 1 (1).pdf#p52#c1", "source": "RM/book 1 (1).pdf", "page": 52, "snippet": "84 research methods for engineers 3. 3. 4 project outline as is the case for all research reports ( as described in chapter 7 ), the project outline s"}
{"id": "RM/book 1 (1).pdf#p53#c1", "source": "RM/book 1 (1).pdf", "page": 53, "snippet": "86 research methods for engineers their results might be presented. in most cases of engineering research, the number of constraining variables greatl"}
{"id": "RM/book 1 (1).pdf#p54#c1", "source": "RM/book 1 (1).pdf", "page": 54, "snippet": "88 research methods for engineers [ work qi ] a2 ] a3 [ as literalure revi xx te q7 [ a8 [ a9 faro ant t bl equipment arrival x _ [ x to ni see pees x"}
{"id": "RM/book 1 (1).pdf#p55#c1", "source": "RM/book 1 (1).pdf", "page": 55, "snippet": "90 research methods for engineers ng model fo the publicity sought from the award of the research contract? if the research team can argue that the su"}
{"id": "RM/book 1 (1).pdf#p56#c1", "source": "RM/book 1 (1).pdf", "page": 56, "snippet": "92 research methods for engineers esearch contracts additional human resources not listed by name as researchers and not named specifically in the app"}
{"id": "RM/book 1 (1).pdf#p57#c1", "source": "RM/book 1 (1).pdf", "page": 57, "snippet": "94 research methods for engineers 3. 4 design for outcomes the research plan must be designed to give definitive answers to the research question. thi"}
{"id": "RM/book 1 (1).pdf#p58#c1", "source": "RM/book 1 (1).pdf", "page": 58, "snippet": "96 research methods for engineers 06 08 12 16 of ) bins figure 3. 3 two hundred voltage measurements were made from a strain gauge attached to a stati"}
{"id": "RM/book 1 (1).pdf#p59#c1", "source": "RM/book 1 (1).pdf", "page": 59, "snippet": "98 research methods for engineers 3. 4. 3 now that the one - dimensional problem has developed into a two - dimensional problem, so also the possibili"}
{"id": "RM/book 1 (1).pdf#p60#c1", "source": "RM/book 1 (1).pdf", "page": 60, "snippet": "100 research methods for engineers research plan v! loping a research plan e research tools ‘ every research project, the challenge is to validate the"}
{"id": "RM/book 1 (1).pdf#p61#c1", "source": "RM/book 1 (1).pdf", "page": 61, "snippet": "102 research methods for engineers 3. 5. 1 experimental measurements in most cases, standard measurement tools provide the best method of gaining reli"}
{"id": "RM/book 1 (1).pdf#p62#c1", "source": "RM/book 1 (1).pdf", "page": 62, "snippet": "104 research methods for engineers prob! ich i = ese involve more than one set of physical equa s are referred to as ‘ multi ics ’ e ; - physics ’ pro"}
{"id": "RM/book 1 (1).pdf#p63#c1", "source": "RM/book 1 (1).pdf", "page": 63, "snippet": "106 research methods for engineers the result. a visual inspection of the line will suggest the level of random error and data ranges where the relati"}
{"id": "RM/book 1 (1).pdf#p64#c1", "source": "RM/book 1 (1).pdf", "page": 64, "snippet": "108 research methods for engineers 08 4 ~ 08 : s oa } r, ibe 0. 2! z 4 ss \\ f & if 5.. s 2 02 8 \\ fd s ~ 0. 4 + i £ fandee t | 08 ar ) 60 80 100 12 14"}
{"id": "RM/book 1 (1).pdf#p65#c1", "source": "RM/book 1 (1).pdf", "page": 65, "snippet": "110 research methods for engineers 3. 6 3. 1 chapter summary a well prepared research plan will articulate the nature of the research question to be s"}
{"id": "RM/book 1 (1).pdf#p66#c1", "source": "RM/book 1 (1).pdf", "page": 66, "snippet": "2 3. 8 3. 9 3. 10 3. 11 research methods for engineers and contributions of any persons acknowledged at the end of the paper. with this list of resear"}
{"id": "RM/book 1 (1).pdf#p67#c1", "source": "RM/book 1 (1).pdf", "page": 67, "snippet": "read rs i ess stl sll ea no et, statistical analysis statistical analysis 115 — — — challenge in research is to prove observations and conclu - ‘ beyo"}
{"id": "RM/book 1 (1).pdf#p68#c1", "source": "RM/book 1 (1).pdf", "page": 68, "snippet": "116 research methods for engineers simply a deviation from a specification ) are unlikely to be broadly acceptable to the engineering industry and its"}
{"id": "RM/book 1 (1).pdf#p69#c1", "source": "RM/book 1 (1).pdf", "page": 69, "snippet": "118 research methods for engineers figure 4. 1 a linear approximation to the response of a measurement system is defined in terms of the ratio ax / xm"}
{"id": "RM/book 1 (1).pdf#p70#c1", "source": "RM/book 1 (1).pdf", "page": 70, "snippet": "120 research methods for engineers the absolute error is the difference between the measured value and the true value. unfortunately there is no simpl"}
{"id": "RM/book 1 (1).pdf#p71#c1", "source": "RM/book 1 (1).pdf", "page": 71, "snippet": "122 research methods for engineers 4. 3 one - dimensional statistics a research paper reports a distance measurement of 10. 5 m. the implication of th"}
{"id": "RM/book 1 (1).pdf#p72#c1", "source": "RM/book 1 (1).pdf", "page": 72, "snippet": "124 research methods for engineers figure 4. 2 the normal distribution of an infinite population ( n > oo ) when the mean value. = 0 and the standard "}
{"id": "RM/book 1 (1).pdf#p73#c1", "source": "RM/book 1 (1).pdf", "page": 73, "snippet": "126 research methods for engineers x can be scaled using the transformation equation : ( 4. 7 ) the z value ( called the z - score ) representation of"}
{"id": "RM/book 1 (1).pdf#p74#c1", "source": "RM/book 1 (1).pdf", "page": 74, "snippet": "128 research methods for engineers of skewness might also be explained by the influence of an addi - tional variable which contributes to the populati"}
{"id": "RM/book 1 (1).pdf#p75#c1", "source": "RM/book 1 (1).pdf", "page": 75, "snippet": "130 4. 3. 2 research methods for engineers / paar me ale i al. lal is nonlinear, then an input number with equal limits will result in an output numbe"}
{"id": "RM/book 1 (1).pdf#p76#c1", "source": "RM/book 1 (1).pdf", "page": 76, "snippet": "132 research methods for engineers the t test can be conveniently evaluated using the ms excel func - tion ttest - paired and ttest - unpaired. in mat"}
{"id": "RM/book 1 (1).pdf#p77#c1", "source": "RM/book 1 (1).pdf", "page": 77, "snippet": "134 research methods for engineers figure 4. 3 a painted wooden post serves as a river height gauge in queensland, australia. he it \" the initial assu"}
{"id": "RM/book 1 (1).pdf#p78#c1", "source": "RM/book 1 (1).pdf", "page": 78, "snippet": "136 research methods for engineers statistical analysis ad = the river height has risen by an average of 0. 92 + 0. 06 m. = there is a 2. 5 % chance t"}
{"id": "RM/book 1 (1).pdf#p79#c1", "source": "RM/book 1 (1).pdf", "page": 79, "snippet": "138 research methods for engineers where ho is the river height at t = 0 and m is the rate of change of the height with time ( the slope of the line )"}
{"id": "RM/book 1 (1).pdf#p80#c1", "source": "RM/book 1 (1).pdf", "page": 80, "snippet": "140 research methods for engineers option 2 : this strategy would be to scatter plot the data versus time and perform a linear regression analysis bet"}
{"id": "RM/book 1 (1).pdf#p81#c1", "source": "RM/book 1 (1).pdf", "page": 81, "snippet": "142 research methods for engineers 1. 67 > a — we — — 1. 664 e 1. 65 4 yuh 4 a river height ( m ) z e = oak 15ge es we 4. 1 1 9 9. 05 91 9. 15 02 9. 2"}
{"id": "RM/book 1 (1).pdf#p82#c1", "source": "RM/book 1 (1).pdf", "page": 82, "snippet": "144 research methods for engineers = there is no statistically strong mathematical function which can be used to describe the rise and fall of the riv"}
{"id": "RM/book 1 (1).pdf#p83#c1", "source": "RM/book 1 (1).pdf", "page": 83, "snippet": "146 4. 5. 1 research methods for engineers equation qr fe al ae ( 4. 18 ) ii qu where qz is given by n or = ys, [ yj — ( ao + ayy + @ 222i ) ) ’, ( 4."}
{"id": "RM/book 1 (1).pdf#p84#c1", "source": "RM/book 1 (1).pdf", "page": 84, "snippet": "148 research methods for engineers table 4. 2 a data record for quarry blasting. y / n indicates whether rain fell in the days ( 1 - 5 ) before the bl"}
{"id": "RM/book 1 (1).pdf#p85#c1", "source": "RM/book 1 (1).pdf", "page": 85, "snippet": "150 research methods for engineers 4. 6 null hypothesis testing the objective of research projects is to provide an answer to a carefully phrased rese"}
{"id": "RM/book 1 (1).pdf#p86#c1", "source": "RM/book 1 (1).pdf", "page": 86, "snippet": "152 research methods for engineers 4. 7 chapter summary engineering researchers must engage in the statistical analysis of their results in order to p"}
{"id": "RM/book 1 (1).pdf#p87#c1", "source": "RM/book 1 (1).pdf", "page": 87, "snippet": "154 research methods for engineers and perform a 2d and multi - dimensional analysis on the data. if appropriate, calculate the partial correlation co"}
{"id": "RM/book 1 (1).pdf#p88#c1", "source": "RM/book 1 (1).pdf", "page": 88, "snippet": "optimization techniques optimization techniques 157 | introduction almost all engineering design problems are multi - parameter prob - lems. for examp"}
{"id": "RM/book 1 (1).pdf#p89#c1", "source": "RM/book 1 (1).pdf", "page": 89, "snippet": "158 research methods for engineers dimensions of an object are determined by engineering standards or previous commonly accepted sizes. while the phys"}
{"id": "RM/book 1 (1).pdf#p90#c1", "source": "RM/book 1 (1).pdf", "page": 90, "snippet": "160 research methods for engineers determined. this process is referred to as inverse modelling or optimization. the cost function does not have to be"}
{"id": "RM/book 1 (1).pdf#p91#c1", "source": "RM/book 1 (1).pdf", "page": 91, "snippet": "162 research methods for engineers 5. 2 two - parameter optimization methods the methods commonly used to optimize a design problem are most simply il"}
{"id": "RM/book 1 (1).pdf#p92#c1", "source": "RM/book 1 (1).pdf", "page": 92, "snippet": "164 research methods for engineers fi | nea aon : rarity ga m2004 006008 2 ote 018 o18 02 oe time ( s ) figure 5. 2 experimentally determined accelera"}
{"id": "RM/book 1 (1).pdf#p93#c1", "source": "RM/book 1 (1).pdf", "page": 93, "snippet": "166 | research methods for engineers by physical processes ( such as simulated annealing [ 10 ] ). in the following subsections, four techniques are i"}
{"id": "RM/book 1 (1).pdf#p94#c1", "source": "RM/book 1 (1).pdf", "page": 94, "snippet": "168 research methods for engineers optimization techniques applied more than once. the first attempt should use a coarse grid ( a ) with large increme"}
{"id": "RM/book 1 (1).pdf#p95#c1", "source": "RM/book 1 (1).pdf", "page": 95, "snippet": "170 research methods for engineers ( a ) frequency ( rad / s ) 0. 15 02 amplitude 025 03 ( b ) 7 0. 35 5p oes 03 0. 15 frequency ( rad / s ) oo. 08 am"}
{"id": "RM/book 1 (1).pdf#p96#c1", "source": "RM/book 1 (1).pdf", "page": 96, "snippet": "172 research methods for engineers 45 35 a tae 2500 3000 1500 2000 iteration number figure 5. 7 the variation in the cost function c plotted for every"}
{"id": "RM/book 1 (1).pdf#p97#c1", "source": "RM/book 1 (1).pdf", "page": 97, "snippet": "174 research methods for engineers set l & wrange and increments ol & 6 @ count = 0 calculate c increment count save c, l & @ select l & @ for minimum"}
{"id": "RM/book 1 (1).pdf#p98#c1", "source": "RM/book 1 (1).pdf", "page": 98, "snippet": "176 research methods for engineers calculate ul 03 ), c ( ls1509 ) & c ( lis0 ) s1 ) step 1, & @ figure 5. 11 the simplex algorithm flow chart. the st"}
{"id": "RM/book 1 (1).pdf#p99#c1", "source": "RM/book 1 (1).pdf", "page": 99, "snippet": "178 research methods for engineers 5. 2. 4 ) 10 20 30 7 7 ) — 40 50 60 iteration number figure 5. 13 the variation in c as a function of iteration num"}
{"id": "RM/book 1 (1).pdf#p100#c1", "source": "RM/book 1 (1).pdf", "page": 100, "snippet": "180 research methods for engineers set l & wranges and dl & do randomly select li & @ calculate c ( l ;, « ), c ( list, @ ), c ( li, @ + 1 ) & vc + 6l"}
{"id": "RM/book 1 (1).pdf#p101#c1", "source": "RM/book 1 (1).pdf", "page": 101, "snippet": "182 research methods for engineers _ optimization techniques 5. 2. 5 summary sped of convergence is significantly different for each o ti - mization m"}
{"id": "RM/book 1 (1).pdf#p102#c1", "source": "RM/book 1 (1).pdf", "page": 102, "snippet": "184 research methods for engineers there are several additional strategies used to reduce the com - putational time required for multi - parameter inv"}
{"id": "RM/book 1 (1).pdf#p103#c1", "source": "RM/book 1 (1).pdf", "page": 103, "snippet": "186 research methods for engineers 5. 4 the cost function the selection of the cost function is of critical importance to all optimization algorithms."}
{"id": "RM/book 1 (1).pdf#p104#c1", "source": "RM/book 1 (1).pdf", "page": 104, "snippet": "188 research methods for engineers the cost function can include a wide variety of parameters including those which are discrete ( such as an option t"}
{"id": "RM/book 1 (1).pdf#p105#c1", "source": "RM/book 1 (1).pdf", "page": 105, "snippet": "190 5. 2 5. 3 5. 4 5. 5 research methods for engineers cost function. locate a published paper in your engineering dis cipline in which optimization h"}
{"id": "RM/book 1 (1).pdf#p106#c1", "source": "RM/book 1 (1).pdf", "page": 106, "snippet": "ans ts ss sear survey research methods survey research methods 193 why undertake a survey? engineering research only has value when it directly or ind"}
{"id": "RM/book 1 (1).pdf#p107#c1", "source": "RM/book 1 (1).pdf", "page": 107, "snippet": "194 survey research methods ' survey research methods 195 or non - intended users who might have accidental exposure to the system. the only method of"}
{"id": "RM/book 1 (1).pdf#p108#c1", "source": "RM/book 1 (1).pdf", "page": 108, "snippet": "196 survey research methods survey research methods 197 162 peat ae ee ts 178 176 ) stature ( % ) 174 172 170 ) % a | 40 6070 80 age ( years ) figure "}
{"id": "RM/book 1 (1).pdf#p109#c1", "source": "RM/book 1 (1).pdf", "page": 109, "snippet": "198 survey research methods the population must be sufficiently diverse to ensure that population groupings are not subjected to adverse stereotyping,"}
{"id": "RM/book 1 (1).pdf#p110#c1", "source": "RM/book 1 (1).pdf", "page": 110, "snippet": "200 survey research methods 6. 2 ergonomics and human factors the human engineering specification and the normal range of human functions is the subje"}
{"id": "RM/book 1 (1).pdf#p111#c1", "source": "RM/book 1 (1).pdf", "page": 111, "snippet": "202 survey research methods a many of these issues require feedback from the population. the role of a research team engaged in human factors research"}
{"id": "RM/book 1 (1).pdf#p112#c1", "source": "RM/book 1 (1).pdf", "page": 112, "snippet": "204 survey research methods as a survey involves seeking the opinions or personal details of others, ethics approval to conduct the research is mandat"}
{"id": "RM/book 1 (1).pdf#p113#c1", "source": "RM/book 1 (1).pdf", "page": 113, "snippet": "206 survey research methods f es = a survey title | introduction ( outline the reasons for the survey, the survey team, the ethics approval number, re"}
{"id": "RM/book 1 (1).pdf#p114#c1", "source": "RM/book 1 (1).pdf", "page": 114, "snippet": "208 survey research methods section 5 : open ended questions and comments the research team can learn more about the thoughts, background and motivati"}
{"id": "RM/book 1 (1).pdf#p115#c1", "source": "RM/book 1 (1).pdf", "page": 115, "snippet": "210 survey research methods 6. 5. 2 statements must be short surveys must contain concise statements without lengthy explanations. in addition the num"}
{"id": "RM/book 1 (1).pdf#p116#c1", "source": "RM/book 1 (1).pdf", "page": 116, "snippet": "212 survey research methods 6. 5. 5 6. 5. 6 phrases or negative and positive statements to ‘ check ’ the authen ticity of the responses, trying to tri"}
{"id": "RM/book 1 (1).pdf#p117#c1", "source": "RM/book 1 (1).pdf", "page": 117, "snippet": "214 survey research methods 6. 6 survey delivery 6. 6. 1 a number of different methods can be used to distribute the sur vey. all have advantages and "}
{"id": "RM/book 1 (1).pdf#p118#c1", "source": "RM/book 1 (1).pdf", "page": 118, "snippet": "216 survey research methods survey research methods 217 6. 6. 3 on - line surveys through random selection of email addresses or using social media, t"}
{"id": "RM/book 1 (1).pdf#p119#c1", "source": "RM/book 1 (1).pdf", "page": 119, "snippet": "218 survey research methods details of the respondent must be included in the response and anonymity is compromised. for this reason, the ethics appro"}
{"id": "RM/book 1 (1).pdf#p120#c1", "source": "RM/book 1 (1).pdf", "page": 120, "snippet": "220 survey research methods 6. 8 survey timelines once the research question has been decided and the reseurel ) methodology includes a survey ( often"}
{"id": "RM/book 1 (1).pdf#p121#c1", "source": "RM/book 1 (1).pdf", "page": 121, "snippet": "222 survey research methods 6. 9 statistical analysis a 5 point survey with 50 respondents and 25 statements ( includ - ing demographics ) results in "}
{"id": "RM/book 1 (1).pdf#p122#c1", "source": "RM/book 1 (1).pdf", "page": 122, "snippet": "224 survey research methods 6. 10 reporting the usual scientific style of reporting is required to define the research outcomes for publication in the"}
{"id": "RM/book 1 (1).pdf#p123#c1", "source": "RM/book 1 (1).pdf", "page": 123, "snippet": "226 survey research methods 6. 11 chapter summary the work of engineers is designed to improve the human con - dition. this requires an understanding "}
{"id": "RM/book 1 (1).pdf#p124#c1", "source": "RM/book 1 (1).pdf", "page": 124, "snippet": "228 6. 3 6. 4 6. 5 6. 6 6. 7 survey research methods apply the knowledge gained from reading this chapter to a survey you have been asked to complete."}
{"id": "RM/book 1 (1).pdf#p125#c1", "source": "RM/book 1 (1).pdf", "page": 125, "snippet": "research presentation research presentation 231 introduction assume that the process of research is approaching a conclusion. the research team has wr"}
{"id": "RM/book 1 (1).pdf#p126#c1", "source": "RM/book 1 (1).pdf", "page": 126, "snippet": "232 research methods for engineers broad perspective und / literature r. perspective to a narrow description of the ws interpretation, and then conclu"}
{"id": "RM/book 1 (1).pdf#p127#c1", "source": "RM/book 1 (1).pdf", "page": 127, "snippet": "234 research methods for engineers no cost. not only that, the research team has the opportunity ( \\ ain the attention of eminent researchers in the f"}
{"id": "RM/book 1 (1).pdf#p128#c1", "source": "RM/book 1 (1).pdf", "page": 128, "snippet": "236 research methods for engineers a third simple rule is to use symbols in equations that are consis ~ tent with textbooks and other papers. all symb"}
{"id": "RM/book 1 (1).pdf#p129#c1", "source": "RM/book 1 (1).pdf", "page": 129, "snippet": "238 research methods for engineers research presentation 239 paper title and keywords the research team must write a title for their research paper wh"}
{"id": "RM/book 1 (1).pdf#p130#c1", "source": "RM/book 1 (1).pdf", "page": 130, "snippet": "240 research methods for engineers time between submission and a decision ( accept, revise or reject ) is minimal. a careful selection of keywords is "}
{"id": "RM/book 1 (1).pdf#p131#c1", "source": "RM/book 1 (1).pdf", "page": 131, "snippet": "242 research methods for engineers example research presentation 243 one sentence describes the research context - what is the general field of resear"}
{"id": "RM/book 1 (1).pdf#p132#c1", "source": "RM/book 1 (1).pdf", "page": 132, "snippet": "244 research methods for engineers 7. 6 nd uw ff wwnkr — paper preparation and review there are many books, journal articles and tutorials which descr"}
{"id": "RM/book 1 (1).pdf#p133#c1", "source": "RM/book 1 (1).pdf", "page": 133, "snippet": "_ _ _ > ‘ yr engineers it is essential that all figures and equations follow the tules :. scr are referenced in the main text. i.. ‘ ae a captions whi"}
{"id": "RM/book 1 (1).pdf#p134#c1", "source": "RM/book 1 (1).pdf", "page": 134, "snippet": "248 research methods for engineers reviewers ’ comments and the associate editor ' s decision. usually all reviewers of the paper must agree that the "}
{"id": "RM/book 1 (1).pdf#p135#c1", "source": "RM/book 1 (1).pdf", "page": 135, "snippet": "250 research methods for engineers 1632 will be rejected. this is self - check on the quality of a paper about to be submitted for review. paper revie"}
{"id": "RM/book 1 (1).pdf#p136#c1", "source": "RM/book 1 (1).pdf", "page": 136, "snippet": "252 research methods for engineers research presentation an the structure of the presentation should follow figure 7. 1. there are some general presen"}
{"id": "RM/book 1 (1).pdf#p137#c1", "source": "RM/book 1 (1).pdf", "page": 137, "snippet": "254 7. 7. 2 research methods for engineers the number of words on each slide should be less than 50 and the text should be broken up into fewer than e"}
{"id": "RM/book 1 (1).pdf#p138#c1", "source": "RM/book 1 (1).pdf", "page": 138, "snippet": "256 research methods for engineers chair ’ s responsibilities ( a rough guide ) before the day of the session read the papers in your session before t"}
{"id": "RM/book 1 (1).pdf#p139#c1", "source": "RM/book 1 (1).pdf", "page": 139, "snippet": "258 research methods for engineers 7. 8 poster presentations poster sessions at science and engineering conferences can be of significant benefit in p"}
{"id": "RM/book 1 (1).pdf#p140#c1", "source": "RM/book 1 (1).pdf", "page": 140, "snippet": "260 research methods for engineers research presentation 261 3 give additional important information about the research more generally ( e. g. publish"}
{"id": "RM/book 1 (1).pdf#p141#c1", "source": "RM/book 1 (1).pdf", "page": 141, "snippet": "262 research methods for engineers 7. 10 chapter summary report writing, publication and presentations are the final require - ments in the research p"}
{"id": "RM/book 1 (1).pdf#p142#c1", "source": "RM/book 1 (1).pdf", "page": 142, "snippet": "264 research methods for engineers the revie bf f process takes some time between the submissi ion and i es kis lo < p reports. the reports must be a "}
{"id": "RM/book 1 (1).pdf#p143#c1", "source": "RM/book 1 (1).pdf", "page": 143, "snippet": "266 research methods for engineers references keywords : report writing, publication guidelines, instructions for authors, scientific presentations, p"}
{"id": "RM/book 1 (1).pdf#p144#c1", "source": "RM/book 1 (1).pdf", "page": 144, "snippet": "268 research methods for engineers a research project is about creativity. a research team can only exercise this creativity if they : know and unders"}
{"id": "RM/book 1 (1).pdf#p145#c1", "source": "RM/book 1 (1).pdf", "page": 145, "snippet": "270 research methods for engineers ( a ) a fee for access will apply to individual papers in most traditional journals for someone who is not a member"}
{"id": "RM/book 1 (1).pdf#p146#c1", "source": "RM/book 1 (1).pdf", "page": 146, "snippet": "272 research methods for engineers these i a — of questions can be very important to the research olen method of addressing these questions is to revi"}
{"id": "RM/book 1 (1).pdf#p147#c1", "source": "RM/book 1 (1).pdf", "page": 147, "snippet": "274 research methods for engineers the path forward 275 of the discussion, then both the comment and the response will be published together. in this "}
{"id": "RM/book 1 (1).pdf#p148#c1", "source": "RM/book 1 (1).pdf", "page": 148, "snippet": "276 research methods for engineers plagiarism and inappropriate data manipulation can be traced, through appropriate sign - off by the researcher and "}
{"id": "RM/book 1 (1).pdf#p149#c1", "source": "RM/book 1 (1).pdf", "page": 149, "snippet": "278 research methods for engineers 8. 3. 4 too expensive or too time consuming. for this reason, the training of research staff is a very appropriate "}
{"id": "RM/book 1 (1).pdf#p150#c1", "source": "RM/book 1 (1).pdf", "page": 150, "snippet": "280 research methods for engineers the path forward — experimentation outside of the laboratory also requires that the safety aspects are addressed. h"}
{"id": "RM/book 1 (1).pdf#p151#c1", "source": "RM/book 1 (1).pdf", "page": 151, "snippet": "282 research methods for engineers factor, each publisher will maintain a balance between open access ( resulting in more journal citations ) and cost"}
{"id": "RM/book 1 (1).pdf#p152#c1", "source": "RM/book 1 (1).pdf", "page": 152, "snippet": "284 research methods for engineers [ 2 ] wait, j. r., ‘ comments ’, ieee trans ge - 27 ( 1 ), 23 - 26, 1989. [ 3 ] wu, x. w. and thiel, d. v., ‘ reply"}
{"id": "RM/book 1 (1).pdf#p153#c1", "source": "RM/book 1 (1).pdf", "page": 153, "snippet": "286 appendix a : matlab plot functions where the numbers in the square brackets separated by a space constitute the data values. a straight line betwe"}
{"id": "RM/book 1 (1).pdf#p154#c1", "source": "RM/book 1 (1).pdf", "page": 154, "snippet": "index absolute error, 120 abstract, 36, 38, 40, 41, 44, 45, 49, 50, 52, 58, 60, 231, 232, 241, 242, 245, 246, 248, 252, 262, 263, 265 accreditation, 1"}
{"id": "RM/book 1 (1).pdf#p155#c1", "source": "RM/book 1 (1).pdf", "page": 155, "snippet": "290 index pareto front, 186, 190 partial correlation, 146, 147, 149, 154 past tense, use of, 61, 243 patent, 6, 10, 36, 48, 49, 50, 51, 59, 71, 82, 83"}
{"id": "RM/book 1 (1).pdf#p156#c1", "source": "RM/book 1 (1).pdf", "page": 156, "snippet": "learn how to plan for success with this hands - on guide to conducting high - quality engineering research. plan and implement your next project for m"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p1#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 1, "snippet": "~ newage _ research methodology methods and techniques ( second revised edition ) c. r. kothari ( ) new age international publishers"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p2#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 2, "snippet": "methods & techniques c. r. kothari former principal, college of commerce university of rajasthan, jaipur ( india ) ( fa publishing for one world new a"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p3#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 3, "snippet": "copyright © 2004, 1990, 1985, new age international ( p ) ltd., publishers published by new age international ( p ) ltd., publishers all rights reserv"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p4#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 4, "snippet": "in loving memory of my revered father ( the fountain of inspiration )"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p5#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 5, "snippet": "preface to the second editionvii preface to the second edition i feel encouraged by the widespread response from teachers and students alike to the fi"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p6#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 6, "snippet": "preface to the first edition ix preface to the first edition quite frequently these days people talk of research, both in academic institutions and ou"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p7#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 7, "snippet": "x preface to the first edition various multivariate techniques can appropriate be utilized in research studies, specially in behavioural and social sc"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p8#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 8, "snippet": "contents xi contents preface to the second edition vii preface to the first edition ix 1. research methodology : an introduction 1 meaning of research"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p9#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 9, "snippet": "xii research methodology features of a good design33 important concepts relating to research design33 different research designs35 basic principles of"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p10#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 10, "snippet": "contents xiii selection of appropriate method for data collection112 case study method 113 appendices ( i ) guidelines for constructing questionnaire "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p11#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 11, "snippet": "xiv research methodology 9. testing of hypotheses - i ( parametric or 184 standard tests of hypotheses ) what is a hypothesis? 184 basic concepts conc"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p12#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 12, "snippet": "contents xv anova in latin - square design271 analysis of co - variance ( anocov a ) 275 anocov a technique 275 assumptions in anocov a 276 12. testin"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p13#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 13, "snippet": "xvi research methodology 15. the computer : its role in research 361 introduction 361 the computer and computer technology361 the computer system 363 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p14#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 14, "snippet": "research methodology : an introduction 1 1 research methodology : an introduction meaning of research research in common parlance refers to a search f"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p15#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 15, "snippet": "2 research methodology consisting of enunciating the problem, formulating a hypothesis, collecting the facts or data, analysing the facts and reaching"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p16#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 16, "snippet": "research methodology : an introduction 3 the term ex post facto research for descriptive research studies. the main characteristic of this method is t"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p17#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 17, "snippet": "4 research methodology practice is relatively a difficult job and therefore, while doing such research, one should seek guidance from experimental psy"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p18#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 18, "snippet": "research methodology : an introduction 5 research approaches the above description of the types of research brings to light the fact that there are tw"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p19#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 19, "snippet": "6 research methodology decision - making may not be a part of research, but research certainly facilitates the decisions of the policy maker. governme"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p20#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 20, "snippet": "research methodology : an introduction 7 in addition to what has been stated above, the significance of research can also be understood keeping in vie"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p21#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 21, "snippet": "8 research methodology use in performing research operations. in other words, all those methods which are used by the researcher during the course of "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p22#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 22, "snippet": "research methodology : an introduction 9 research and scientific method for a clear perception of the term research, one should know the meaning of sc"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p23#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 23, "snippet": "10 research methodology thus, “ the scientific method encourages a rigorous, impersonal mode of procedure dictated by the demands of logic and objecti"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p24#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 24, "snippet": "research methodology : an introduction 11 fig. 1. 1 review concepts and theories review previous research finding formulate hypotheses design research"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p25#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 25, "snippet": "12 research methodology the chart indicates that the research process consists of a number of closely related activities, as shown through i to vii. b"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p26#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 26, "snippet": "research methodology : an introduction 13 the statement of the objective is of basic importance because it determines the data which are to be collect"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p27#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 27, "snippet": "14 research methodology hypotheses, specially in the case of exploratory or formulative researches which do not aim at testing the hypothesis. but as "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p28#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 28, "snippet": "research methodology : an introduction 15 city ’ s 200 drugstores in a certain way constitutes a sample design. samples can be either probability samp"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p29#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 29, "snippet": "16 research methodology ( iv ) stratified sampling : if the population from which a sample is to be drawn does not constitute a homogeneous group, the"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p30#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 30, "snippet": "research methodology : an introduction 17 should resort to random sampling so that bias can be eliminated and sampling error can be estimated. but pur"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p31#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 31, "snippet": "18 research methodology the researcher should select one of these methods of collecting the data taking into consideration the nature of investigation"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p32#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 32, "snippet": "research methodology : an introduction 19 come from different universes and if the difference is due to chance, the conclusion would be that the two s"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p33#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 33, "snippet": "20 research methodology 2. report should be written in a concise and objective style in simple language avoiding vague expressions such as ‘ it seems,"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p34#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 34, "snippet": "research methodology : an introduction 21 3. good research is empirical : it implies that research is related basically to one or more aspects of a re"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p35#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 35, "snippet": "22 research methodology 6. many researchers in our country also face the difficulty of adequate and timely secretarial assistance, including computeri"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p36#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 36, "snippet": "research methodology : an introduction 23 8. “ creative management, whether in public administration or private industry, depends on methods of inquir"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p37#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 37, "snippet": "24 research methodology 2 defining the research problem in research process, the first and foremost step happens to be that of selecting and properly "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p38#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 38, "snippet": "defining the research problem 25 over and above these conditions, the individual or the organisation can be said to have the problem only if ‘ i ’ doe"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p39#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 39, "snippet": "26 research methodology ( iii ) too narrow or too vague problems should be avoided. ( iv ) the subject selected for research should be familiar and fe"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p40#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 40, "snippet": "defining the research problem 27 solution. it is only on careful detailing the research problem that we can work out the research design and can smoot"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p41#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 41, "snippet": "28 research methodology understanding of the nature of the problem involved, he can enter into discussion with those who have a good knowledge of the "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p42#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 42, "snippet": "defining the research problem 29 ( a ) technical terms and words or phrases, with special meanings used in the statement of the problem, should be cle"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p43#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 43, "snippet": "30 research methodology one in terms of the available data and resources and is also analytically meaningful. all this results in a well defined resea"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p44#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 44, "snippet": "research design 31 3 research design meaning of research design the formidable problem that follows the task of defining the research problem is the p"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p45#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 45, "snippet": "32 research methodology ( b ) the observational design which relates to the conditions under which the observations are to be made ; ( c ) the statist"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p46#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 46, "snippet": "research design 33 features of a good design a good design is often characterised by adjectives like flexible, appropriate, efficient, economical and "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p47#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 47, "snippet": "34 research methodology or absence of the concerning attribute ( s ). phenomena which can take on quantitatively different values even in decimal poin"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p48#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 48, "snippet": "research design 35 of students and for this purpose he randomly selects 50 students and tests their intelligence and reading ability by calculating th"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p49#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 49, "snippet": "36 research methodology point of view. the major emphasis in such studies is on the discovery of ideas and insights. as such the research design appro"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p50#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 50, "snippet": "research design 37 now, what sort of examples are to be selected and studied? there is no clear cut answer to it. experience indicates that for partic"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p51#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 51, "snippet": "38 research methodology bias and unreliability must be ensured. whichever method is selected, questions must be well examined and be made unambiguous "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p52#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 52, "snippet": "research design 39 the difference between research designs in respect of the above two types of research studies can be conveniently summarised in tab"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p53#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 53, "snippet": "40 research methodology according to the principle of replication, the experiment should be repeated more than once. thus, each treatment is applied i"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p54#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 54, "snippet": "research design 41 important experimental designs experimental design refers to the framework or structure of an experiment and as such there are seve"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p55#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 55, "snippet": "42 research methodology fig. 3. 2 the basic assumption in such a design is that the two areas are identical with respect to their behaviour towards th"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p56#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 56, "snippet": "research design 43 extraneous factors are included under the heading of chance variation, we refer to the design of experiment as c. r. design. we can"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p57#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 57, "snippet": "44 research methodology fig. 3. 5 : random replication design ( in diagram form ) ( ii ) random replications design : the limitation of the two - grou"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p58#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 58, "snippet": "research design 45 from the diagram it is clear that there are two populations in the replication design. the sample is taken randomly from the popula"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p59#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 59, "snippet": "46 research methodology 6. latin square design ( l. s. design ) is an experimental design very frequently used in agricultural research. the condition"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p60#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 60, "snippet": "research design 47 equal. this reduces the utility of this design. in case of ( 2 × 2 ) l. s. design, there are no degrees of freedom available for th"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p61#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 61, "snippet": "48 research methodology the main effects of levels. an additional merit of this design is that one can examine the interaction between treatments and "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p62#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 62, "snippet": "research design 49 the graph relating to study i indicates that there is an interaction between the treatment and the level which, in other words, mea"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p63#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 63, "snippet": "50 research methodology such a design the means for the columns provide the researcher with an estimate of the main effects for treatments and the mea"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p64#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 64, "snippet": "research design 51 the dotted line cell in the diagram corresponds to cell 1 of the above stated 2 × 2 × 2 design and is for treatment a, level i of t"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p65#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 65, "snippet": "52 research methodology factorial designs are used mainly because of the two advantages. ( i ) they provide equivalent accuracy ( as happens in the ca"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p66#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 66, "snippet": "appendix : developing a research plan 53 appendix developing a research plan * after identifying and defining the problem as also accomplishing the re"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p67#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 67, "snippet": "54 research methodology 6. a clear mention of the population to be studied should be made. if the study happens to be sample based, the research plan "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p68#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 68, "snippet": "sampling design 55 4 sampling design census and sample survey all items in any field of inquiry constitute a ‘ universe ’ or ‘ population. ’ a complet"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p69#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 69, "snippet": "56 research methodology design may as well lay down the number of items to be included in the sample i. e., the size of the sample. sample design is d"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p70#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 70, "snippet": "sampling design 57 would like to make estimates. all this has a strong impact upon the sample design we would accept. ( vi ) budgetary constraint : co"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p71#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 71, "snippet": "58 research methodology sampling errors are the random variations in the sample estimates around the true population parameters. since they occur rand"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p72#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 72, "snippet": "sampling design 59 fig. 4. 1 non - probability sampling : non - probability sampling is that sampling procedure which does not afford any basis for es"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p73#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 73, "snippet": "60 research methodology probability sampling : probability sampling is also known as ‘ random sampling ’ or ‘ chance sampling ’. under this sampling d"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p74#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 74, "snippet": "sampling design 61 successive drawings each of the remaining elements of the population has the same chance of being selected. this procedure will als"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p75#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 75, "snippet": "62 research methodology the probability of getting a particular number, say 1, is the same for each throw and the 20 throws are all independent, then "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p76#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 76, "snippet": "sampling design 63 the following three questions are highly relevant in the context of stratified sampling : ( a ) how to form strata? ( b ) how shoul"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p77#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 77, "snippet": "64 research methodology nn nn n n kk k11 1 2 2 2 / /......... / σσ σ = = = where σσ 12,,... and σ k denote the standard deviations of the k strata, n "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p78#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 78, "snippet": "sampling design 65 where c 1 = cost of sampling in stratum 1 c 2 = cost of sampling in stratum 2 c k = cost of sampling in stratum k and all other ter"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p79#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 79, "snippet": "66 research methodology sampling unit such as states in a country. then we may select certain districts and interview all banks in the chosen district"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p80#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 80, "snippet": "sampling design 67 table 4. 1 city number no. of departmental stores cumulative total sample 13 53 5 1 0 21 75 2 31 06 2 6 0 43 29 4 5 70 164 110 160 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p81#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 81, "snippet": "68 research methodology questions 1. what do you mean by ‘ sample design ’? what points should be taken into consideration by a researcher in developi"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p82#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 82, "snippet": "measurement and scaling techniques 69 5 measurement and scaling techniques measurement in research in our daily life we are said to measure when we us"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p83#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 83, "snippet": "70 research methodology the person is single, married, widowed or divorced. we can as well record “ yes or no ” answers to a question as “ 0 ” and “ 1"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p84#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 84, "snippet": "measurement and scaling techniques 71 measurement scales from what has been stated above, we can write that scales of measurement can be considered in"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p85#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 85, "snippet": "72 research methodology accepts the assumptions on which the rule is based. interval scales can have an arbitrary zero, but it is not possible to dete"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p86#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 86, "snippet": "measurement and scaling techniques 73 ( a ) respondent : at times the respondent may be reluctant to express strong negative feelings or it is just po"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p87#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 87, "snippet": "74 research methodology research problem and the judgement of the researcher. but one can certainly consider three types of validity in this connectio"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p88#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 88, "snippet": "measurement and scaling techniques 75 two aspects of reliability viz., stability and equivalence deserve special mention. the stability aspect is conc"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p89#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 89, "snippet": "76 research methodology development is more apparent in theoretical studies than in the more pragmatic research, where the fundamental concepts are of"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p90#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 90, "snippet": "measurement and scaling techniques 77 point and the third point indicates a higher degree as compared to the fourth and so on. numbers for measuring t"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p91#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 91, "snippet": "78 research methodology ( f ) scale construction techniques : following are the five main techniques by which scales can be developed. ( i ) arbitrary"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p92#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 92, "snippet": "measurement and scaling techniques 79 fig. 5. 1 this type of scale has several limitations. the respondents may check at almost any position along the"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p93#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 93, "snippet": "80 research methodology ranking scales : under ranking scales ( or comparative scales ) we make relative judgements against other similar objects. the"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p94#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 94, "snippet": "measurement and scaling techniques 81 rank order 2 3 1 4 m p 0. 5375 0. 4625 0. 5450 0. 4550 z j 0. 09 ( – ). 09 0. 11 ( – ). 11 r j 0. 20 0. 02 0. 22"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p95#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 95, "snippet": "82 research methodology ( b ) method of rank order : under this method of comparative scaling, the respondents are asked to rank their choices. this m"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p96#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 96, "snippet": "measurement and scaling techniques 83 table 5. 2 : different scales for measuring attitudes of people name of the scale construction approach name of "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p97#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 97, "snippet": "84 research methodology ( d ) for items that are retained, each is given its median scale value between one and eleven as established by the panel. in"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p98#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 98, "snippet": "measurement and scaling techniques 85 we find that these five points constitute the scale. at one extreme of the scale there is strong agreement with "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p99#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 99, "snippet": "86 research methodology way we determine which statements consistently correlate with low favourability and which with high favourability. ( vi ) only"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p100#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 100, "snippet": "measurement and scaling techniques 87 change or improvement in which case we can use the scales to measure attitudes before and after the programme of"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p101#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 101, "snippet": "88 research methodology procedure : the procedure for developing a scalogram can be outlined as under : ( a ) the universe of content must be defined "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p102#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 102, "snippet": "measurement and scaling techniques 89 the table shows that five items ( numbering 5, 12, 3, 10 and 7 ) have been selected for the final scale. the num"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p103#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 103, "snippet": "90 research methodology appropriately with the universe of content which is multi - dimensional and how to uncover underlying ( latent ) dimensions wh"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p104#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 104, "snippet": "measurement and scaling techniques 91 ( b ) the next step is to select the scales bearing in mind the criterion of factor composition and the criterio"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p105#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 105, "snippet": "92 research methodology minimises the dimensionality of the solution space. this approach utilises all the information in the data in obtaining a solu"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p106#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 106, "snippet": "measurement and scaling techniques 93 questions 1. what is the meaning of measurement in research? what difference does it make whether we measure in "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p107#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 107, "snippet": "94 research methodology ( c ) likert - type scale ; ( d ) arbitrary scales ; ( e ) multidimensional scaling ( mds ). 9. describe the different methods"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p108#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 108, "snippet": "methods of data collection 95 6 methods of data collection the task of data collection begins after a research problem has been defined and research d"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p109#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 109, "snippet": "96 research methodology that there are several methods of collecting primary data, particularly in surveys and descriptive researches. important ones "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p110#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 110, "snippet": "methods of data collection 97 there are several merits of the participant type of observation : ( i ) the researcher is enabled to record the natural "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p111#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 111, "snippet": "98 research methodology the interviewer in a structured interview follows a rigid procedure laid down, asking questions in a form and order prescribed"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p112#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 112, "snippet": "methods of data collection 99 ( viii ) the interviewer may catch the informant off - guard and thus may secure the most spontaneous reactions than wou"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p113#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 113, "snippet": "100 research methodology ( b ) telephone interviews : this method of collecting information consists in contacting respondents on telephone itself. it"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p114#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 114, "snippet": "methods of data collection 101 2. it is free from the bias of the interviewer ; answers are in respondents ’ own words. 3. respondents have adequate t"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p115#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 115, "snippet": "102 research methodology structured questionnaires are simple to administer and relatively inexpensive to analyse. the provision of alternative replie"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p116#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 116, "snippet": "methods of data collection 103 instance, instead of asking. “ how many razor blades do you use annually? ” the more realistic question would be to ask"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p117#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 117, "snippet": "104 research methodology first in terms of financial expenditure and later in terms of weight. the control questions, thus, introduce a cross - check "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p118#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 118, "snippet": "methods of data collection 105 is generally filled out by the research worker or the enumerator, who can interpret questions when necessary. 2. to col"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p119#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 119, "snippet": "106 research methodology some other methods of data collection let us consider some other methods of data collection, particularly used by big busines"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p120#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 120, "snippet": "methods of data collection 107 among others. most of these panels operate by mail. the representativeness of the panel relative to the population and "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p121#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 121, "snippet": "108 research methodology brand names possessing one or more of these. this technique is quick and easy to use, but yields reliable results when applie"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p122#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 122, "snippet": "methods of data collection 109 holtzman inkblot test or h. i. t. has several special features or advantages. for example, it elicits relatively consta"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p123#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 123, "snippet": "110 research methodology individuals by asking them to indicate whom they would choose or reject in various situations. thus, sociometry is a new tech"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p124#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 124, "snippet": "methods of data collection 111 collection of secondary data secondary data means data that are already available i. e., they refer to the data which h"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p125#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 125, "snippet": "112 research methodology spend time and energy in field surveys for collecting information. at times, there may be wealth of usable information in the"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p126#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 126, "snippet": "methods of data collection 113 using direct questions, may yield satisfactory results even in case of attitude surveys. since projective techniques ar"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p127#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 127, "snippet": "114 research methodology 3. in the context of this method we make complete study of the social unit covering all facets. through this method we try to"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p128#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 128, "snippet": "methods of data collection 115 ( v ) follow - up programme to determine effectiveness of the treatment applied. advantages : there are several advanta"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p129#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 129, "snippet": "116 research methodology ( xiii ) case study techniques are indispensable for therapeutic and administrative purposes. they are also of immense value "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p130#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 130, "snippet": "methods of data collection 117 2. “ it is never safe to take published statistics at their face value without knowing their meaning and limitations. ”"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p131#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 131, "snippet": "118 research methodology appendix ( i ) guidelines for constructing questionnaire / schedule the researcher must pay attention to the following points"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p132#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 132, "snippet": "appendix ( ii ) : guidelines for successful interviewing119 appendix ( ii ) guidelines for successful interviewing interviewing is an art and one lear"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p133#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 133, "snippet": "120 research methodology appendix ( iii ) difference between survey and experiment the following points are noteworthy so far as difference between su"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p134#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 134, "snippet": "appendix ( iii ) : difference between survey and experiment 121 ( vi ) surveys are concerned with hypothesis formulation and testing the analysis of t"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p135#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 135, "snippet": "122 research methodology 7 processing and analysis of data the data, after collection, has to be processed and analysed in accordance with the outline"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p136#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 136, "snippet": "processing and analysis of data 123 at the time of recording the respondents ’ responses. this type of editing is necessary in view of the fact that i"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p137#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 137, "snippet": "124 research methodology way the entire data get divided into a number of groups or classes. classification can be one of the following two types, dep"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p138#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 138, "snippet": "processing and analysis of data 125 researcher ’ s objective judgement plays an important part in this connection. multiples of 2, 5 and 10 are genera"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p139#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 139, "snippet": "126 research methodology inclusive type class intervals : they are usually stated as follows : 11 – 20 21 – 30 31 – 40 41 – 50 in inclusive type class"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p140#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 140, "snippet": "processing and analysis of data 127 which can sort out cards at a speed of something like 25000 cards per hour. this method is fast but expensive. 4. "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p141#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 141, "snippet": "128 research methodology information about several interrelated characteristics of data. two - way tables, three - way tables or manifold tables are a"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p142#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 142, "snippet": "processing and analysis of data 129 18. the arrangement of the categories in a table may be chronological, geographical, alphabetical or according to "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p143#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 143, "snippet": "130 research methodology elements / types of analysis as stated earlier, by analysis we mean the computation of certain indices or measures along with"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p144#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 144, "snippet": "processing and analysis of data 131 inferential analysis is concerned with the various tests of significance for testing hypotheses in order to determ"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p145#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 145, "snippet": "132 research methodology measures of central tendency measures of central tendency ( or statistical averages ) tell us the point about which items hav"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p146#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 146, "snippet": "processing and analysis of data 133 median value of n + 1 2 th itemm bg = fhg ikj median is a positional average and is used only in the context of qu"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p147#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 147, "snippet": "134 research methodology where h. m. = harmonic mean rec. = reciprocal x i = ith value of the variable x n = number of items for instance, the harmoni"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p148#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 148, "snippet": "processing and analysis of data 135 mean deviation from mean δ x i xx n ch =, if deviations, xx i −, are obtained from or arithmetic average. mean dev"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p149#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 149, "snippet": "136 research methodology or standard deviationσ bg di = fx x f ii i 2, in case of frequency distribution where f i means the frequency of the ith item"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p150#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 150, "snippet": "processing and analysis of data 137 fig. 7. 2 skewness is, thus, a measure of asymmetry and shows the manner in which the items are clustered around t"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p151#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 151, "snippet": "138 research methodology measures of relationship so far we have dealt with those statistical measures that we use in context of univariate population"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p152#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 152, "snippet": "processing and analysis of data 139 powerful form of statistical correlation and accordingly we use some other methods when data happen to be either o"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p153#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 153, "snippet": "140 research methodology where x i = ith value of x variable x = mean of x y i = ith value of y variable y = mean of y n = number of pairs of observat"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p154#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 154, "snippet": "processing and analysis of data 141 karl pearson ’ s coefficient of correlation is also known as the product moment correlation coefficient. the value"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p155#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 155, "snippet": "142 research methodology thus, the regression analysis is a statistical method to deal with the formulation of mathematical model depicting relationsh"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p156#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 156, "snippet": "processing and analysis of data 143 with more than one independent variable, we may make a difference between the collective effect of the two indepen"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p157#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 157, "snippet": "144 research methodology alternatively, we can work out the partial correlation coefficients thus : r rr r rr yx x yx yx x x yx x x 12 12 1 2 21 2 11 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p158#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 158, "snippet": "processing and analysis of data 145 where ( ab ) = frequency of class ab and a n b n n bg bg ×× = expectation of ab, if a and b are independent, and n"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p159#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 159, "snippet": "146 research methodology some attribute, say c with which attributes a and b are associated ( but in reality there is no association between a and b )"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p160#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 160, "snippet": "processing and analysis of data 147 table 7. 3 attribute aa total attribute b ( ab ) ( ab ) ( b ) b ( ab ) ( ab ) ( b ) total ( a ) ( a ) n after redu"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p161#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 161, "snippet": "148 research methodology ‘ economic barometers measuring the economic phenomenon in all its aspects either directly by measuring the same phenomenon o"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p162#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 162, "snippet": "processing and analysis of data 149 additive model considers the total of various components resulting in the given values of the overall time series "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p163#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 163, "snippet": "150 research methodology ( iii ) coefficient of contingency ; ( iv ) multicollinearity ; ( v ) partial association between two attributes. 11. “ the a"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p164#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 164, "snippet": "appendix : developing a research plan 151 appendix ( summary chart concerning analysis of data ) analysis of data ( in a broad general way can be cate"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p165#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 165, "snippet": "152 research methodology 8 sampling fundamentals sampling may be defined as the selection of some part of an aggregate or totality on the basis of whi"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p166#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 166, "snippet": "sampling fundamentals 153 1. universe / population : from a statistical point of view, the term ‘ universe ’ refers to the total of the items or units"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p167#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 167, "snippet": "154 research methodology those errors which arise on account of sampling and they generally happen to be random variations ( in case of random samplin"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p168#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 168, "snippet": "sampling fundamentals 155 should not deviate from the actual value by more than rs 200 in either direction, in that case the range would be rs 3800 to"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p169#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 169, "snippet": "156 research methodology normal variate z x n p = −µ σ for the sampling distribution of mean. this characteristic of the sampling distribution of mean"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p170#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 170, "snippet": "sampling fundamentals 157 certain level of significance is compared with the calculated value of t from the sample data, and if the latter is either e"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p171#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 171, "snippet": "158 research methodology sample plays a critical role. when n is small, the shape of the distribution will depend largely on the shape of the parent p"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p172#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 172, "snippet": "sampling fundamentals 159 ( i ) statistical estimation : sampling theory helps in estimating unknown population parameters from a knowledge of statist"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p173#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 173, "snippet": "160 research methodology the tests of significance used for dealing with problems relating to large samples are different from those used for small sa"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p174#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 174, "snippet": "sampling fundamentals 161 ( ii ) to test the difference between the means of two samples t xx xx = − − 12 12 σ where x 1 = mean of sample one x 2 = me"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p175#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 175, "snippet": "162 research methodology sandlers a - test joseph sandler has developed an alternate approach based on a simplification of t - test. his approach is d"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p176#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 176, "snippet": "sampling fundamentals 163 ( ii ) square each d i and then obtain the sum of such squares i. e., σ d i 2. ( iii ) find a - statistic as under : ad d ii"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p177#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 177, "snippet": "164 research methodology table 8. 1 : criteria for judging significance at various important levels significance confidence critical sampling confiden"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p178#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 178, "snippet": "sampling fundamentals 165 ( ii ) standard error of proportion of successes pq n ⋅ bg ( iii ) standard error of the difference between proportions of t"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p179#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 179, "snippet": "166 research methodology ( iii ) standard error of standard deviation when population standard deviation is known : σ σ σ s p n = 2 ( iv ) standard er"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p180#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 180, "snippet": "sampling fundamentals 167 σ σσ s ss nn n x x n x x nn 12 12 1 2 2 2 11 1 2 2 22 1 2 2 12 ⋅ = + + − + − + ⋅⋅ di di d i d i where x nx nx nn 12 11 2 2 1"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p181#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 181, "snippet": "168 research methodology researcher usually makes these two types of estimates through sampling analysis. while making estimates of population paramet"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p182#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 182, "snippet": "sampling fundamentals 169 mean is critical for drawing inferences about parameters. the relationship between the dispersion of a population distributi"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p183#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 183, "snippet": "170 research methodology n = 36 x = 40 years σ s = 45. years and the standard variate, z, for 95 per cent confidence is 1. 96 ( as per the normal curv"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p184#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 184, "snippet": "sampling fundamentals 171 = × − − 08 64 2400 64 2400 1. = × 08 64 2336 2399. = ( 0. 1 ) (. 97 ) =. 097 ( 3 ) 90 per cent confidence interval for the m"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p185#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 185, "snippet": "172 research methodology thus, 90 per cent confidence interval for population mean is xt n s ± σ = ±36 8 2 353 28 4... = ±36 8 2 353 14... bg b g = ±3"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p186#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 186, "snippet": "sampling fundamentals 173 we now illustrate the use of this formula by an example. illustration 4 a market research survey in which 64 consumers were "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p187#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 187, "snippet": "174 research methodology in case of infinite in case of finite population * population and use σ s as the best estimate of σ p and sample is large ( i"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p188#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 188, "snippet": "sampling fundamentals 175 ( v ) standard of accuracy and acceptable confidence level : if the standard of acuracy or the level of precision is to be k"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p189#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 189, "snippet": "176 research methodology σ p = standard deviation of the popultion ( to be estimated from past experience or on the basis of a trial sample ). suppose"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p190#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 190, "snippet": "sampling fundamentals 177 where n = size of population n = size of sample e = acceptable error ( the precision ) σ p = standard deviation of populatio"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p191#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 191, "snippet": "178 research methodology n z e p = 22 2 σ = = = − 25 7 2 08 26 4196 06 4 4128 41 22 2..... ~ bg b g bg thus, in the given case the sample size remains"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p192#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 192, "snippet": "sampling fundamentals 179 since $ p is actually what we are trying to estimate, then what value we should assign to it? one method may be to take the "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p193#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 193, "snippet": "180 research methodology = − − + − 2 005 02 1 02 4000 02 4000 1 2 005 02 1 02 2 22....... bg b g b g b g bg b gb g bg b g = + = = 3151699 15996 0788 3"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p194#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 194, "snippet": "sampling fundamentals 181 ( i ) find the expected value of the sample information ( evsi ) * for every possible n ; ( ii ) also workout reasonably app"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p195#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 195, "snippet": "182 research methodology 11. from a packet containing iron nails, 1000 iron nails were taken at random and out of them 100 were found defective. estim"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p196#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 196, "snippet": "sampling fundamentals 183 25. a team of medico research experts feels confident that a new drug they have developed will cure about 80 % of the patien"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p197#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 197, "snippet": "184 research methodology 9 testing of hypotheses i ( parametric or standard tests of hypotheses ) hypothesis is usually considered as the principal in"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p198#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 198, "snippet": "testing of hypotheses i 185 characteristics of hypothesis : hypothesis must possess the following characteristics : ( i ) hypothesis should be clear a"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p199#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 199, "snippet": "186 research methodology if our sample results do not support this null hypothesis, we should conclude that something else is true. what we conclude r"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p200#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 200, "snippet": "testing of hypotheses i 187 when the sampling result ( i. e., observed evidence ) has a less than 0. 05 probability of occurring if h 0 is true. in ot"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p201#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 201, "snippet": "188 research methodology in such a situation one should prefer a type i error to a type ii error. as a result one must set very high level for type i "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p202#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 202, "snippet": "testing of hypotheses i 189 mathematically we can state : acceptance region az : < 196. rejection region rz : > 196. if the significance level is 5 pe"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p203#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 203, "snippet": "190 research methodology if our µ = 100 and if our sample mean deviates significantly from100 in the lower direction, we shall reject h 0, otherwise w"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p204#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 204, "snippet": "testing of hypotheses i 191 procedure for hypothesis testing to test a hypothesis means to tell ( on the basis of the data the researcher has collecte"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p205#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 205, "snippet": "192 research methodology ( vi ) comparing the probability : yet another step consists in comparing the probability thus calculated with the specified "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p206#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 206, "snippet": "testing of hypotheses i 193 measuring the power of a hypothesis test as stated above we may commit type i and type ii errors while testing a hypothesi"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p207#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 207, "snippet": "194 research methodology solution : as we want to test the hypothesis that the average quantity of waste per batch of 60 lbs. is 15 or less pounds aga"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p208#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 208, "snippet": "testing of hypotheses i 195 we can find out the probability of the area that lies between 15. 64 and 16 in the above curve first by finding z and then"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p209#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 209, "snippet": "196 research methodology in some cases the population may not be normally distributed, yet the tests will be applicable on account of the fact that we"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p210#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 210, "snippet": "testing of hypotheses i 197 hypothesis testing of means mean of the population can be tested presuming different situations such as the population may"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p211#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 211, "snippet": "198 research methodology table 9. 3 : names of some parametric tests along with test situations and test statistics used in context of hypothesis test"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p212#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 212, "snippet": "testing of hypotheses i 199 12 3 4 5 or z x x n n p p 1 2 1 2 1 2 2 2 − + σ σ is used when two samples are drawn from different populations. in case σ"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p213#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 213, "snippet": "200 research methodology 12 3 4 5 σ s i x x n = − − σ d i 2 1 pairs in two samples. alternatively, t can be worked out as under : x x n n n n n n n n "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p214#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 214, "snippet": "testing of hypotheses i 201 1 2 3 4 5 and q p 0 0 1 = − in which case we calculate test statistic z p p p q n n = − + f h g i k j $ $ 1 2 0 0 1 2 1 1 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p215#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 215, "snippet": "202 research methodology and σ s i xx n = − dibg 2 1 5. population may not be normal but sample size is large, variance of the population may be known"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p216#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 216, "snippet": "testing of hypotheses i 203 to have been taken from a population with mean height 67. 39 \" and standard deviation 1. 30 \" at 5 % level of significance"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p217#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 217, "snippet": "204 research methodology h ah : µ 0 50 < ( since the manager wants to safeguard against decreasing values of mean. ) and the given information as x p "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p218#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 218, "snippet": "testing of hypotheses i 205 s. no. x i xx i − di xx i − di 2 4 568 – 4 16 5 572 0 0 6 578 6 36 7 570 – 2 4 8 572 0 0 9 596 24 576 10 544 – 28 784 n = "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p219#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 219, "snippet": "206 research methodology s. no. x i hypothesised mean dx ii h = − µ 0 di d i 2 m5 7 8 k g. h 0 = 5 572 578 – 6 36 6 578 578 0 0 7 570 578 – 8 64 8 572"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p220#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 220, "snippet": "testing of hypotheses i 207 table 9. 4 s. no. x i xx i − di xx i − di 2 1 550 2 4 2 570 22 484 3 490 – 58 3364 4 615 67 4489 5 505 – 43 1849 6 580 32 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p221#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 221, "snippet": "208 research methodology hypothesis testing for differences between means. the null hypothesis for testing of difference between means is generally st"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p222#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 222, "snippet": "testing of hypotheses i 209 x nx nx nn 12 11 2 2 12. = + + 3. samples happen to be small samples and population variances not known but assumed to be "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p223#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 223, "snippet": "210 research methodology = − = −20 142 14 08.. as h a is two - sided, we shall apply a two - tailed test for determining the rejection regions at 5 pe"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p224#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 224, "snippet": "testing of hypotheses i 211 ( since the population variances are not known, we have used the sample variances, considering the sample variances as the"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p225#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 225, "snippet": "212 research methodology = − + + − × + = −57 61 45 3 64 8 572 1 5 1 7 3 053... bg bg degrees of freedom = ( n 1 + n 2 – 2 ) = 5 + 7 – 2 = 10 as h a is"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p226#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 226, "snippet": "testing of hypotheses i 213 table 9. 7 sample one sample two s. no. x 1i x 1i – a 1 xa i1 2 1 − bg s. no. x 2i x 2i – a 2 xa i2 2 2 − bg ( a 1 = 10 ) "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p227#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 227, "snippet": "214 research methodology = × = = 3 4 6 345 3 126 2 381.... degrees of freedom = ( n 1 + n 2 – 2 ) = 10 as h a is one - sided, we shall apply a one - t"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p228#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 228, "snippet": "testing of hypotheses i 215 σ diff. = standard deviation of differences n = number of matched pairs this calculated value of t is compared with its ta"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p229#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 229, "snippet": "216 research methodology mean of differences or d d n i = = − = −7 9 0 778. and standard deviation of differences or σ diff i dd n n. = ⋅ − 2 2 1 di ="}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p230#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 230, "snippet": "testing of hypotheses i 217 solution : let the sales before campaign be represented as x and the sales after campaign as y and then taking the null hy"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p231#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 231, "snippet": "218 research methodology solution : using a - test : using a - test, we work out the test statistic for the given problem as under : a d d i i = = − ="}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p232#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 232, "snippet": "testing of hypotheses i 219 hp p h0 0 1 2 : = = hpp ah : = 0 hence the probability of boy birth or p = 1 2 and the probability of girl birth is also 1"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p233#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 233, "snippet": "220 research methodology hence, p = 0. 20 and q = 0. 80 observed sample proportion $ p bg = 70 / 400 = 0. 175 and the test statistic z pp pq n = − ⋅ ="}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p234#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 234, "snippet": "testing of hypotheses i 221 of success in sample two $ p 2 bg is due to fluctuations of random sampling. in other words, we take the null hypothesis a"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p235#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 235, "snippet": "222 research methodology solution : we take the null hypothesis that there is no difference between the two drugs i. e., hp p 01 2 : $ $ = the alterna"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p236#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 236, "snippet": "testing of hypotheses i 223 thus, q 0 = 1 – p 0 =. 2727 the test statistic z can be worked out as under : z pp pq n pq n = − + = − + $ $.... 12 00 1 0"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p237#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 237, "snippet": "224 research methodology solution : let hp p 0 : $ = ( there is no difference between sample proportion and population proportion ) and hpp a : $ = ( "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p238#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 238, "snippet": "testing of hypotheses i 225 the values happen to be positive ; one must simply know the degrees of freedom for using such a distribution. * testing th"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p239#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 239, "snippet": "226 research methodology context of analysis of variance. the following examples illustrate the use of f - test for testing the equality of variances "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p240#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 240, "snippet": "testing of hypotheses i 227 and σ s i xx n 2 2 22 2 2 1 314 12 1 28 55 = − = − = di. hence, f s s ss = > σ σ σσ 2 1 21 2 2 22 q ej = = 28 55 1333 21 4"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p241#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 241, "snippet": "228 research methodology the table value of f at 5 per cent level for v 1 = 8 and v 2 = 7 is 3. 73. since the calculated value of f is greater than th"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p242#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 242, "snippet": "testing of hypotheses i 229 limitations of the tests of hypotheses we have described above some important test often used for testing hypotheses on th"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p243#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 243, "snippet": "230 research methodology 4. what do you mean by the power of a hypothesis test? how can it be measured? describe and illustrate by an example. 5. brie"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p244#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 244, "snippet": "testing of hypotheses i 231 20. ten young recruits were put through a strenuous physical training programme by the army. their weights ( in kg ) were "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p245#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 245, "snippet": "232 research methodology 28. ( i ) a random sample from 200 villages was taken from kanpur district and the average population per village was found t"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p246#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 246, "snippet": "chi - square test 233 10 chi - square test the chi - square test is an important test amongst the several tests of significance developed by statistic"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p247#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 247, "snippet": "234 research methodology the χ 2 - distribution is not symmetrical and all the values are positive. for making use of this distribution, one is requir"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p248#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 248, "snippet": "chi - square test 235 s. no. 123456789 1 0 weight ( kg. ) 38 40 45 53 47 43 55 48 52 49 can we say that the variance of the distribution of weight of "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p249#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 249, "snippet": "236 research methodology = −3111 20 10 1. bg = 13. 999. degrees of freedom in the given case is ( n – 1 ) = ( 10 – 1 ) = 9. at 5 per cent level of sig"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p250#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 250, "snippet": "chi - square test 237 as a test of goodness of fit, χ 2 test enables us to see how well does the assumed theoretical distribution ( such as binomial d"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p251#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 251, "snippet": "238 research methodology ascertained by looking at the tabulated values of χ 2 for given degrees of freedom at a certain level of significance. if the"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p252#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 252, "snippet": "chi - square test 239 ( i ) first of all calculate the expected frequencies on the basis of given hypothesis or on the basis of null hypothesis. usual"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p253#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 253, "snippet": "240 research methodology [ ( o i – e i ) 2 / e i ] = 9. hence, the calculated value of χ 2 = 9. q degrees of freedom in the given problem is ( n – 1 )"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p254#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 254, "snippet": "chi - square test 241 the expected frequencies of type a, ab and b ( as per the genetic theory ) should have been 75, 150 and 75 respectively. we now "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p255#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 255, "snippet": "242 research methodology ( a ) = 500 ( b ) = 216 n = 2000 expectation of ( ) = ab 500 216 2000 54× = now using the expectation of ( ab ), we can write"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p256#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 256, "snippet": "chi - square test 243 show that the sampling technique of at least one research worker is defective. solution : let us take the hypothesis that the sa"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p257#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 257, "snippet": "244 research methodology hence, χ 2 2 = −oe e ij ij ij di = 55. 54 q degrees of freedom = ( c – 1 ) ( r – 1 ) = ( 3 – 1 ) ( 2 – 1 ) = 2. the table val"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p258#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 258, "snippet": "chi - square test 245 events or expected frequencies no. of heads 3 8 3 35 1 2 1 2 256 56c fhg ikj fhg ikj × = 4 8 4 44 1 2 1 2 256 70c fhg ikj fhg ik"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p259#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 259, "snippet": "246 research methodology degrees of freedom = ( n – 1 ) = ( 9 – 1 ) = 8 the table value of χ 2 for eight degrees of freedom at 5 per cent level of sig"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p260#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 260, "snippet": "chi - square test 247 χ 2 2 05 ( corrected ) = ⋅− − + + + + na d b c n ab cd ac bd. ch bg bg bg bg in case we use the usual formula for calculating th"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p261#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 261, "snippet": "248 research methodology shops in towns shops in villages t otal run by men 14 ( ab ) 21 ( ab ) 3 5 run by women 6 ( ab ) 9 ( ab ) 1 5 total 20 30 50 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p262#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 262, "snippet": "chi - square test 249 from a number of samples of similar data, then because of the additive nature of χ 2 we can combine the various values of χ 2 by"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p263#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 263, "snippet": "250 research methodology conversion of chi - square into coefficient of contingency ( c ) chi - square value may also be converted into coefficient of"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p264#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 264, "snippet": "chi - square test 251 questions 1. what is chi - square text? explain its significance in statistical analysis. 2. write short notes on the following "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p265#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 265, "snippet": "252 research methodology 7. find chi - square from the following information : condition of home condition of child total clean dirty clean 70 50 120 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p266#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 266, "snippet": "chi - square test 253 no. of boys 5 4 3 2 1 0 no. of girls 0 1 2 3 4 5 no. of families 14 56 110 88 40 12 is this distribution consistent with the hyp"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p267#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 267, "snippet": "254 research methodology area total ab c strikes 7 10 8 25 dry holes 10 18 9 37 total number of test wells 17 28 17 62 do the three areas have the sam"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p268#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 268, "snippet": "chi - square test 255 23. for the data in question 12, find the coefficient of contingency to measure the magnitude of relationship between a and b. 2"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p269#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 269, "snippet": "256 research methodology 11 analysis of variance and co - variance analysis of variance ( anova ) analysis of variance ( abbreviated as anov a ) is an"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p270#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 270, "snippet": "analysis of variance and co - variance 257 later on professor snedecor and many others contributed to the development of this technique. anova is esse"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p271#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 271, "snippet": "258 research methodology this value of f is to be compared to the f - limit for given degrees of freedom. if the f value we work out is equal or excee"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p272#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 272, "snippet": "analysis of variance and co - variance 259 ms ss nkwithin = within ( – ) where ( n – k ) represents degrees of freedom within samples, n = total numbe"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p273#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 273, "snippet": "260 research methodology table 11. 1 : analysis of variance table for one - way anova ( there are k samples having in all n items ) source of sum of s"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p274#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 274, "snippet": "analysis of variance and co - variance 261 ( iii ) find out the square of all the item values one by one and then take its total. subtract the correct"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p275#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 275, "snippet": "262 research methodology per acre production data plot of land variety of wheat ab c 16 5 5 27 5 4 33 3 3 48 7 4 solution : we can solve the problem b"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p276#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 276, "snippet": "analysis of variance and co - variance 263 ss x x ij for total variance = fh ik 2 i = 1, 2, 3 … j = 1, 2, 3 … = ( 6 – 5 ) 2 + ( 7 – 5 ) 2 + ( 3 – 5 ) "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p277#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 277, "snippet": "264 research methodology = ( 6 ) 2 + ( 7 ) 2 + ( 3 ) 2 + ( 8 ) 2 + ( 5 ) 2 + ( 5 ) 2 + ( 3 ) 2 + ( 7 ) 2 + ( 5 ) 2 + ( 4 ) 2 + ( 3 ) 2 + ( 4 ) 2 – 60 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p278#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 278, "snippet": "analysis of variance and co - variance 265 the various steps involved are as follows : ( i ) use the coding device, if the same simplifies the task. ("}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p279#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 279, "snippet": "266 research methodology table 11. 3 : analysis of variance table for two - way anova source of sum of squares degrees of mean square f - ratio variat"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p280#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 280, "snippet": "analysis of variance and co - variance 267 solution : as the given problem is a two - way design of experiment without repeated values, we shall adopt"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p281#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 281, "snippet": "268 research methodology table 11. 5 : the anova table source of variation ss d. f. ms f - ratio 5 % f - limit ( or the tables values ) between column"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p282#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 282, "snippet": "analysis of variance and co - variance 269 table 11. 6 : computations for two - way anova ( in design with repeated values ) step ( i ) t = 187, n = 1"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p283#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 283, "snippet": "270 research methodology source of variation ss d. f. ms f - ratio 5 % f - limit interaction 29. 23 * 4 * 29 23 4. 73 0 8 03 8 9.. f ( 4, 9 ) = 3. 63 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p284#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 284, "snippet": "analysis of variance and co - variance 271 the graph indicates that there is a significant interaction because the different connecting lines for grou"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p285#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 285, "snippet": "272 research methodology where total ss x t n ij = − di bg 2 2 c = number of columns r = number of rows v = number of varieties illustration 4 analyse"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p286#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 286, "snippet": "analysis of variance and co - variance 273 fig. 11. 2 ( b ) correction factor = t n bg b g b g 2 12 12 16 9 = −− = ss x t n ij for total variance = = "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p287#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 287, "snippet": "274 research methodology for finding ss for variance between varieties, we would first rearrange the coded data in the following form : table 11. 9 va"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p288#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 288, "snippet": "analysis of variance and co - variance 275 source of ss d. f. ms f - ratio 5 % f - limit variation between varieties 48. 50 3 48 50 3 1617.. = 1617 17"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p289#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 289, "snippet": "276 research methodology assumptions in anocova the anocov a technique requires one to assume that there is some sort of relationship between the depe"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p290#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 290, "snippet": "analysis of variance and co - variance 277 correction factor for = x x n = bg 2 7616 27. = + + = y 33 72 105 210 correction factor for = y y n = bg 2 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p291#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 291, "snippet": "278 research methodology anova table for x, y and xy can now be set up as shown below : anova table for x, y and xy source d. f. ss for x ss for y sum"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p292#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 292, "snippet": "analysis of variance and co - variance 279 = = 198 274 40 0 7216.. deviation of initial group means from final means of groups in x ( unadjusted ) gen"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p293#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 293, "snippet": "280 research methodology output on plots abc d 893 3 12 4 8 7 172 8 315 2 find out whether the difference in the means of the production of crops of t"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p294#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 294, "snippet": "analysis of variance and co - variance 281 fertilizer treatment varieties of wheat total w 1 w 2 w 3 f 1 55 72 47 174 f 2 64 66 53 183 f 3 58 57 74 18"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p295#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 295, "snippet": "282 research methodology brands of gasoline wx y z a 13 12 12 11 11 10 11 13 cars b 12 10 11 9 13 11 12 10 c 14 11 13 10 13 10 14 8 13. the following "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p296#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 296, "snippet": "testing of hypotheses - ii 283 12 testing of hypotheses - ii ( nonparametric or distribution - free tests ) it has already been stated in earlier chap"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p297#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 297, "snippet": "284 research methodology important nonparametric or distribution - free tests tests of hypotheses with ‘ order statistics ’ or ‘ nonparametric statist"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p298#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 298, "snippet": "testing of hypotheses - ii 285 illustration 1 suppose playing four rounds of golf at the city club 11 professionals totalled 280, 282, 290, 273, 283, "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p299#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 299, "snippet": "286 research methodology fig. 12. 1 as the observed proportion of success is only 1 / 10 or 0. 1 which comes in the rejection region, we reject the nu"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p300#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 300, "snippet": "testing of hypotheses - ii 287 table 12. 1 by x 1 0 2 3 1 0 2 2 3 0 1 1 4 1 2 1 3 5 2 1 3 2 4 1 3 2 0 2 4 2 by y 0 0 1 0 2 0 0 1 1 2 0 1 2 1 1 0 2 2 6"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p301#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 301, "snippet": "288 research methodology by using the table of area under normal curve, we find the appropriate z value for 0. 49 of the area under normal curve and i"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p302#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 302, "snippet": "testing of hypotheses - ii 289 fact, equally good, ( alternatively the probability that the particular result or worse for b group would occur ) be wo"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p303#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 303, "snippet": "290 research methodology and after the treatment by setting up a table in the following form in respect of the first and second set of responses : tab"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p304#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 304, "snippet": "testing of hypotheses - ii 291 the test statistic, utilising the mcnemer test, can be worked out as under : χ 2 22 1 200 100 1 200 100 = −− + = −− + a"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p305#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 305, "snippet": "292 research methodology z tu t t = − σ we may now explain the use of this test by an example. illustration 4 an experiment is conducted to judge the "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p306#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 306, "snippet": "testing of hypotheses - ii 293 pair brand a brand b difference rank of rank with signs d i | d i | + – 3 47 43 4 4. 5 4. 5 … 4 5 3 4 1 1 2 1 11 1 … 5 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p307#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 307, "snippet": "294 research methodology values of the first sample ( and call it r 1 ) and also the sum of the ranks assigned to the values of the second sample ( an"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p308#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 308, "snippet": "testing of hypotheses - ii 295 table 12. 5 size of sample item in rank name of related sample : ascending order [ a for sample one and b for sample tw"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p309#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 309, "snippet": "296 research methodology µ u nn = 12 2 12 12 2 72× = = bg bg σ u nn n n = + + = + + 12 1 2 1 12 12 12 12 12 1 12 b g bg bg b g = 17. 32 as the alterna"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p310#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 310, "snippet": "testing of hypotheses - ii 297 illustration 6 two samples with values 90, 94, 36 and 44 in one case and the other with values 53, 39, 6, 24, and 33 ar"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p311#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 311, "snippet": "298 research methodology probability of 0. 05, given the null hypothesis and the significance level. if the calculated probability happens to be great"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p312#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 312, "snippet": "testing of hypotheses - ii 299 solution : to apply the h test or the kruskal - wallis test to this problem, we begin by ranking all the given figures "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p313#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 313, "snippet": "300 research methodology h nn r n n i ii k = + − + = 12 1 31 2 1 bg bg = + + + + rs | t | uv | w | − + 12 20 20 1 52 5 37 5 75 5 46 5 32 0 1 2222 bg b"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p314#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 314, "snippet": "testing of hypotheses - ii 301 r = number of runs. in the given case the values of n 1, n 2 and r would be as follows : n 1 = 20 ; n 2 = 10 ; r = 7 th"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p315#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 315, "snippet": "302 research methodology by using the table of area under normal curve, we find the appropriate z value for 0. 495 of the area under the curve and it "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p316#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 316, "snippet": "testing of hypotheses - ii 303 where n = number of paired observations. the value of spearman ’ s rank correlation coefficient will always vary betwee"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p317#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 317, "snippet": "304 research methodology sample data concerning 35 applicants serial number interview score aptitude test score 1 81 113 28 8 8 8 35 5 7 6 4 83 129 57"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p318#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 318, "snippet": "testing of hypotheses - ii 305 table 12. 8 : calculation of spearmans s. no. interview aptitude rank rank rank differences squared score x test score "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p319#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 319, "snippet": "306 research methodology spearman ’ s ‘ ’ = 1 – r d nn i 6 1 1 6 3583 35 35 1 2 22 − rs | t | uv | w | = − × − rs | t | uv | w | ej e j = − = 1 21498 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p320#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 320, "snippet": "testing of hypotheses - ii 307 we now find the observed r = 0. 498 and as such it comes in the rejection region and, therefore, we reject the null hyp"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p321#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 321, "snippet": "308 research methodology w s kn n = −1 12 23 ej where sr r jj = − di 2 ; k = no. of sets of rankings i. e., the number of judges ; n = number of objec"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p322#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 322, "snippet": "testing of hypotheses - ii 309 ( ii ) if n is larger than 7, we may use χ 2 value to be worked out as : χ 2 = k ( n – 1 ). w with d. f. = ( n – 1 ) fo"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p323#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 323, "snippet": "310 research methodology q r r n j j = = = 112 7 16 q s = 332 w s kn n = − = − = = = 1 12 332 1 12 47 7 332 16 12 336 332 448 0 741 23 2 3 ej bg ej bg"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p324#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 324, "snippet": "testing of hypotheses - ii 311 all possible pairs. the relationship between the average of spearman ’ s r ’ s and kendall ’ s w can be put in the foll"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p325#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 325, "snippet": "312 research methodology table 12. 10 : difference between ranks | d i | assigned by k = 4 judges and the square values of such differences ( d i 2 ) "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p326#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 326, "snippet": "testing of hypotheses - ii 313 conclusion there are many situations in which the various assumptions required for standard tests of significance ( suc"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p327#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 327, "snippet": "314 research methodology on the basis of the above test results, determine whether the contractor should place order for bricks with concern a or with"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p328#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 328, "snippet": "multivariate analysis techniques 315 13 multivariate analysis techniques all statistical techniques which simultaneously analyse more than two variabl"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p329#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 329, "snippet": "316 research methodology characteristics and applications multivariate techniques are largely empirical and deal with the reality ; they possess the a"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p330#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 330, "snippet": "multivariate analysis techniques 317 the data are quantitative, collected on interval or ratio scale, or whether the data are qualitative, collected o"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p331#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 331, "snippet": "318 research methodology variables in multivariate analysis before we describe the various multivariate techniques, it seems appropriate to have a cle"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p332#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 332, "snippet": "multivariate analysis techniques 319 where z ' y stands for the predicted value of the standardized y score, z y. the expression on the right side of "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p333#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 333, "snippet": "320 research methodology this numerical value of z can then be transformed into the probability that the individual is an early user, a late user or a"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p334#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 334, "snippet": "multivariate analysis techniques 321 u 2 = the mean vector for group ii v = the common variance matrix by transformation procedure, this d 2 statistic"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p335#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 335, "snippet": "322 research methodology area and want to infer from these some factor ( such as social class ) which summarises the commonality of all the said four "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p336#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 336, "snippet": "multivariate analysis techniques 323 important methods of factor analysis there are several methods of factor analysis, but they do not necessarily gi"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p337#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 337, "snippet": "324 research methodology ( vii ) factor scores : factor score represents the degree to which each respondent gets high scores on the group of items th"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p338#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 338, "snippet": "multivariate analysis techniques 325 correlation, r, and the result is the first matrix of residual coefficients, r 1. * after obtaining r 1, one must"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p339#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 339, "snippet": "326 research methodology solution : given correlation matrix, r, is a positive manifold and as such the weights for all variables be + 1. 0. according"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p340#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 340, "snippet": "multivariate analysis techniques 327 first matrix of factor cross product ( q 1 ) first centroid. 693. 618. 642. 641. 629. 694. 679. 683 factor a. 693"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p341#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 341, "snippet": "328 research methodology variables 12 3 * 4 * 56 * 7 * 8 8. 301. 230. 366. 327. 294. 354. 312. 534 column sums : 2. 580 2. 642 2. 470 2. 757 2. 558 2."}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p342#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 342, "snippet": "multivariate analysis techniques 329 v ariables factor loadings communality ( h 2 ) centroid factor centroid factor ab eigen value ( variance accounte"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p343#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 343, "snippet": "330 research methodology total variance is related to these two factors, i. e., approximately 77 % of the total variance is common variance whereas re"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p344#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 344, "snippet": "multivariate analysis techniques 331 correlation matrix, r v ariables x 1 x 2 x 3 …. x k x 1 r 11 r 12 r 13 …. r 1k x 2 r 21 r 22 r 23 …. r 3k variabl"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p345#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 345, "snippet": "332 research methodology other steps involved in factor analysis ( a ) next the question is : how many principal components to retain in a particular "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p346#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 346, "snippet": "multivariate analysis techniques 333 1 234 56 7 8 7. 155. 083. 582. 613. 201. 801 1. 000. 152 8. 774. 652. 072. 111. 724. 120. 152 1. 000 column 3. 66"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p347#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 347, "snippet": "334 research methodology for finding principal component ii, we have to proceed on similar lines ( as stated in the context of obtaining centroid fact"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p348#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 348, "snippet": "multivariate analysis techniques 335 ( c ) maximum likelihood ( ml ) method of factor analysis the ml method consists in obtaining sets of factor load"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p349#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 349, "snippet": "336 research methodology there are several methods of rotating the initial factor matrix ( obtained by any of the methods of factor analysis ) to atta"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p350#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 350, "snippet": "multivariate analysis techniques 337 ( i ) factor analysis, like all multivariate techniques, involves laborious computations involving heavy cost bur"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p351#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 351, "snippet": "338 research methodology from the above description we find that clustering methods in general are judgemental and are devoid of statistical inference"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p352#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 352, "snippet": "multivariate analysis techniques 339 path analysis the term ‘ path analysis ’ was first introduced by the biologist sewall wright in 1934 in connectio"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p353#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 353, "snippet": "340 research methodology we may illustrate the path analysis technique in connection with a simple problem of testing a causal model with three explic"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p354#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 354, "snippet": "multivariate analysis techniques 341 in hypothesis testing and interval estimation studies. multivariate analysis ( consequently the use of multivaria"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p355#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 355, "snippet": "342 research methodology 8. compute the proportion of total variance explained by the two factors worked out in question six above by the principal co"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p356#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 356, "snippet": "appendix : summary chart 343 appendix summary chart : showing the appropriateness of a particular multivariate technique techniques of number of multi"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p357#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 357, "snippet": "344 research methodology 14 interpretation and report writing after collecting and analyzing the data, the researcher has to accomplish the task of dr"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p358#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 358, "snippet": "interpretation and report writing 345 ( i ) it is through interpretation that the researcher can well understand the abstract principle that works ben"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p359#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 359, "snippet": "346 research methodology of interpretation be accomplished with patience in an impartial manner and also in correct perspective. researcher must pay a"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p360#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 360, "snippet": "interpretation and report writing 347 writing research report. there are people who do not consider writing of report as an integral part of the resea"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p361#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 361, "snippet": "348 research methodology in some way pertinent to the research which has been done. it should contain all those works which the researcher has consult"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p362#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 362, "snippet": "interpretation and report writing 349 form an opinion of how seriously the findings are to be taken. for this purpose there is the need of proper layo"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p363#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 363, "snippet": "350 research methodology ( iii ) results : a detailed presentation of the findings of the study, with supporting data in the form of tables and charts"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p364#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 364, "snippet": "interpretation and report writing 351 types of reports research reports vary greatly in length and type. in each individual case, both the length and "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p365#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 365, "snippet": "352 research methodology 4. data : discussion of data collected, their sources, characteristics and limitations. if secondary data are used, their sui"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p366#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 366, "snippet": "interpretation and report writing 353 there can be several variations of the form in which a popular report can be prepared. the only important thing "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p367#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 367, "snippet": "354 research methodology types of reports have been described in this chapter earlier which should be taken as a guide for report - writing in case of"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p368#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 368, "snippet": "interpretation and report writing 355 2. title of work, underlined to indicate italics ; 3. place and date of publication ; 4. number of volume ; 5. p"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p369#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 369, "snippet": "356 research methodology the page number. a single page should be referred to as p., but more than one page be referred to as pp. if there are several"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p370#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 370, "snippet": "interpretation and report writing 357 et seq., et sequens : and the following ex., example f., ff., and the following fig ( s )., figure ( s ) fn., fo"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p371#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 371, "snippet": "358 research methodology a thousand words. statistics are usually presented in the form of tables, charts, bars and line - graphs and pictograms. such"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p372#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 372, "snippet": "interpretation and report writing 359 graphs and the statistical tables may be used for the various results in the main report in addition to the summ"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p373#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 373, "snippet": "360 research methodology 5. “ it is only through interpretation the researcher can expose the relations and processes that underlie his findings ”. ex"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p374#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 374, "snippet": "the computer : its role in research 361 15 the computer : its role in research introduction problem solving is an age old activity. the development of"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p375#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 375, "snippet": "362 research methodology in brief, computer is a machine capable of receiving, storing, manipulating and yielding information such as numbers, words, "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p376#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 376, "snippet": "the computer : its role in research 363 are being replaced by devices such as bubble memories and optical video discs. in brief, computer technology h"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p377#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 377, "snippet": "364 research methodology processing by the arithmetic logical unit, which conveys the result of the calculations and comparisons back to the internal "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p378#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 378, "snippet": "the computer : its role in research 365 forgets them. hence, it is impossible to store all types of information inside the computer records. if need b"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p379#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 379, "snippet": "366 research methodology illustration 1 find the binary equivalents of 26 and 45. solution : table for conversion of 26 into its binary equivalent : n"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p380#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 380, "snippet": "the computer : its role in research 367 begin the conversion process by doubling the leftmost bit of the given number and add to it the bit at its rig"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p381#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 381, "snippet": "368 research methodology solution : carry 111 carry 11 10111000 184 + 111011 + 59 11110011 243 in illustration 4, we find a new situation ( 1 + 1 + 1 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p382#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 382, "snippet": "the computer : its role in research 369 solution : decimal binary according to complementary method number number 14 0001110 0001110 subtract 72 10010"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p383#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 383, "snippet": "370 research methodology illustration 8 convert 3. 375 into its equivalent binary number. solution : this can be done in two stages. first ( 3 ) 10 = "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p384#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 384, "snippet": "the computer : its role in research 371 applications in some of the various uses 2. commerce ( i ) assist the production of text material ( known as w"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p385#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 385, "snippet": "372 research methodology researchers in economics and other social sciences have found, by now, electronic computers to constitute an indispensable pa"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p386#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 386, "snippet": "the computer : its role in research 373 call for a maximum of 80 columns per line in such forms ) at the appropriate space meant for each variable. on"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p387#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 387, "snippet": "374 research methodology 5. “ the advancement in computers is astonishing ”. do you agree? answer pointing out the various characteristics of computer"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p388#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 388, "snippet": "appendix 375 appendix ( selected statistical tables )"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p389#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 389, "snippet": "376 research methodology table 1 : area under normal curve an entry in the table is the proportion under the entire curve which is between z = 0 and a"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p390#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 390, "snippet": "appendix 377 table 2 : critical values of students t - distribution level of significance for two - tailed test d. f. 0. 20 0. 10 0. 05 0. 02 0. 01 d."}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p391#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 391, "snippet": "378 research methodology table 3 : critical values of χ 2 degrees probability under h 0 that of χ 2 > chi square of freedom. 99. 95. 50. 10. 05. 02. 0"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p392#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 392, "snippet": "appendix 379 table 4 ( a ) : critical values of f - distribution ( at 5 per cent ) v 1 1234568 1 22 4 ∞ v 2 1 161. 4 199. 5 215. 7 224. 6 230. 2 234. "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p393#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 393, "snippet": "380 research methodology table 4 ( b ) : critical values of f - distribution ( at 1 per cent ) v 1 1234568 1 22 4 ∞ v 2 1 4052 4999. 5 5403 5625 5764 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p394#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 394, "snippet": "appendix 381 table 5 : values for spearmans rank correlation ( r s ) for combined areas in both tails n. 20. 10. 05. 02. 01. 002 4. 8000. 8000 — — — —"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p395#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 395, "snippet": "382 research methodology table 6 : selected values of wilcoxons ( unpaired ) distribution [ w s min w s ] or [ max. w l w l ] sl m i n m a x 0 1234 5 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p396#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 396, "snippet": "appendix 383 sl m i n m a x 0 1234 5 67 8 9 1 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 2 0 w s w l 6 3 21 24. 012. 024 * 4 21 34. 005. 010. 019. 033. 057"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p397#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 397, "snippet": "384 research methodology table 7 : critical values of t in the wilcoxon matched pairs test level of significance for one - tailed test. 025. 01. 005 l"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p398#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 398, "snippet": "appendix 385 table 8 : cumulative binomial probabilities : p ( r < r | n, p ) nr 0. 10. 25. 40. 50 1 0. 9000. 7500. 6000. 5000 1 1. 0000 1. 0000 1. 00"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p399#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 399, "snippet": "386 research methodology nr 0. 10. 25. 40. 50 20 0. 1216. 0032. 0000. 0000 1. 3917. 0243. 0005. 0000 2. 6768. 0912. 0036. 0002 3. 8669. 2251. 0159. 00"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p400#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 400, "snippet": "appendix 387 table 9 : selected critical values of s in the kendalls coefficient of concordance v alues at 5 % level of significance kn some additiona"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p401#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 401, "snippet": "388 research methodology table 10 : table showing critical values of a - statistic for any given value of n 1, corresponding to various levels of prob"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p402#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 402, "snippet": "appendix 389 1 23 456 40 0. 368 0. 263 0. 191 0. 158 0. 102 60 0. 369 0. 262 0. 189 0. 155 0. 099 120 0. 369 0. 261 0. 187 0. 153 0. 095 ∞ 0. 370 0. 2"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p403#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 403, "snippet": "390 research methodology selected references and recommended readings 1. ackoff, russell l., the design of social research, chicago : university of ch"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p404#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 404, "snippet": "selected references and recommended readings 391 19. burgess, ernest w., “ research methods in sociology ” in georges gurvitch and w. e. moore ( ed. )"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p405#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 405, "snippet": "392 research methodology 49. giles, g. b., marketing, 2nd ed., london : macdonald & evans ltd., 1974. 50. glock, charles y., survey research in the so"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p406#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 406, "snippet": "selected references and recommended readings 393 79. levine, s. and elzey, freeman f., a programmed introduction to research, california : wods worth "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p407#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 407, "snippet": "394 research methodology 109. siegel, s., nonparametric statistics for the behavioral sciences, new york : mcgraw - hill publishing co., inc., 1956. 1"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p408#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 408, "snippet": "author index 395 author index ackoff, r. l., 25, 390 allen, t. harrell, 390 anderson, g. l., 390 anderson, h. h., 390 anderson, john f., 390 anderson,"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p409#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 409, "snippet": "396 research methodology festinger, leon, 391 fiebleman, j. k., 391 fisher, r. a., 39, 61, 256, 391 fox, james herold, 20, 391 freedman, p., 391 fruch"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p410#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 410, "snippet": "author index 397 murphy, james l., 158, 195, 214, 257, 392 nagel, stuart s., 393 neiswanger, w. a., 12 newell, william t., 5, 393 nie, n. h., 393 noet"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p411#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 411, "snippet": "398 research methodology subject index after - only with control design, 41 alpha error, 187 analysis of covariance ( anocov a ), 275 – 79 analysis of"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p412#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 412, "snippet": "subject index 399 content analysis, 110 content validity, 74 contingency table, 146 continuous variable, 34, 318 control, 34 control groups, 35 conven"}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p413#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 413, "snippet": "400 research methodology median, 132 – 33 mode, 133 – 34 measures of dispersion, 134 – 37 mean deviation, 134 – 35 range, 134 standard deviation, 135 "}
{"id": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf#p414#c1", "source": "RM/Research methodology methods and techniques by C.R.Kothari (1).pdf", "page": 414, "snippet": "subject index 401 research plan, 53 – 54 research problem, 24 – 29 research process, 10 – 20 roarhach test, 108 rosenzweig test, 108 r - type factor a"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p1#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 1, "snippet": "reviewing the literature in research methodology chatgpt said : chatgpt reviewing the literature in research methodology involves a systematic explora"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p2#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 2, "snippet": "• key debates : identify ongoing discussions and controversies surrounding specific methods. • gaps in the literature : highlight areas where certain "}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p3#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 3, "snippet": "o example : an article detailing the application of a mixed - methods approach in a specific field like education or psychology. • review articles : t"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p4#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 4, "snippet": "5. technical reports • methodological guidelines : these are detailed reports, often produced by research organizations, government agencies, or acade"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p5#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 5, "snippet": "10. data sets with methodological notes • public data sets with methodological documentation : researchers sometimes publish their datasets along with"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p6#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 6, "snippet": "o example : a new type of smartphone or a novel drug formulation. 2. design patents : these protect the ornamental design or appearance of a product. "}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p7#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 7, "snippet": "• licensing : patent holders can license their rights to others, allowing them to use the invention in exchange for royalties or other forms of compen"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p8#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 8, "snippet": "• informed consent : researchers must obtain voluntary, informed consent from participants, ensuring they understand the study ’ s purpose, risks, and"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p9#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 9, "snippet": "reporting standards ensure that research findings are presented clearly, transparently, and accurately, allowing others to evaluate and build on the w"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p10#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 10, "snippet": "• systematic review standards : researchers conducting systematic reviews must follow strict guidelines, such as predefining inclusion criteria, condu"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p11#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 11, "snippet": "• clinical research standards : clinical trials must follow strict protocols as regulated by agencies such as the u. s. food and drug administration ("}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p12#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 12, "snippet": "• research question : identify the main research question or problem the authors are addressing. what gap in existing methodology are they attempting "}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p13#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 13, "snippet": "in the discussion section, the authors interpret their findings and place their methodology in the broader context of the field. • significance of the"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p14#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 14, "snippet": "10. compare with other articles reading multiple articles on similar methods can deepen your understanding. compare how different researchers apply th"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p15#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 15, "snippet": "o are there inconsistencies in the results of different studies? o what limitations do previous studies mention? o which areas have been well - studie"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p16#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 16, "snippet": "research hasn ’ t covered. reading recent papers, attending conferences, or engaging with news in your field can help identify these emerging areas. •"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p17#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 17, "snippet": "• example : in a clinical trial on a new drug, researchers may recommend follow - up studies on long - term effects or studies involving different pop"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p18#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 18, "snippet": "identifying research gaps is a critical step in advancing scientific knowledge and improving the relevance of research. it requires a combination of t"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p19#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 19, "snippet": "once you have a broad topic, narrow it down to a more specific issue or problem. this involves focusing on a particular aspect of the broad topic that"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p20#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 20, "snippet": "o \" while several studies have examined the impact of technology on student learning, there is limited research on the use of virtual reality in high "}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p21#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 21, "snippet": "• examples of research objectives : o to assess the impact of virtual reality on student engagement in urban high schools. o to compare the academic p"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p22#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 22, "snippet": "2. how does the academic performance of students using vr compare to those using traditional teaching methods? 3. what factors influence the effective"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p23#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 23, "snippet": "4. based on existing knowledge : it should be grounded in existing theories or previous research, even if it suggests something new. 5. directional or"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p24#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 24, "snippet": "the development of a hypothesis and its subsequent testing are integral parts of the scientific research process. this process allows researchers to m"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p25#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 25, "snippet": "identify the variables involved in your hypothesis. the independent variable is what you manipulate or change, and the dependent variable is what you "}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p26#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 26, "snippet": "o if p ≤ α ( e. g., p ≤ 0. 05 ), reject the null hypothesis ( h₀ ). this suggests that the alternative hypothesis ( h₁ ) is supported by the data. o i"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p27#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 27, "snippet": "• data collection : conduct an experiment with two groups — students who use the app daily and a control group who do not. • statistical test : use a "}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p28#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 28, "snippet": "o often achieved using random number generators or drawing lots. o example : selecting students randomly from a list of all students in a school. 2. s"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p29#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 29, "snippet": "o example : choosing experts in a field to participate in a study on emerging technologies. 3. snowball sampling : o existing study participants recru"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p30#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 30, "snippet": "• allows for generalization to the population. • reduces bias and increases the representativeness of the sample. • enables statistical analysis and i"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p31#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 31, "snippet": "3. benchmarking : the process of comparing one ’ s processes and performance metrics to industry bests or best practices from other companies. this ca"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p32#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 32, "snippet": "16. qualitative research : research that focuses on understanding the meaning and characteristics of human behavior and experiences. it often involves"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p33#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 33, "snippet": "4o mini you said : shodhganga chatgpt said : chatgpt shodhganga is an initiative by the indian institute of science ( iisc ) and inflibnet ( informati"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p34#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 34, "snippet": "• downloading documents : full - text documents can typically be downloaded in pdf format, facilitating easy access to research work. • contributing t"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p35#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 35, "snippet": "• photographs and prints : millions of photographs, prints, and illustrations, documenting the visual history of the u. s. and beyond. • audio and vid"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p36#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 36, "snippet": "specific questions about the library of congress or need information on a particular topic, feel free to ask! 4o mini you said : research gate chatgpt"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p37#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 37, "snippet": "o users can view their \" researchgate score, \" which reflects their research impact based on the engagement with their publications. 6. job listings :"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p38#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 38, "snippet": "chatgpt said : chatgpt google scholar is a free web search engine developed by google that indexes scholarly articles across various disciplines, incl"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p39#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 39, "snippet": "• free access : google scholar is free to use, making it accessible to a broad audience, including students and researchers without institutional acce"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p40#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 40, "snippet": "1. citations : citations are references to other works within a publication. bibliometrics analyzes citation patterns to evaluate the influence and im"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p41#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 41, "snippet": "2. journal ranking : impact factors and citation metrics help rank academic journals, guiding researchers in choosing where to publish their work. 3. "}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p42#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 42, "snippet": "chatgpt citations are references to published or unpublished works that acknowledge the sources of information, ideas, or research findings used in a "}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p43#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 43, "snippet": "5. harvard style : o a widely used author - date citation style, particularly in the uk. in - text citations include the author ' s last name and year"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p44#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 44, "snippet": "the impact factor ( if ) is a quantitative measure used to evaluate the relative importance or influence of a scholarly journal within its field. it r"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p45#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 45, "snippet": "3. time lag : the impact factor is calculated based on citations from the past two years, which may not fully reflect the long - term impact or signif"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p46#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 46, "snippet": "here ’ s an overview of how the h - index works, its significance, advantages, limitations, and how it is calculated. calculation of the h - index the"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p47#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 47, "snippet": "• resistance to manipulation : the h - index is less susceptible to manipulation than other metrics, as it requires both quantity and quality of citat"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p48#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 48, "snippet": "the i - index is calculated by taking into account the number of citations a researcher receives, emphasizing the most highly cited papers while also "}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p49#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 49, "snippet": "the i - index is a bibliometric measure that seeks to provide a more balanced and nuanced view of research impact compared to the h - index. while it "}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p50#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 50, "snippet": "2. poor time management : researchers may resort to plagiarism when they are pressed for time, leading them to cut corners in their writing and resear"}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p51#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 51, "snippet": "4. using quotation marks : when directly quoting a source, always use quotation marks and provide an appropriate citation. 5. encouraging originality "}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p52#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 52, "snippet": "community to maintain ethical standards and avoid legal repercussions. here ’ s an overview of the key concepts, causes, consequences, and strategies "}
{"id": "RM/RM Unit 2 Chatgpt.pdf#p53#c1", "source": "RM/RM Unit 2 Chatgpt.pdf", "page": 53, "snippet": "2. academic penalties : institutions may impose disciplinary actions on researchers found to be infringing copyrights, which can include retraction of"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p1#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 1, "snippet": "you said : use of statistics in research chatgpt said : chatgpt statistics plays a crucial role in research across various disciplines. it involves co"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p2#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 2, "snippet": "5. analyzing relationships • correlation analysis : measures the strength and direction of relationships between variables. • regression analysis : mo"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p3#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 3, "snippet": "chatgpt one - dimensional statistics : combining errors and uncertainties in scientific research and data analysis, one - dimensional statistics often"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p4#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 4, "snippet": "if z = xnz = xn, where nn is a constant : • uncertainty in zz : δzz = = 4. combining systematic and random errors to calculate the total uncertainty :"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p5#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 5, "snippet": "a t - test is a statistical test used to determine whether there is a significant difference between the means of two groups. it is commonly used in h"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p6#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 6, "snippet": "t = + s22n2t = n1s12 + •,, : sample means of groups 1 and 2. • s12, s22s12, s22 : variances of groups 1 and 2. • n1, n2n1, n2 : sample sizes of groups"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p7#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 7, "snippet": "example : independent t - test scenario : testing if two diets result in different weight loss. • group 1 : mean = 5. 2 kg, s1 = 1. 1s1 = 1. 1, n1 = 1"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p8#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 8, "snippet": "1. one - way anova • compares the means of three or more independent groups based on a single factor. • example : testing whether three teaching metho"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p9#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 9, "snippet": "• : overall mean. 3. f - ratio the f - statistic is the ratio of between - group variance to within - group variance : f = msbmswf = mswmsb where : • "}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p10#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 10, "snippet": "f = msbmsw = 94. 112. 11 = 44. 6f = mswmsb = 2. 1194. 11 = 44. 6 5. decision : • compare ff ( 44. 6 ) with the critical value from the f - distributio"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p11#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 11, "snippet": "where : • cov ( x, y ) cov ( x, y ) : covariance between xx and yy. • σx, σyσx, σy : standard deviations of xx and yy. • regression analysis : examine"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p12#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 12, "snippet": "• rxy. z > 0rxy. z > 0 : positive relationship between xx and yy after controlling for zz. • rxy. z < 0rxy. z < 0 : negative relationship between xx a"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p13#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 13, "snippet": "methods in statistics sed in research methodology chatgpt said : chatgpt methods in statistics used in research methodology statistical methods are fu"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p14#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 14, "snippet": "• common tests include : • t - tests : comparing means of two groups ( independent or paired ). • z - test : comparing means when the population varia"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p15#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 15, "snippet": "• used when data do not meet parametric test assumptions ( e. g., normality ). • mann - whitney u test • kruskal - wallis test • wilcoxon signed - ran"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p16#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 16, "snippet": "5. statistical software researchers often use statistical software to perform analyses efficiently : • spss : for social science research. • r and pyt"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p17#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 17, "snippet": "a probability distribution describes how the probabilities are distributed over the values of a random variable. it provides the likelihood of differe"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p18#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 18, "snippet": "6. hypergeometric distribution • models the number of successes in nn draws without replacement from a finite population. • pmf : p ( x = k ) = ( kk )"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p19#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 19, "snippet": "relationship between distributions 1. central limit theorem : • the sampling distribution of the sample mean approaches a normal distribution as the s"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p20#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 20, "snippet": "statistical analysis and inference are the core components of research methodology, allowing researchers to extract insights from data and make conclu"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p21#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 21, "snippet": "• random sampling. • stratified sampling. • systematic sampling. • cluster sampling. c. hypothesis testing • a process to test assumptions about a pop"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p22#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 22, "snippet": "3. regression analysis • models the relationship between independent and dependent variables. • types : • simple linear regression. • multiple regress"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p23#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 23, "snippet": "• modeling pollution levels. challenges in statistical analysis and inference • sampling bias : poor sampling methods lead to inaccurate inferences. •"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p24#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 24, "snippet": "• principal component analysis ( pca ) • reduces the dimensionality of data by identifying principal components ( linear combinations of variables ) t"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p25#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 25, "snippet": "• models the relationship between a binary dependent variable and multiple independent variables. • applications : disease diagnosis, customer churn p"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p26#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 26, "snippet": "• dynamic factor models • extracts common trends and patterns from multivariate time series data. • applications : climate studies, finance. 5. advanc"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p27#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 27, "snippet": "5. interpret results : • evaluate patterns, relationships, or predictions. • validate results using cross - validation or other techniques. 6. visuali"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p28#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 28, "snippet": "multivariate methods are essential in modern research and decision - making, enabling the analysis of complex datasets to uncover deeper insights and "}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p29#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 29, "snippet": "• based on the ranks of data rather than raw values. • suitable for ordinal data or when assumptions of pearson ’ s rr are not met. 3. kendall ’ s tau"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p30#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 30, "snippet": "• β0β0 : intercept. • β1β1 : slope ( rate of change in yy per unit change in xx ). • : error term. 2. multiple linear regression • models the relation"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p31#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 31, "snippet": "• identifying causal factors in experimental studies. key differences between correlation and regression aspect correlation regression purpose measure"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p32#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 32, "snippet": "1. correlation analysis : • compute the pearson correlation coefficient ( rr ) to measure the strength and direction of the relationship. 2. regressio"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p33#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 33, "snippet": "1. trend : the long - term movement or direction in the data. 2. seasonality : regular, repeating patterns or cycles in the data ( e. g., yearly, mont"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p34#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 34, "snippet": "• a time series forecasting method that weights recent observations more heavily than older ones. types include : • simple exponential smoothing : for"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p35#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 35, "snippet": "• the fourier transform converts time - domain data into frequency - domain data, decomposing a time series into sinusoidal components of different fr"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p36#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 36, "snippet": "aspect time series analysis spectral analysis domain works in the time domain. works in the frequency domain. method arima, exponential smoothing, etc"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p37#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 37, "snippet": "chatgpt content analysis : an overview content analysis is a research method used to systematically analyze and interpret textual, visual, or audio da"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p38#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 38, "snippet": "• identify the specific research questions you want to answer through content analysis. for example, you may want to know how often a particular theme"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p39#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 39, "snippet": "• present the results in a structured manner, including the themes, patterns, frequencies, and any interpretations or conclusions drawn from the conte"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p40#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 40, "snippet": "6. applications of content analysis 1. media and communication studies : • analyzing how certain themes ( e. g., violence, gender roles, political ide"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p41#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 41, "snippet": "• nvivo : a qualitative data analysis tool that helps researchers code and analyze textual data, identifying patterns and themes. • maxqda : another t"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p42#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 42, "snippet": "• flexibility : thematic analysis is adaptable and can be applied to a variety of data types, such as interviews, focus groups, open - ended survey re"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p43#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 43, "snippet": "• the researcher should also give clear, concise names to each theme to capture the essence of what it represents. step 6 : writing the report • final"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p44#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 44, "snippet": "3. time - consuming : • thematic analysis can be labor - intensive and time - consuming, especially with large datasets, as it requires a detailed exa"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p45#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 45, "snippet": "• step 1 : familiarization with data : the researcher conducts interviews with employees from different industries. • step 2 : generating codes : code"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p46#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 46, "snippet": "narrative analysis : an overview narrative analysis is a qualitative research method used to study the stories or personal narratives shared by indivi"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p47#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 47, "snippet": "• it explores how the storyteller arranges the events to create meaning, highlight conflicts, or emphasize certain points. 2. thematic narrative analy"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p48#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 48, "snippet": "• researchers analyze the content and structure of the narrative, identifying key events, turning points, or significant moments in the story. • they "}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p49#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 49, "snippet": "• narrative analysis highlights the narrator ' s role in constructing their own story, emphasizing personal agency and self - determination. 4. flexib"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p50#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 50, "snippet": "• example : examining student stories to understand the role of education in shaping their identity and future goals. 4. healthcare and illness : • it"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p51#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 51, "snippet": "narrative analysis is a powerful method for exploring the richness of human experience through the stories people tell about their lives. by focusing "}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p52#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 52, "snippet": "• iterative process : grounded theory is an ongoing, iterative process that involves constant revisiting and revising of data, codes, and emerging the"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p53#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 53, "snippet": "• theoretical coding involves developing an abstract theoretical framework or theory that explains how the categories and concepts are related to one "}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p54#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 54, "snippet": "• grounded theory allows for a deep understanding of the social processes and behaviors under study by providing a rich, nuanced perspective on how pe"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p55#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 55, "snippet": "1. sociology : • grounded theory is widely used in sociology to study social processes, relationships, and phenomena such as group dynamics, social mo"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p56#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 56, "snippet": "• step 4 : theoretical coding : the researcher develops theoretical concepts explaining how new teachers navigate their transition, including \" balanc"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p57#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 57, "snippet": "• literary works • social media posts • legal and institutional texts 2. key features of discourse analysis • language as social practice : discourse "}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p58#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 58, "snippet": "3. foucauldian discourse analysis : • based on the work of michel foucault, this approach explores how knowledge, power, and truth are constructed in "}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p59#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 59, "snippet": "• the researcher begins to analyze the data for recurring themes, phrases, and discursive patterns. • the analysis often involves identifying keywords"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p60#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 60, "snippet": "• discourse analysis, especially critical discourse analysis, allows researchers to uncover hidden power relations, inequalities, and biases that may "}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p61#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 61, "snippet": "• researchers use discourse analysis to examine how media texts ( such as news articles, advertisements, and tv shows ) represent certain social group"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p62#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 62, "snippet": "conclusion discourse analysis is a powerful qualitative research method that allows researchers to explore the ways in which language constructs socia"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p63#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 63, "snippet": "• data import and export : pspp supports various data formats, including csv, excel, and spss ’ s own. sav format, making it easy to import data from "}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p64#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 64, "snippet": "• in pspp : analyze → descriptive statistics → frequencies • this will produce tables showing how many respondents fall into each education and income"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p65#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 65, "snippet": "• free and open - source : one of the most significant benefits of pspp is that it is free to use, which makes it an excellent choice for researchers "}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p66#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 66, "snippet": "sofa ( statistics open for all ) sofa ( statistics open for all ) is a free, open - source statistical analysis software designed to make statistical "}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p67#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 67, "snippet": "• the software allows users to create high - quality, customizable visuals to help present their data and results effectively. • pivot tables : • sofa"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p68#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 68, "snippet": "• limited advanced statistical methods : • while sofa covers many standard statistical techniques, it lacks some of the more advanced statistical meth"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p69#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 69, "snippet": "• if the anova test shows significant differences, the researcher may want to perform post - hoc tests to determine which specific groups differ from "}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p70#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 70, "snippet": "for this case study, we will assume nost data refers to \" non - ordered statistical time \" data, a hypothetical type of observational data often seen "}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p71#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 71, "snippet": "week group a ( solo ) group b ( group ) group c ( none ) 2 52 58 42 3 55 60 41............ 16 80 75 50 step 2 : data preparation to plot nost data, we"}
{"id": "RM/RM Unit 3(Chatgpt.pdf#p72#c1", "source": "RM/RM Unit 3(Chatgpt.pdf", "page": 72, "snippet": "• group c ( no study ) : the line for this group might show little improvement, indicating that not studying results in stagnant or low performance. s"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p1#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 1, "snippet": "page 1 of 73 research methodology – assignment question : systematic result in any field of inquiry involves three basic operations, what are these? s"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p2#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 2, "snippet": "page 2 of 73 question : compare and contrast between basic research and applied research in brief. question : describe the necessary steps of conducti"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p3#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 3, "snippet": "page 3 of 73 financial analysis : a financial analysis determine the cost of each items used to produce goods and services. management will also revie"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p4#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 4, "snippet": "page 4 of 73 question : what is historical research? historical research is a qualitative technique. historical research studies the meaning of past e"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p5#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 5, "snippet": "page 5 of 73 gadgets, a researcher con ducting exploratory research on this topic may simply wish to learn more about students ’ use of these gadgets."}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p6#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 6, "snippet": "page 6 of 73 as citizens ’ need for advanced health - care system become overwhelming, paramount progress must be made in conventional drug developmen"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p7#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 7, "snippet": "page 7 of 73 crop diversification and intensification through multiple cropping 3. soil science : micronutrient related research on specific crops and"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p8#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 8, "snippet": "page 8 of 73 political acceptability : 1. not acceptable - 01 2. more or less acceptable - 02 3. fully acceptable - 03 applicability : 1. no chance of"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p9#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 9, "snippet": "page 9 of 73 question : what do you mean by typology of research? briefly write about basic, applied and evaluative research with example. the term ty"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p10#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 10, "snippet": "page 10 of 73 example : goiter is highly prevalent in many parts of ba ngladesh. unicef, bangladesh initiated a lipiodol injection campaign in some se"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p11#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 11, "snippet": "page 11 of 73 question : what is qualitative research? how does it differ from quantitative research? qualitative research is a scientific method of o"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p12#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 12, "snippet": "page 12 of 73 1. ethnography ethnographic research is probably the most familiar and applicable type of qualitative method to ux professionals. in eth"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p13#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 13, "snippet": "page 13 of 73 interviews and existing documents to build a theory based on the data. you go through a series of open and axial coding techniques to id"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p14#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 14, "snippet": "page 14 of 73 types of research methods according to nature of the study types of the research methods according to the nature of research can be divi"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p15#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 15, "snippet": "page 15 of 73 main differences between exploratory and conclusive research designs : question : describe the characteristics of a research in brief. c"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p16#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 16, "snippet": "page 16 of 73 4. credibility : credibility comes with the use of the best source of information and best procedures in research. if you are using seco"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p17#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 17, "snippet": "page 17 of 73 3. the procedural design of the research should be carefully planned to yield results that are as objective as possible. 4. the research"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p18#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 18, "snippet": "page 18 of 73 question : describe the principle goals of research the goals of research are – 1. to explore a phenomena. 2. to produce some new knowle"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p19#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 19, "snippet": "page 19 of 73 5. deciding on the sample design 6. collecting data 7. processing and analyzing data 8. writting the report 9. disseminating the finding"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p20#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 20, "snippet": "page 20 of 73 1. relevance : before one decides on a topic, each proposed topic should be compared with all other options. this enables the researcher"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p21#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 21, "snippet": "page 21 of 73 ( a ) identification of research topic / problem the identification of research problem is the first and foremost step that every resear"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p22#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 22, "snippet": "page 22 of 73 the purpose of a literature review is to : place each work in the context of its contribution to understanding the research problem bein"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p23#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 23, "snippet": "page 23 of 73 ( c ) data collection methods data collection is a process of collecting information from all the rele vant sources to find answers to t"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p24#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 24, "snippet": "page 24 of 73 research report is divided into three parts as : i. first part ( formality part ) : ( i ) cover page ( ii ) title page ( iii ) certifica"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p25#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 25, "snippet": "page 25 of 73 ( x ) order of contents ( xi ) number of copies ( xii ) format – type and size of paper ; lengths width, and depth of report ; and patte"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p26#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 26, "snippet": "page 26 of 73 the following is a general summary of some ethical research proposals : 1. honesty 2. objectivity 3. integrity 4. carefulness 5. opennes"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p27#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 27, "snippet": "page 27 of 73 there are three basic methods of research : 1 ) survey, 2 ) observation, and 3 ) experiment. each method has its advantages and disadvan"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p28#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 28, "snippet": "page 28 of 73 the “ iterative ” process of research ultimately, the key to a successful research project lies in iteration : the process of returning "}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p29#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 29, "snippet": "page 29 of 73 questionnaires and surveys responses can be analyzed with quantitative methods by assigning numerical values to likert - type scales res"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p30#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 30, "snippet": "page 30 of 73 ethnography is a more holistic approach to evaluation researcher can become a confounding variable documents and records consists of exa"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p31#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 31, "snippet": "page 31 of 73 their skills may result in biasing effect. in a face to face interview, the respondent may mistrust the interviewer or dodge certain que"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p32#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 32, "snippet": "page 32 of 73 question : write the scope of research in brief. environmental level 1. technological innovations : research is conducted to know & adap"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p33#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 33, "snippet": "page 33 of 73 develop new methods for the same. ultimately the motive remains for reducing loss & increase profitability. 3. production : here, resear"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p34#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 34, "snippet": "page 34 of 73 2. data can be lost on a small or independent scale whether it ' s personal data loss, or data loss within businesses and organizations,"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p35#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 35, "snippet": "page 35 of 73 question : what is research hypothesis? write the characteristics of it. research hypothesis : a research hypothesis is a specific, clea"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p36#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 36, "snippet": "page 36 of 73 1. hypothesis should be clear and precise. if the hypothesis is not clear and precise, the inferences drawn on its basis cannot be taken"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p37#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 37, "snippet": "page 37 of 73 below are some of the important types of hypothesis : - 1. simple hypothesis simple hypothesis is that one in which there exits relation"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p38#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 38, "snippet": "page 38 of 73 question : what are errors usually found in testing a hypothesis. the null hypothesis h0 represents a theory that has been put forward e"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p39#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 39, "snippet": "page 39 of 73 question : what is experimental design? experimental design is the process of planning a study to meet specified objectives. or - a blue"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p40#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 40, "snippet": "page 40 of 73 question : “ disseminate findings is crucial for research success. ” – explain disseminate findings is crucial for research success. it "}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p41#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 41, "snippet": "page 41 of 73 partners / influencers - think about who i will engage with to amplify my message. involve stakeholders in research planning from an ear"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p42#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 42, "snippet": "page 42 of 73 data collection procedures : data is collected from each study by two independent reviewers. if the reviewers report different informati"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p43#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 43, "snippet": "page 43 of 73 9. apart from this, every year, new university students of bangladesh can aware of research in bangladesh and also can join research ins"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p44#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 44, "snippet": "page 44 of 73 question : conclusions should be drawn based on findings and recommendation should be drawn based on conclusions. the conclusion is inte"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p45#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 45, "snippet": "page 45 of 73 a null hypothesis is a type of hypothesis used in statistics that proposes that no statistical significance exists in a set of given obs"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p46#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 46, "snippet": "page 46 of 73 for example, suppose someone claims that shigella is resistant to ciprofloxacin. so, we have raise a statement against the claim that sh"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p47#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 47, "snippet": "page 47 of 73 ( iv ) index ( brief contents ) ( v ) table of contents ( detailed index ) ( vi ) acknowledgement ( vii ) list of tables and figures use"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p48#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 48, "snippet": "page 48 of 73 ( xii ) format – type and size of paper ; lengths width, and depth of report ; and pattern of writing including paragraph, indent, numbe"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p49#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 49, "snippet": "page 49 of 73 example : the university of south australia is committed to promo ting an environment of honesty, integrity, accuracy and responsibility"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p50#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 50, "snippet": "page 50 of 73 the research process far more arduous, and you will likely regret it. in addition to being time - consuming, research can also be frustr"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p51#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 51, "snippet": "page 51 of 73 it will help the decision makers evaluate the research questions your project should answer as well as the research methods your project"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p52#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 52, "snippet": "page 52 of 73 learning objectives : identify the process for writing meaningful research questions. evaluate research questions. developing a good res"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p53#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 53, "snippet": "page 53 of 73 this could lead to new exploration on an operation table, wasting precious time. inefficient management, to stressful procedure and even"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p54#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 54, "snippet": "page 54 of 73 7. lack of enough fund and well equipped lab. 8. lack of adequate supervision and corruption. question : describe the problems encounter"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p55#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 55, "snippet": "page 55 of 73 blame on that, unless until the research is re - done. this le ads to undertaking of overlapping studies, as there is a want of informat"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p56#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 56, "snippet": "page 56 of 73 once problem situation has been identified and clearly stated, it is important to justify th e importance of the problem. it is very imp"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p57#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 57, "snippet": "page 57 of 73 respondent may have had contact with during the last weeks. not only will it be easier for the respondent to find an answer but also lea"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p58#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 58, "snippet": "page 58 of 73 can be expected during the course of a project, but the project team should be realistic in setting the project scope, budget and schedu"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p59#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 59, "snippet": "page 59 of 73 scholarly peer reviewed journals ( also called refereed journals ) are academic publications that carry out a review process of an autho"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p60#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 60, "snippet": "page 60 of 73 secondary data : a secondary data is one that gives information about a primary source. in this source, the original information is sel "}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p61#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 61, "snippet": "page 61 of 73 area of his interest that have not been so far researched. such review not only provides him an exposure to a larger body of knowledge, "}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p62#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 62, "snippet": "page 62 of 73 1 ) is there any limitation to the period that one must go upto in reviewing literature?................................................"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p63#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 63, "snippet": "page 63 of 73 question : what is literature for research? describe the importance of literature review in biotechnological research? a literature revi"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p64#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 64, "snippet": "page 64 of 73 the literature review will help a s tudent compare and contrast what a student is doing in the historical context of the research as wel"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p65#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 65, "snippet": "page 65 of 73 a student can locate the hard copies of the journals that are appropriate to your study ; look at citation or abstract indices to identi"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p66#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 66, "snippet": "page 66 of 73 abstracts booklets and published proceedings, as well as from questionnaire responses. records include complete ordering information to "}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p67#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 67, "snippet": "page 67 of 73 # it relates a study to the larger ongoing dialogue in the literature about a topic, filling in gap and extending prior studies ( cooper"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p68#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 68, "snippet": "page 68 of 73 reason # 4 : developing a methodology conducting a literature review before beginning research also lets us see how similar studies have"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p69#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 69, "snippet": "page 69 of 73 question : depict the necessity of biotech research in bangladesh necessity of biotech research in bangladesh : 1. increase in farming p"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p70#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 70, "snippet": "page 70 of 73 question : mention different dimension of a proposal? the elements of a research proposal are highlighted below : 1. title : it should b"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p71#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 71, "snippet": "page 71 of 73 interventions : if an intervention is introduced, a description must be given of the drugs or devices ( proprietary names, manufa cturer"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p72#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 72, "snippet": "page 72 of 73 references : the proposal should end with relevant references on the subject. for web based search include the date of access for the ci"}
{"id": "RM/Research Methodology notes and assignments downloaded  (1).pdf#p73#c1", "source": "RM/Research Methodology notes and assignments downloaded  (1).pdf", "page": 73, "snippet": "page 73 of 73 5. aman agro industries - produce virus free potato. 1. lal teer - produce livestock and seed. question : why annual research review of "}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p1#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 1, "snippet": "123 world wide web internet and web information systems issn 1386 - 145x world wide web doi 10. 1007 / s11280 - 018 - 0582 - 1 deep learning approache"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p2#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 2, "snippet": "123 your article is protected by copyright and all rights are held exclusively by springer science + business media, llc, part of springer nature. thi"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p3#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 3, "snippet": "deep learning approaches for video - based anomalous activity detection karishma pawar 1 & vahida attar1 received : 16 august 2017 / revised : 16 apri"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p4#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 4, "snippet": "level of anomaly detection, and anomaly measurement for anomalous activity detec - tion. the focus has been given on various anomaly detection framewo"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p5#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 5, "snippet": "the aim of this survey paper is to thoroughly analyze the progress made by deep learning techniques in the field of video based anomaly detection. the"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p6#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 6, "snippet": "2 taxonomy of video based anomalous activity detection “ anomalies are patterns in data that do not conform to a well - defined notion of normal behav"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p7#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 7, "snippet": "varies from its neighbors ( for example, driving a vehicle in wrong direction ). local anomalous activity detection has been well investigated in [ 2,"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p8#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 8, "snippet": "other context. therefore, activities related to each other by space and time forms the context [ 123 ], and it is necessary to model the appearance fe"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p9#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 9, "snippet": "“ negative ” in the confusion matrix. receiver operating characteristic ( roc ) curves are preferably used for visualizing and comparing the performan"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p10#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 10, "snippet": "level and pixel - level measures do not take into account this false region ( “ lucky guess ” ). in order to detect such “ lucky guess ” region, duel "}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p11#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 11, "snippet": "systems at public places requires the practical constraints like how much amount of video the system buffers before processing, time required for dete"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p12#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 12, "snippet": "3. 1. 1 local feature modeling methods statistical approach the statistical method for describing anomalous activity is given as follows [ 85 ]. in th"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p13#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 13, "snippet": "anomaly detection. in this paper, multi - scale analysis method is used for accurate localization of anomalies in the video. cheng et al. [ 13 ] appli"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p14#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 14, "snippet": "moves to finer level i. e. actual frame level. this method of hierarchical processing reduces computational overhead, and thus detects anomalies at re"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p15#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 15, "snippet": "4 deep learning approaches for anomalous activity detection the availability of large datasets and high availability of gpus at lower costs has result"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p16#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 16, "snippet": "models generally assume that the features come from a predetermined type of distribution and therefore are likely to fail if the feature distribution "}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p17#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 17, "snippet": "encoder and sparse representation of video. as ae gives more reconstruction error for anomalous patch and sparse representation of video implies chanc"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p18#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 18, "snippet": "recognition to detect anomalies and employed two - channel approach for representing video in terms of appearance and motion ( optical flow ) similar "}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p19#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 19, "snippet": "though deep learning models are good at extracting high level abstraction from the given video, it is difficult to model regression tasks since labels"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p20#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 20, "snippet": "features and oc - svm is used for building the normalcy model. this framework is based on motion feature representation. it can be extended by introdu"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p21#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 21, "snippet": "table 1 deep learning approahces for anomalous activity detection deep learning approach ref., year deep architectures technique anomaly formulation a"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p22#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 22, "snippet": "table 1 ( continued ) deep learning approach ref., year deep architectures technique anomaly formulation anomaly measurement perf. metric dataset fram"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p23#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 23, "snippet": "table 2 traditional approaches for anomalous activity detection approach ref., year video representation technique anomaly formulation anoma ly measur"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p24#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 24, "snippet": "table 3 comparative study of real time performance of anomaly detection ap proaches based on traditiona l and deep learning ( dl ) me thods ( values m"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p25#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 25, "snippet": "5. 2 public datasets for indoor and outdoor surveillance considering the requirements of real - life scenarios, various datasets for anomaly detection"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p26#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 26, "snippet": "table 4 benchmarked datasets of anomaly detection dataset features scenarios gt resolution ucsd pedestrian 1 ( ped 1 ) and pedestrian 2 ( ped 2 ) [ 10"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p27#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 27, "snippet": "table 4 ( continued ) dataset features scenarios gt resolution mit traffic [ 62 ] covers traffic video captured by stationary camera detection of pede"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p28#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 28, "snippet": "done in multi - view anomaly detection. it is very challenging to incorporate different levels of anomaly detection using multiple views in a single f"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p29#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 29, "snippet": "of anomaly detection. this research domain is very promising area since it will act as foundation stone in many future computer vision based projects "}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p30#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 30, "snippet": "21. del giorno, a., bagnell, j. a. & hebert, m. a discriminative framework for anomaly detection in large videos. in computer vision – eccv 2016 : 14t"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p31#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 31, "snippet": "48. leach, m. j. v., sparks, e. p., robertson, n. m. : contextual anomaly detection in crowded surveillance scenes. pattern recogn. lett. 44, 7 1 – 79"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p32#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 32, "snippet": "74. ravanbakhsh, m., nabi, m., mousavi, h., sangineto, e. & sebe, n. : plug - and - play cnn for crowd motion analysis : an application in abnormal ev"}
{"id": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf#p33#c1", "source": "RM/Sample Survey Paper Karishma_ Pawar (1).pdf", "page": 33, "snippet": "102. video surveillance market : http : / / www. transparencymarketresearch. com / video - surveillance - vsaas - market. html 103. violent - flows da"}
{"id": "RM/updated_iit_survey(1).pptx#p1#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 1, "snippet": "iit bombay # › iste workshop introduction to research methodologies sahana murthy iit bombay june 25 – july 4, 2012 tips on literature review"}
{"id": "RM/updated_iit_survey(1).pptx#p2#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 2, "snippet": "iit bombay # › what is a literature review summary of related work descriptive and evaluative analytical in nature"}
{"id": "RM/updated_iit_survey(1).pptx#p3#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 3, "snippet": "iit bombay # › why literature review need to give credit to others ’ work giving credit is the “ law ”! gives background information to your work situ"}
{"id": "RM/updated_iit_survey(1).pptx#p4#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 4, "snippet": "iit bombay # › when should i do the literature review at the beginning, when you start looking for a problem"}
{"id": "RM/updated_iit_survey(1).pptx#p5#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 5, "snippet": "iit bombay # › when should i do the literature review at the beginning, when you start looking for a problem in the middle, once your work is under wa"}
{"id": "RM/updated_iit_survey(1).pptx#p6#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 6, "snippet": "iit bombay # › when should i do the literature review at the beginning, when you start looking for a problem in the middle, once your work is under wa"}
{"id": "RM/updated_iit_survey(1).pptx#p7#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 7, "snippet": "iit bombay # › when should i do the literature review at the beginning, when you start looking for a problem in the middle, once your work is under wa"}
{"id": "RM/updated_iit_survey(1).pptx#p8#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 8, "snippet": "iit bombay # › what type of papers should i look for? engineering / scientific research papers published research papers in “ established ” journals, "}
{"id": "RM/updated_iit_survey(1).pptx#p9#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 9, "snippet": "iit bombay # › what type of papers should i look for? engineering / scientific research papers published research papers in “ established ” journals, "}
{"id": "RM/updated_iit_survey(1).pptx#p10#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 10, "snippet": "iit bombay # › where should i look for papers? databases and indexes inspec web of science ieee xplore compendex scifinder scopus check your field jou"}
{"id": "RM/updated_iit_survey(1).pptx#p11#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 11, "snippet": "iit bombay # › where should i look for papers? what about google?"}
{"id": "RM/updated_iit_survey(1).pptx#p12#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 12, "snippet": "iit bombay # › where should i look for papers? what about google? google scholar search more appropriate than simple google search. also try citeseer"}
{"id": "RM/updated_iit_survey(1).pptx#p13#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 13, "snippet": "iit bombay # › what information should i track bibliographic information title authors year of publication source – journal / proceedings / book name "}
{"id": "RM/updated_iit_survey(1).pptx#p14#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 14, "snippet": "iit bombay # › how should i select papers to read? some tips ask your guide survey / review articles are a good starting point try to identify seminal"}
{"id": "RM/updated_iit_survey(1).pptx#p15#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 15, "snippet": "iit bombay # › what to do if i find too few sources discuss with expert, guide look for related topics, synonyms broaden scope of topic a little, then"}
{"id": "RM/updated_iit_survey(1).pptx#p16#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 16, "snippet": "iit bombay # › what to do if i find too many sources plan to scope or scale down your search restrict by year ( recent ) restrict by source ( only two"}
{"id": "RM/updated_iit_survey(1).pptx#p17#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 17, "snippet": "iit bombay # › how to report literature search"}
{"id": "RM/updated_iit_survey(1).pptx#p18#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 18, "snippet": "iit bombay # › according to paper1, clickers were found to be improve student motivation and attendance. in paper2, authors used clickers in a cs prog"}
{"id": "RM/updated_iit_survey(1).pptx#p19#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 19, "snippet": "iit bombay # › how not to write the literature review / related work section according to paper1, clickers were found to be improve student motivation"}
{"id": "RM/updated_iit_survey(1).pptx#p20#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 20, "snippet": "iit bombay # › how not to write the literature review / related work section a literature review is not : a descriptive list of papers a summary of on"}
{"id": "RM/updated_iit_survey(1).pptx#p21#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 21, "snippet": "iit bombay # › how should i write the literature review / related work section identify themes, factors, or variables relevant to your problem. one wa"}
{"id": "RM/updated_iit_survey(1).pptx#p22#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 22, "snippet": "iit bombay # › how should i write the literature review / related work section analyze on the basis of categories, strengths & weaknesses"}
{"id": "RM/updated_iit_survey(1).pptx#p23#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 23, "snippet": "iit bombay # › how should i write the literature review / related work section analyze on the basis of categories, strengths & weaknesses 1 ) clickers"}
{"id": "RM/updated_iit_survey(1).pptx#p24#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 24, "snippet": "iit bombay # › how should i write the literature review / related work section provide synthesis of the reviewed literature. what are existing solutio"}
{"id": "RM/updated_iit_survey(1).pptx#p25#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 25, "snippet": "iit bombay # › how should i write the literature review / related work section identify the gaps in existing work. in most current solutions aimed at "}
{"id": "RM/updated_iit_survey(1).pptx#p26#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 26, "snippet": "iit bombay # › how should i write the literature review / related work section identify the gaps in existing work. in most current solutions aimed at "}
{"id": "RM/updated_iit_survey(1).pptx#p27#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 27, "snippet": "iit bombay # › points to note be aware of disciplinary conventions"}
{"id": "RM/updated_iit_survey(1).pptx#p28#c1", "source": "RM/updated_iit_survey(1).pptx", "page": 28, "snippet": "iit bombay # › points to note you have to write about what has already been written, but write something original."}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p1#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 1, "snippet": "research methodology ( class : mtech )"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p2#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 2, "snippet": "# ›"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p3#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 3, "snippet": "ethics ethics ( also moral philosophy ) is the branch of philosophy that involves systematic, defending, and recommending concepts of right and wrong "}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p4#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 4, "snippet": "the following is a rough and general summary of some ethical principals that various codes address : honesty strive for honesty in all scientific comm"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p5#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 5, "snippet": "carefulness avoid careless errors and negligence ; carefully and critically examine your own work and the work of your peers. keep good records of res"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p6#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 6, "snippet": "confidentiality protect confidential communications, such as papers or grants submitted for publication, personnel records, trade or military secrets,"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p7#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 7, "snippet": "respect for colleagues respect your colleagues and treat them fairly. social responsibility strive to promote socially good and prevent or mitigate so"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p8#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 8, "snippet": "three codes of ethics acm codes of ethics and professional conduct : http : / / www. acm. org / about / code - of - ethics acm / ieee - cs software en"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p9#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 9, "snippet": "case study 1 kylie turville owns her own consulting business, and has several people working for her. kylie is currently designing a database manageme"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p10#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 10, "snippet": "case study 1 : relevant clauses principle 1. public software engineers shall act consistently with the public interest. in particular, software engine"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p11#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 11, "snippet": "principle 2. client and employer software engineers shall act in a manner that is in the best interests of their client and employer, consistent with "}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p12#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 12, "snippet": "case study 1 : relevant clauses principle 3. product software engineers shall ensure that their products and related modifications meet the highest pr"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p13#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 13, "snippet": "case study 2 george smith is an experienced software developer working for acme software company. mr. smith is now working on a project for the u. s. "}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p14#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 14, "snippet": "plagiarism plagiarism is taking another ’ s work and passing it off as your own. in a broad sense we are all guilty of plagiarism many times each day."}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p15#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 15, "snippet": "plagiarism in research is usually quite different true plagiarism is, quite bluntly, stealing. sometimes a person just copies text word for word from "}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p16#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 16, "snippet": "plagiarism extends to more than words one should always provide references for any statistics graphs tables numbers, etc. that one wishes to use in on"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p17#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 17, "snippet": "copy direct quotations make sure that you write it precisely, word - for - word as in the original. also essential that you enclose the quoted text in"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p18#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 18, "snippet": "paraphrasing is often preferable to taking direct quotations it ’ s often preferable to take down the substance of an author ’ s idea in your own word"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p19#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 19, "snippet": "“ plagiarism ” sometimes not intentional many instances of plagiarism stem from sloppy research rather than through a deliberate desire to cheat. many"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p20#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 20, "snippet": "“ common knowledge ” and plagiarism it is not necessary to document every single statement. one need not give a reference for stating that president j"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx#p21#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1)(1).pptx", "page": 21, "snippet": "thank you # ›"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p1#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 1, "snippet": "you said : significance of report writing in research methodology chatgpt said : chatgpt report writing is a crucial aspect of research methodology fo"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p2#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 2, "snippet": "chatgpt said : chatgpt writing a research report involves several systematic steps to ensure clarity, structure, and completeness. here are the key st"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p3#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 3, "snippet": "• importance and relevance of the research • scope and limitations of the study 5. literature review • purpose : provides an overview of previous rese"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p4#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 4, "snippet": "• discussion of the implications of the findings 9. conclusion • purpose : summarizes the key findings and their broader implications. • contents : • "}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p5#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 5, "snippet": "the layout of a research report typically follows a structured format to ensure clarity, organization, and consistency. while the exact layout may var"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p6#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 6, "snippet": "table of contents 1. introduction.................................................. page 1 2. literature review......................................."}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p7#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 7, "snippet": "• sample or participant details • tools or instruments used • ethical considerations example layout : markdown copy code 3. methodology 3. 1 research "}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p8#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 8, "snippet": "• elements : • summary of the key findings. • practical or theoretical implications. • recommendations for future research. example layout : css copy "}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p9#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 9, "snippet": "example layout : css copy code acknowledgments i would like to thank [ name ( s ) ] for their guidance and support during the course of this research."}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p10#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 10, "snippet": "• incident reports : document unexpected events ( e. g., accidents, equipment failures ). • sales reports : provide insights into sales performance an"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p11#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 11, "snippet": "• purpose : to provide regular updates on specific activities or metrics over a fixed period. • examples : • monthly performance reports • quarterly e"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p12#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 12, "snippet": "• contents : • description of the proposed project • cost - benefit analysis • potential risks and rewards • final recommendation 11. evaluation repor"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p13#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 13, "snippet": "• design proposals • storyboard reports • artistic project summaries 15. news reports • purpose : to provide factual information about current events,"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p14#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 14, "snippet": "• use note - taking and mind - mapping techniques for better organization. 3. drafting • begin with a rough draft based on the outline. • focus on con"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p15#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 15, "snippet": "• zotero : manage and format references, create bibliographies, and store research materials. • mendeley : reference management with pdf annotation an"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p16#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 16, "snippet": "• canva : for designing covers, visuals, or infographics. 9. online libraries and research databases • google scholar : to access academic articles an"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p17#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 17, "snippet": "4. mosaic plagiarism : patching together phrases from different sources without proper attribution. 5. accidental plagiarism : failing to cite or para"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p18#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 18, "snippet": "3. tools to detect plagiarism a. plagiarism detection software • turnitin : commonly used in academic institutions to identify unoriginal content. • g"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p19#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 19, "snippet": "by following these practices, you can produce original, ethical, and high - quality work while respecting the intellectual property of others. you sai"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p20#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 20, "snippet": "• name of your supervisor ( if applicable ) • date of submission 2. abstract / executive summary • brief overview of your research ( 150 – 300 words )"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p21#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 21, "snippet": "• justify the need for funding, if applicable. 9. expected outcomes • discuss the potential impact of your research. • describe how the results will a"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p22#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 22, "snippet": "• overly ambitious goals : ensure that your objectives are achievable within the available resources. • lack of structure : follow the standard format"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p23#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 23, "snippet": "• outcome : dr. doe developed a scalable battery technology, leading to publications in nature energy and further collaboration with private industry."}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p24#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 24, "snippet": "• a nonprofit organization received $ 2. 5 million to develop low - cost diagnostic tools for malaria in sub - saharan africa. • outcome : scaled up d"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p25#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 25, "snippet": "• outcome : published comparative policy analyses influencing energy legislation in the student ’ s home country. • application process : • requires a"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p26#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 26, "snippet": "conclusion these case studies illustrate how research grants empower individuals and institutions to advance knowledge, address societal challenges, a"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p27#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 27, "snippet": "• file for a patent before publishing, as public disclosure may invalidate the novelty requirement. b. institutional guidelines • check your instituti"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p28#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 28, "snippet": "• define how others can use your work. use licenses like creative commons to specify permissions. b. monitoring infringement • use tools like plagiari"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p29#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 29, "snippet": "• licensing options : • creative commons • open data initiatives 9. case study : balancing publication and patent scenario : a researcher discovers a "}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p30#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 30, "snippet": "1. automatic protection : copyright protection begins as soon as a work is created and fixed in a tangible form ( e. g., written, recorded, or saved e"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p31#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 31, "snippet": "5. copyright registration while copyright is automatic, registration with the appropriate authority ( e. g., u. s. copyright office ) is recommended f"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p32#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 32, "snippet": "• digital rights management ( drm ) : technology to prevent unauthorized copying or distribution of digital works. • challenges : • piracy and unautho"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p33#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 33, "snippet": "the invention. patents are critical for encouraging innovation by providing inventors with a legal mechanism to protect and commercialize their creati"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p34#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 34, "snippet": "• conduct a prior art search to ensure the invention is unique. • document the invention in detail, including diagrams, working principles, and applic"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p35#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 35, "snippet": "• injunctions to stop the infringement. 8. international patents • patents are territorial, meaning protection is only valid in the country where the "}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p36#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 36, "snippet": "patents play a vital role in fostering innovation by protecting and incentivizing inventors. understanding the patenting process, criteria, and benefi"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p37#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 37, "snippet": "• allows users to define custom style guides for teams or businesses. d. plagiarism detection • checks content against billions of web pages and acade"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p38#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 38, "snippet": "• initially targeted academic users, especially non - native english speakers. • expanded to professionals and general users as communication demands "}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p39#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 39, "snippet": "• personalization : offering more tailored writing solutions based on individual user preferences. 8. lessons from grammarly ' s success 1. user - cen"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p40#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 40, "snippet": "2. core features and functionalities excel provides a wide range of features that cater to diverse user needs : a. basic features • spreadsheet layout"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p41#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 41, "snippet": "• in 2010, microsoft introduced excel online, allowing users to access spreadsheets via web browsers. • integration with microsoft 365 enabled cloud s"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p42#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 42, "snippet": "7. impact and importance excel has had a transformative impact on the way data is managed and analyzed : 1. accessibility : simplifies complex tasks, "}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p43#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 43, "snippet": "4. big data handling : improved support for external databases and large datasets. 11. conclusion excel remains a cornerstone of modern productivity t"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p44#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 44, "snippet": "• org charts : to represent hierarchical structures in businesses or teams. • mind maps : for brainstorming and organizing ideas. b. integration and c"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p45#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 45, "snippet": "b. rebranding and expansion • in 2020, draw. io was rebranded as diagrams. net to emphasize its focus on being a universal diagramming tool and broade"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p46#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 46, "snippet": "• the open - source aspect has allowed for constant innovation through community contributions, making the platform adaptable to various needs. c. rea"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p47#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 47, "snippet": "3. cost - effective solution : offering a high - quality, free alternative to expensive diagramming software has attracted a diverse user base. 4. sim"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p48#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 48, "snippet": "• kanban boards : visualize tasks in columns based on project phases ( e. g., \" to do, \" \" in progress, \" \" completed \" ). • task management : create "}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p49#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 49, "snippet": "3. add - ons and integrations : while the base focalboard tool is free, mattermost offers paid add - ons or integrations with other enterprise systems"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p50#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 50, "snippet": "• roadmaps : product managers use focalboard for tracking the development of features, product roadmaps, and user stories. • cross - department collab"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p51#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 51, "snippet": "8. future prospects focalboard is positioned to grow with the following developments : 1. enhanced integrations : focalboard is likely to expand its i"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p52#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 52, "snippet": "• purpose : mendeley was developed to provide researchers with an intuitive, collaborative, and efficient way to manage academic papers and references"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p53#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 53, "snippet": "e. integration with word processors • word plugin : mendeley integrates with microsoft word and libreoffice, allowing users to insert citations and au"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p54#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 54, "snippet": "• in 2013, elsevier acquired mendeley for an undisclosed amount, allowing mendeley to expand its capabilities and integrate more closely with elsevier"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p55#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 55, "snippet": "6. impact and success factors a. streamlined research process • mendeley has significantly simplified the research process by providing an integrated "}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p56#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 56, "snippet": "3. collaboration enhancements : further improvements to collaboration features, possibly integrating with other academic platforms and tools for impro"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p57#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 57, "snippet": "• bibtex integration : jabref is built around the bibtex format, which is a widely used reference format in latex documents. it allows users to store "}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p58#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 58, "snippet": "• latex support : jabref is fully integrated with latex, making it the go - to reference manager for academics who use latex for writing papers. it ge"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p59#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 59, "snippet": "5. use cases jabref is used primarily by researchers, students, and academics in fields where latex is commonly used for writing, such as engineering,"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p60#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 60, "snippet": "• jabref allows extensive customization, enabling researchers to tailor the tool to their specific needs. it supports a wide range of citation styles,"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p61#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 61, "snippet": "community have contributed to its success. while it faces challenges related to user experience and non - latex compatibility, jabref ’ s strengths in"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p62#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 62, "snippet": "• trademark search : this service allows businesses to search for existing trademarks to ensure their brand name or logo is unique and does not infrin"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p63#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 63, "snippet": "geographical indications ( gi ) are signs used on products that have a specific geographical origin and possess qualities or a reputation due to that "}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p64#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 64, "snippet": "• licensing and monetization : ip professionals help businesses find licensing opportunities to monetize patents, trademarks, copyrights, or designs. "}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p65#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 65, "snippet": "chatgpt inpass - indian patent advanced search system inpass ( indian patent advanced search system ) is an online tool provided by the indian patent "}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p66#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 66, "snippet": "• it offers information about the stages of patent applications, including filing, publication, and examination. users can track the progress of their"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p67#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 67, "snippet": "4. legal and regulatory compliance : • inpass aids in ensuring compliance with patent laws by helping users monitor their patents ’ legal status and t"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p68#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 68, "snippet": "• examples : the shape of a bottle, the design of a chair, or a smartphone ' s appearance. 3. plant patents : • these patents are granted for the disc"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p69#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 69, "snippet": "3. examination : • after filing, the application is assigned to a uspto examiner. the examiner reviews the application to ensure the invention meets a"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p70#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 70, "snippet": "3. uspto patent full - text and image database ( patft ) : a database for searching the full - text of u. s. patents, including images of the original"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p71#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 71, "snippet": "• abstract : a short abstract summarizing the paper was included at the beginning of the document. • introduction : the introduction followed the abst"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p72#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 72, "snippet": "• the act stipulated that a patent could only be granted for an invention that was novel, non - obvious, and had industrial applicability. • the act d"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p73#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 73, "snippet": "• it also introduced the provisions for protection of traditional knowledge and bio - diversity, providing safeguards against misappropriation. 3. pat"}
{"id": "RM/RM Unit 5 (ChatGPt).pdf#p74#c1", "source": "RM/RM Unit 5 (ChatGPt).pdf", "page": 74, "snippet": "• indian patents act 1970 : the indian patents act of 1970 was a critical piece of legislation that established the framework for patents in india, la"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p1#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 1, "snippet": "ie research methodology ranjit kumar 8 )"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p2#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 2, "snippet": "r esearch m ethodology a step - by - step guide for beginners"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p3#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 3, "snippet": "sage has been part of the global academic community since 1965, supporting high quality research and learning that transforms society and our understa"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p4#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 4, "snippet": "3 rd edition"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p5#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 5, "snippet": "r esearch m ethodology a step - by - step guide for beginners ranjit kumar"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p6#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 6, "snippet": "© ranjit kumar 1999, 2005, 2011 first edition published 1999 second edition published 2005. reprinted 2007, 2008 ( twice ), 2009 ( twice ) this third "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p7#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 7, "snippet": "to my daughter, parul"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p8#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 8, "snippet": "contents list of figures list of tables preface 1 research : a way of thinking research : an integral part of your practice research : a way to gather"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p9#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 9, "snippet": "how to review the literature searching for the existing literature reviewing the selected literature developing a theoretical framework developing a c"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p10#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 10, "snippet": "7 the research design what is a research design? the functions of a research design the theory of causality and the research design summary 8 selectin"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p11#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 11, "snippet": "prerequisites for data collection methods of data collection in qualitative research constructing a research instrument in qualitative research collec"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p12#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 12, "snippet": "the calculation of sample size sampling in qualitative research the concept of saturation point in qualitative research summary step v writing a resea"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p13#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 13, "snippet": "summary step vii processing and displaying data 15 processing data part one : data processing in quantitative studies editing coding part two : data p"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p14#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 14, "snippet": "undertaking an evaluation : the process step 1 : determining the purpose of evaluation step 2 : developing objectives or evaluation questions step 3 :"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p15#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 15, "snippet": "figures 1. 1 the applications of research 1. 2 types of research 2. 1 the research journey 2. 2 the research process 2. 3 the chapters in the book in "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p16#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 16, "snippet": "8. 6 experimental and non - experimental studies 8. 7 randomisation in experiments 8. 8 the after - only design 8. 9 measurement of change through a b"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p17#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 17, "snippet": "16. 1 the structure of a table 16. 2a two - dimensional histogram 16. 2b three - dimensional histogram 16. 2c two - dimensional histogram with two var"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p18#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 18, "snippet": "tables 1. 1 types of research studies from the perspective of objectives 2. 1 differences between qualitative and quantitative research 3. 1 some comm"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p19#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 19, "snippet": "preface this book is based upon my experiences in research as a student, practitioner and teacher. the difficulties i faced in understanding research "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p20#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 20, "snippet": "variables ’ and ‘ constructing hypotheses ’. similarly, for the operational step, step iii, ‘ constructing an instrument for data collection ’, the ch"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p21#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 21, "snippet": "each chapter has a list of keywords that students are likely to encounter in the chapter. in places the language has been changed to enhance flow, und"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p22#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 22, "snippet": "c hapter 1 research : a way of thinking in this chapter you will learn about : some of the reasons for doing research how research can be used to gath"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p23#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 23, "snippet": "how many patients do i see every day? what are some of the most common conditions prevalent among my patients? what are the causes of these conditions"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p24#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 24, "snippet": "as a supervisor, administrator or manager of an agency, again different questions relating to effectiveness and efficiency of a service may come to yo"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p25#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 25, "snippet": "practice, ebp has become an important part of many other professions such as nursing, allied health services, mental health, community health, social "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p26#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 26, "snippet": "the fairly informal, based upon clinical impressions, to the strictly scientific, adhering to the conventional expectations of scientific procedures. "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p27#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 27, "snippet": "figure 1. 1 the applications of research however, the degree to which these criteria are expected to be fulfilled varies from discipline to discipline"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p28#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 28, "snippet": "as a prefix meaning again, anew or over again and the latter as a verb meaning to examine closely and carefully, to test and try, or to probe. togethe"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p29#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 29, "snippet": "rigorous – you must be scrupulous in ensuring that the procedures followed to find answers to questions are relevant, appropriate and justified. again"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p30#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 30, "snippet": "types of research : application perspective if you examine a research endeavour from the perspective of its application, there are two broad categorie"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p31#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 31, "snippet": "environment affects children ’ s level of academic achievement. the fourth type of research, from the viewpoint of the objectives of a study, is calle"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p32#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 32, "snippet": "the structured approach to enquiry is usually classified as quantitative research and unstructured as qualitative research. other distinctions between"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p33#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 33, "snippet": "variables ; and if the analysis is geared to ascertain the magnitude of the variation. examples of quantitative aspects of a research study are : how "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p34#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 34, "snippet": "information gathering, analysis and interpretation that enables it to be called a research process. summary there are several ways of collecting and u"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p35#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 35, "snippet": "c hapter 2 the research process : a quick glance in this chapter you will learn about : the eight - step model for carrying out research phase i decid"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p36#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 36, "snippet": "the world by critical exposition in non - technical terms the results and methods of their constructive work, that more than mere instinct is involved"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p37#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 37, "snippet": "figure 2. 1 the research journey – touch each post and select methods and procedures appropriate for your journey since, at a number of steps of the r"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p38#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 38, "snippet": "figure 2. 2 shows the proposed model. the tasks identified in arrows are the operational steps you need to follow in order to conduct a study, quantit"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p39#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 39, "snippet": "figure 2. 2 the research process"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p40#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 40, "snippet": "figure 2. 3 the chapters in the book in relation to the operational steps the following sections of this chapter provide a quick glance at the whole p"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p41#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 41, "snippet": "your disposal, the time available, and your own and your research supervisor ’ s expertise and knowledge in the field of study. it is equally importan"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p42#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 42, "snippet": "methods for collecting data using attitudinal scales. the concepts of validity and reliability in relation to a research instrument are discussed in c"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p43#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 43, "snippet": "about your study : what you are proposing to do ; how you plan to proceed ; why you selected the proposed strategy. therefore it should contain the fo"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p44#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 44, "snippet": "if you want quantitative analysis, it is also necessary to decide upon the type of analysis required ( i. e. frequency distribution, cross - tabulatio"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p45#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 45, "snippet": "s tep i formulating a research problem this operational step includes four chapters : chapter 3 : reviewing the literature chapter 4 : formulating a r"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p46#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 46, "snippet": "c hapter 3 reviewing the literature in this chapter you will learn about : the functions of the literature review in research how to carry out a liter"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p47#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 47, "snippet": "it provides a theoretical background to your study. it helps you establish the links between what you are proposing to examine and what has already be"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p48#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 48, "snippet": "thorough literature review helps you to fulfil this expectation. another important reason for doing a literature review is that it helps you to unders"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p49#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 49, "snippet": "books though books are a central part of any bibliography, they have their disadvantages as well as advantages. the main advantage is that the materia"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p50#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 50, "snippet": "if you have been able to identify any useful journals and articles, prepare a list of those you want to examine, by journal. select one of these journ"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p51#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 51, "snippet": "the internet in almost every academic discipline and professional field, the internet has become an important tool for finding published literature. t"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p52#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 52, "snippet": "themes, using a separate sheet of paper for each theme of the framework so far developed. as you read further, go on slotting the information where it"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p53#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 53, "snippet": "figure 3. 1a developing a theoretical framework – the relationship between mortality and fertility figure 3. 1b theoretical framework for the study ‘ "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p54#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 54, "snippet": "writing about the literature reviewed now, all that remains to be done is to write about the literature you have reviewed. as mentioned in the beginni"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p55#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 55, "snippet": "summary reviewing the literature is a continuous process. it begins before a research problem is finalised and continues until the report is finished."}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p56#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 56, "snippet": "c hapter 4 formulating a research problem in this chapter you will learn about : the importance of formulating a research problem sources of research "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p57#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 57, "snippet": "the research problem broadly speaking, any question that you want answered and any assumption or assertion that you want to challenge or investigate c"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p58#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 58, "snippet": "again be classified as correlational and the study design used, methods of collecting data and its analysis will be a part of the quantitative methodo"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p59#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 59, "snippet": "you can study a problem, a programme or a phenomenon in any academic field or from any professional perspective. for example, you can measure the effe"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p60#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 60, "snippet": "within the time and with the resources at your disposal. even if you are undertaking a descriptive study, you need to consider its magnitude carefully"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p61#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 61, "snippet": "figure 4. 1 dissecting the subject area of domestic violence into subareas step 1 identify a broad field or subject area of interest to you. ask yours"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p62#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 62, "snippet": "figures 4. 2 to 4. 4 operationalise steps 1 – 7 with examples from different academic disciplines ( health, social work / social sciences and communit"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p63#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 63, "snippet": "figure 4. 2 steps in formulating a research problem – alcoholism example 2 : suppose you want to study the relationship between fertility and mortalit"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p64#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 64, "snippet": "figure 4. 4 narrowing a research problem – health subobjectives should be numerically listed. they should be worded clearly and unambiguously. make su"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p65#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 65, "snippet": "figure 4. 5 characteristics of objectives if your study is primarily descriptive, your main objective should clearly describe the major focus of your "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p66#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 66, "snippet": "select only those who have come from a specific country ( ies )? in a way you need to narrow your definition of the study population as you have done "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p67#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 67, "snippet": "in a research study you need to define these clearly in order to avoid ambiguity and confusion. this is achieved through the process of developing ope"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p68#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 68, "snippet": "research : once i supervised a student who was interested in attention - deficit hyperactivity disorder ( adhd ). she wanted to find out, as she put i"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p69#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 69, "snippet": "for you to think about refamiliarise yourself with the keywords listed at the beginning of this chapter and if you are uncertain about the meaning or "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p70#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 70, "snippet": "c hapter 5 identifying variables in this chapter you will learn about : what variables and concepts are and how they are different how to turn concept"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p71#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 71, "snippet": "concepts, and knowledge about variables, plays an important role in reducing this variability and ‘ fine tuning ’ your research problem. what is a var"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p72#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 72, "snippet": "there are, indeed, a great many writers who believe that scientific method is inherently inapplicable to such judgements as estimation or value, as ‘ "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p73#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 73, "snippet": "different people as such cannot be measured to variable ( e. g. attitude – subjective, income – objective ) converting concepts into variables if you "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p74#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 74, "snippet": "types of variable a variable can be classified in a number of ways. the classification developed here results from looking at variables in three diffe"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p75#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 75, "snippet": "3. variables which affect or influence the link between cause - and - effect variables ; 4. connecting or linking variables, which in certain situatio"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p76#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 76, "snippet": "figure 5. 1 types of variable note : classification across a classification base is not mutually exclusive but classification within a classification "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p77#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 77, "snippet": "smoking might cause cancer. these variables may either increase or decrease the magnitude of the relationship. in the above example the extent of smok"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p78#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 78, "snippet": "relationship, the type of counselling service is the independent variable and the extent of marriage problems is the dependent variable. the magnitude"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p79#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 79, "snippet": "developing countries is primarily due to lack of acceptance of contraceptives. the extent of the use of contraceptives determines the level of the dec"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p80#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 80, "snippet": "whether it is qualitative ( as in nominal and ordinal scales ) or quantitative in nature ( as in interval and ratio scales ). on the whole there is ve"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p81#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 81, "snippet": "for a beginner it is important to understand that the way a variable is measured determines the type of analysis that can be performed, the statistica"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p82#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 82, "snippet": "measurement scale into four categories : nominal or classificatory scale ; ordinal or ranking scale ; interval scale ; ratio scale. table 5. 4 summari"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p83#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 83, "snippet": "those identifying with the liberals are classified as ‘ liberal ’, and so on. the name chosen for a subcategory is notional, but for effective communi"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p84#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 84, "snippet": "in temperature between two other objects, c and d, is 45°c, you can say that the difference in temperature between c and d is three times greater than"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p85#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 85, "snippet": "for you to think about refamiliarise yourself with the keywords listed at the beginning of this chapter and if you are uncertain about the meaning or "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p86#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 86, "snippet": "c hapter 6 constructing hypotheses in this chapter you will learn about : the definition of a hypothesis the functions of a hypothesis in your researc"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p87#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 87, "snippet": "research study. they tell a researcher what specific information to collect, and thereby provide greater focus. let us imagine you are at the races an"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p88#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 88, "snippet": "a hypothesis is written in such a way that it can be proven or disproven by valid and reliable data – it is in order to obtain these data that we perf"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p89#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 89, "snippet": "the characteristics of a hypothesis there are a number of considerations to keep in mind when constructing a hypothesis, as they are important for val"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p90#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 90, "snippet": "broadly, there are two categories of hypothesis : 1. research hypotheses ; 2. alternate hypotheses. the formulation of an alternate hypothesis is a co"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p91#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 91, "snippet": "2. a greater proportion of females than males are smokers in the study population. 3. a total of 60 per cent of females and 30 per cent of males in th"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p92#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 92, "snippet": "errors in testing a hypothesis as already mentioned, a hypothesis is an assumption that may prove to be either correct or incorrect. it is possible to"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p93#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 93, "snippet": "procedures make the convention of hypotheses formulation far less practicable and advisable. even within quantitative studies the importance attached "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p94#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 94, "snippet": "s tep ii conceptualising a research design this operational step includes two chapters : chapter 7 : the research design chapter 8 : selecting a study"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p95#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 95, "snippet": "c hapter 7 the research design in this chapter you will learn about : what research design means the important functions of research design issues to "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p96#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 96, "snippet": "1993 : 94 ) a research design is a procedural plan that is adopted by the researcher to answer questions validly, objectively, accurately and economic"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p97#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 97, "snippet": "in the case of interviews, where will they be conducted? how will ethical issues be taken care of? chapter 8 describes some of the commonly used study"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p98#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 98, "snippet": "1. counselling per se. 2. all the factors other than counselling that affect the marital problems. 3. the outcome – that is, the change or otherwise i"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p99#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 99, "snippet": "expressed as an equation : [ change in the outcome variable ] = [ change because of the chance variable ] ± [ change because of extraneous variables ]"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p100#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 100, "snippet": "sole purpose of having a control group, as mentioned earlier, is to measure the change that is a result of extraneous variables. the effect of chance "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p101#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 101, "snippet": "affect the dependent variable will be similar in both groups. the following two methods ensure that the control and experimental groups are comparable"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p102#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 102, "snippet": "for you to think about refamiliarise yourself with the keywords listed at the beginning of this chapter and if you are uncertain about the meaning or "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p103#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 103, "snippet": "c hapter 8 selecting a study design in this chapter you will learn about : the differences between quantitative and qualitative study designs common s"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p104#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 104, "snippet": "information gathering methods and processes, are often flexible and evolving ; hence, most qualitative designs are not as structured and sequential as"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p105#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 105, "snippet": "study designs in quantitative research some of the commonly used designs in quantitative studies can be classified by examining them from three differ"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p106#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 106, "snippet": "figure 8. 1 types of study design the cross - sectional study design cross - sectional studies, also known as one - shot or status studies, are the mo"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p107#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 107, "snippet": "the quality assurance of a service provided by an organisation. the impact of unemployment on street crime ( this could also be a before - and - after"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p108#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 108, "snippet": "the impact of administrative restructuring on the quality of services provided by an organisation. the effectiveness of a marriage counselling service"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p109#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 109, "snippet": "measurement scale at the pre - test stage may, for a number of reasons, shift towards the mean at the post - test stage ( see figure 8. 3 ). they migh"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p110#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 110, "snippet": "in the enquiry, with the same result. the main advantage of a longitudinal study is that it allows the researcher to measure the pattern of change and"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p111#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 111, "snippet": "the retrospective – prospective study design retrospective – prospective studies focus on past trends in a phenomenon and study it into the future. pa"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p112#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 112, "snippet": "experimental ; non - experimental ; quasi - or semi - experimental. to understand the differences, let us consider some examples. suppose you want to "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p113#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 113, "snippet": "fact comparable in every respect except the treatment. the process of randomisation is designed to ensure that the groups are comparable. in a random "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p114#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 114, "snippet": "figure 8. 8 the after - only design the after - only experimental design in an after - only design the researcher knows that a population is being, or"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p115#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 115, "snippet": "intervention ( see figure 8. 9 ). figure 8. 9 measurement of change through a before - and - after design the before - and - after design takes care o"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p116#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 116, "snippet": "figure 8. 10 the control experimental design in the experimental group, total change in the dependent variable ( y e ) can be calculated as follows : "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p117#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 117, "snippet": "therefore, the impact of any intervention is equal to the difference in the ‘ before ’ and ‘ after ’ observations in the dependent variable between th"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p118#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 118, "snippet": "sometimes you seek to compare the effectiveness of different treatment modalities and in such situations a comparative design is appropriate. with a c"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p119#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 119, "snippet": "one of the three teaching models, following up with an ‘ after ’ observation. the difference in the levels of comprehension is attributed to the diffe"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p120#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 120, "snippet": "other designs commonly used in quantitative research there are some research designs that may be classified in the typology described above but, becau"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p121#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 121, "snippet": "people who were recently recruited to the programme and following them through until the intervention has been completed may take a long time. in such"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p122#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 122, "snippet": "cohort studies are based upon the existence of a common characteristic such as year of birth, graduation or marriage, within a subgroup of a populatio"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p123#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 123, "snippet": "for an in - depth understanding you are advised to consult books on qualitative research. case study the case study, though dominantly a qualitative s"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p124#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 124, "snippet": "other sources such as older people, ancestors, folklore, stories. according to ritchie ( 2003 : 19 ), ‘ memory is the core of oral history, from which"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p125#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 125, "snippet": "diversity. participant observation participant observation is another strategy for gathering information about a social interaction or a phenomenon in"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p126#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 126, "snippet": "terms of number of participants. also, in group discussions you may select the participants, but for community forums there is self - selection of the"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p127#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 127, "snippet": "figure 8. 16 action research design action research seems to follow two traditions. the british tradition tends to view action research as a means of "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p128#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 128, "snippet": "3. the goal of feminist research is changing the social inequality between men and women. in fact, feminist research may be classified as action resea"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p129#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 129, "snippet": "for you to think about refamiliarise yourself with the keywords listed at the beginning of this chapter and if you are uncertain about the meaning or "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p130#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 130, "snippet": "s tep iii constructing an instrument for data collection this operational step includes three chapters : chapter 9 : selecting a method of data collec"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p131#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 131, "snippet": "c hapter 9 selecting a method of data collection in this chapter you will learn about : differences in methods of data collection in quantitative and "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p132#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 132, "snippet": "data collection? were the questions or issues discussed during data collection predetermined or developed during data collection? how was the informat"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p133#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 133, "snippet": "information gathered using the first approach is said to be collected from primary sources, whereas the sources used in the second approach are called"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p134#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 134, "snippet": "observation is one way to collect primary data. observation is a purposeful, systematic and selective way of watching and listening to an interaction "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p135#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 135, "snippet": "there is always the possibility of observer bias. if an observer is not impartial, s / he can easily introduce bias and there is no easy way to verify"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p136#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 136, "snippet": "observer / researcher. a scale may be one -, two - or three - directional, depending upon the purpose of the observation. for example, in the scale in"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p137#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 137, "snippet": "figure 9. 2 a three - directional rating scale the choice of a particular method for recording your observation is dependent upon the purpose of the o"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p138#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 138, "snippet": "you in the context of the discussion. unstructured interviews are prevalent in both quantitative and qualitative research. the difference is in how in"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p139#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 139, "snippet": "figure 9. 4 example 1 ways of administering a questionnaire a questionnaire can be administered in different ways."}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p140#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 140, "snippet": "figure 9. 5 example 2 the mailed questionnaire – the most common approach to collecting information is to send the questionnaire to prospective respon"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p141#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 141, "snippet": "advantages of a questionnaire a questionnaire has several advantages : it is less expensive. as you do not interview respondents, you save time, and h"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p142#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 142, "snippet": "before responding. in situations where an investigator wants to find out only the study population ’ s opinions, this method may be inappropriate, tho"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p143#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 143, "snippet": "contents of the covering letter it is essential that you write a covering letter with your mailed questionnaire. it should very briefly : introduce yo"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p144#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 144, "snippet": "assumptions ). from the responses to question c in figure 9. 7, where the income for a respondent is recorded in exact dollars, the different descript"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p145#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 145, "snippet": "figure 9. 7 examples of open - ended questions advantages and disadvantages of open - ended questions open - ended questions provide in - depth inform"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p146#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 146, "snippet": "formulating effective questions the wording and tone of your questions are important because the information and its quality largely depend upon these"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p147#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 147, "snippet": "whereas others may answer the second part and some may answer both parts. incidentally, this question is also ambiguous in that it does not specify ‘ "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p148#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 148, "snippet": "extremely important role as each question in the instrument must stem from the objectives, research questions and / or hypotheses of the study. it is "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p149#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 149, "snippet": "in terms of the best technique for asking sensitive or threatening questions, there appears to be two opposite opinions, based on the manner in which "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p150#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 150, "snippet": "the order of questions in a questionnaire or in an interview schedule is important as it affects the quality of information, and the interest and even"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p151#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 151, "snippet": "to draw a clear distinction between quantitative and qualitative methods of data collection is both difficult and inappropriate because of the overlap"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p152#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 152, "snippet": "the only difference between a focus group interview and an in - depth interview is that the former is undertaken with a group and the latter with an i"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p153#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 153, "snippet": "depth information is needed or little is known about the area. the flexibility allowed to the interviewer in what s / he asks of a respondent is an as"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p154#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 154, "snippet": "note that these served as starting points for discussions. the group members were encouraged to discuss whatever they wanted to in relation to the per"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p155#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 155, "snippet": "make sure that the required data is available before you proceed further with your study. format – before deciding to use data from secondary sources "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p157#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 157, "snippet": "c hapter 10 collecting data using attitudinal scales in this chapter you will learn about : what attitudinal scales are and how to use them the functi"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p158#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 158, "snippet": "in quantitative research there are three scales which have been developed to ‘ measure ’ attitudes. each of these scales is based upon different assum"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p159#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 159, "snippet": "overall indicator. this reduces the risk of an expression of opinion by respondents being influenced by their opinion on only one or two aspects of th"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p160#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 160, "snippet": "figure 10. 1 an example of a categorical scale it is important to remember that the likert scale does not measure attitude per se. it does help to pla"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p161#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 161, "snippet": "of the attitude in question and on the capacity of the population to make fine distinctions. figure 10. 1 shows a five - point categorical scale that "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p162#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 162, "snippet": "figure 10. 5 scoring positive and negative statements figure 10. 6 calculating an attitudinal score statement 2 is a negative statement. in this case "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p163#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 163, "snippet": "unlike the likert scale, the thurstone scale calculates a ‘ weight ’ or ‘ attitudinal value ’ for each statement. the weight ( equivalent to the media"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p164#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 164, "snippet": "observation – you can explore the diversity in the attitudes but cannot find other aspects like : how many people have a particular attitude, the inte"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p165#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 165, "snippet": "c hapter 11 establishing the validity and reliability of a research instrument in this chapter you will learn about : the concept of validity differen"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p166#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 166, "snippet": "in this chapter we will discuss the concept of validity as applied to measurement procedures or the research tools used to collect the required inform"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p167#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 167, "snippet": "for example, if you want to find out about age, income, height or weight, it is relatively easy to establish the validity of the questions, but to est"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p168#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 168, "snippet": "suppose you develop an instrument to determine the suitability of applicants for a profession. the instrument ’ s validity might be determined by comp"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p169#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 169, "snippet": "that is, the extent of difference in the measurements when you collect the same set of information more than once, using the same instrument under the"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p170#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 170, "snippet": "external consistency procedures external consistency procedures compare findings from two independent processes of data collection with each other as "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p171#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 171, "snippet": "the idea behind internal consistency procedures is that items or questions measuring the same phenomenon, if they are reliable indicators, should prod"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p172#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 172, "snippet": "by four indicators – credibility, transferability, dependability and confirmability – and it is these four indicators that reflect validity and reliab"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p173#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 173, "snippet": "summary one of the differences in quantitative and qualitative research is in the use of and importance attached to the concepts of validity and relia"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p174#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 174, "snippet": "s tep iv selecting a sample this operational step includes one chapter : chapter 12 : selecting a sample"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p175#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 175, "snippet": "c hapter 12 selecting a sample in this chapter you will learn about : the differences between sampling in qualitative and quantitative research defini"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p176#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 176, "snippet": "may influence the selection of a sample such as : the ease in accessing the potential respondents ; your judgement that the person has extensive knowl"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p177#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 177, "snippet": "sampling in quantitative research the concept of sampling let us take a very simple example to explain the concept of sampling. suppose you want to es"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p178#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 178, "snippet": "the number of students, families or electors from whom you obtain the required information is called the sample size and is usually denoted by the let"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p179#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 179, "snippet": "table 12. 1 the difference between sample statistics and the population mean this analysis suggests a very important principle of sampling : principle"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p180#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 180, "snippet": "different : a = 18, b = 26, c = 32 and d = 40. in other words, we are visualising a population where the individuals with respect to age – the variabl"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p181#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 181, "snippet": "selection, does not cover the sampling population accurately and completely ; a section of a sampling population is impossible to find or refuses to c"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p182#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 182, "snippet": "probability of selection of each element in the population is the same ; that is, the choice of an element in the sample is not influenced by other co"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p183#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 183, "snippet": "decided upon. this method is used in some lotteries. 2. computer program – there are a number of programs that can help you to select a random sample."}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p184#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 184, "snippet": "source : statistical tables, 3e, by f. james rohlf and robert r. sokal. copyright © 1969, 1981, 1994 by w. h. freeman and company. used with permissio"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p185#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 185, "snippet": "sampling with or without replacement random sampling can be selected using two different systems : 1. sampling without replacement ; 2. sampling with "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p186#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 186, "snippet": "stratification are clearly identifiable in the study population. for example, it is much easier to stratify a population on the basis of gender than o"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p187#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 187, "snippet": "figure 12. 5 the procedure for selecting a stratified sample figure 12. 6 the concept of cluster sampling non - random / non - probability sampling de"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p188#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 188, "snippet": "1. quota sampling ; 2. accidental sampling ; 3. judgemental sampling or purposive sampling ; 4. expert sampling ; 5. snowball sampling. what different"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p189#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 189, "snippet": "sampling makes no such attempt. you stop collecting data when you reach the required number of respondents you decided to have in your sample. this me"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p190#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 190, "snippet": "snowball sampling is the process of selecting a sample using networks. to start with, a few individuals in a group or organisation are selected and th"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p191#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 191, "snippet": "assigned to a case, or arranged in a way that is convenient to the users of the records. if the ‘ width of an interval ’ is large, say, 1 in 30 cases,"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p192#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 192, "snippet": "are studying, in the study population? answering these questions is necessary regardless of whether you intend to determine the sample size yourself o"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p193#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 193, "snippet": "1. guessing ; 2. consulting an expert ; 3. obtaining the value of σ from previous comparable studies ; or 4. carrying out a pilot study to calculate t"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p194#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 194, "snippet": "as you already know, in qualitative research data is usually collected to a point where you are not getting new information or it is negligible – the "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p195#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 195, "snippet": "consider the implications of selecting a sample based upon your choice as a researcher and how you could make sure that you do not introduce bias. in "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p196#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 196, "snippet": "s tep v writing a research proposal this operational step includes one chapter : chapter 13 : writing a research proposal"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p197#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 197, "snippet": "c hapter 13 how to write a research proposal in this chapter you will learn about : the purpose of a research proposal in quantitative and qualitative"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p198#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 198, "snippet": "outline the various tasks you plan to undertake to fulfil your research objectives, test hypotheses ( if any ) or obtain answers to your research ques"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p199#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 199, "snippet": "your proposal should follow the suggested guidelines and be written in an academic style. it must contain appropriate references in the body of the te"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p200#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 200, "snippet": "the proposal should start with an introduction to include some of the information listed below. remember that some of the contents suggested in this s"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p201#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 201, "snippet": "suppose your research project is to conduct a study of the attitudes of foster carers towards foster payment in … ( name of the place / state / countr"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p202#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 202, "snippet": "management of adhd. etc. the problem having provided a broad introduction to the area under study, now focus on issues relating to its central theme, "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p203#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 203, "snippet": "example b what are the broad issues, debates, arguments and counter - arguments regarding foster - care payment? what are the attitudes of foster pare"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p204#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 204, "snippet": "objectives of the study in this section include a statement of both your study ’ s main and subobjectives ( see chapter 4 ). your main objective indic"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p205#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 205, "snippet": "1. to determine the form and mode of payment for taking care of a foster child. 2. to identify the factors that foster parents believe should be the b"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p206#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 206, "snippet": "h i = etc. example b h 1 = most people become foster parents because of their love of children. h 2 = a majority of foster parents would like to be tr"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p207#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 207, "snippet": "how and where can respondents contact you if they have queries? example a the study is primarily designed to find out from a cross - section of immigr"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p208#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 208, "snippet": "with adhd among different types of family. the potential respondents will be individually contacted by the researcher to seek their consent for partic"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p209#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 209, "snippet": "all academic institutions are particular about any ethical issues that research may have. to deal with them, all institutions have some form of policy"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p210#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 210, "snippet": "informal discussions with the group members, those families who are expected to be information rich in treating and managing a child with adhd will be"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p211#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 211, "snippet": "number of children ; education ; occupation ; etc. example b frequency distributions in terms of : age ; income ; education ; occupation ; marital sta"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p212#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 212, "snippet": "parental involvement in students ’ studies ; self - esteem ; peer group influence ; number of hours spent on studies ; etc. cross - tabulations : acad"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p213#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 213, "snippet": "example a it is proposed that the report will be divided into the following chapters : chapter 1 : introduction chapter 2 : the socioeconomic – demogr"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p214#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 214, "snippet": "chapter 2 : issues and difficulties faced by family members in bringing up a child with adhd chapter 3 : adhd and its perceived effects on the child c"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p215#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 215, "snippet": "summary a research proposal details the operational plan for obtaining answers to research questions. it must tell your supervisor and others what you"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p216#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 216, "snippet": "s tep vi collecting data this operational step includes one chapter to make you aware of the ethical issues in research : chapter 14 : considering eth"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p217#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 217, "snippet": "c hapter 14 considering ethical issues in data collection in this chapter you will learn about : ethics : the concept stakeholders in research ethical"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p218#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 218, "snippet": "in whose judgement must they be considered correct? closely related questions are as follows : are there universal principles of conduct that can be a"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p219#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 219, "snippet": "who should be considered as a research participant varies from profession to profession. generally, all those with direct and indirect involvement in "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p220#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 220, "snippet": "conditions. provided any piece of research is likely to help society directly or indirectly, it is acceptable to ask questions, if you first obtain th"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p221#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 221, "snippet": "the dilemma you face as a researcher is whether you should ask sensitive and intrusive questions. in the author ’ s opinion it is not unethical to ask"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p222#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 222, "snippet": "provision or deprivation of a treatment both the provision and deprivation of a treatment may pose an ethical dilemma for you as a researcher. when te"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p223#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 223, "snippet": "the possibility of its being used against some of them, and you let them decide if they want to participate. some may participate for the betterment o"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p224#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 224, "snippet": "for you to think about refamiliarise yourself with the keywords listed at the beginning of this chapter and if you are uncertain about the meaning or "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p225#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 225, "snippet": "s tep vii processing and displaying data this operational step includes two chapters : chapter 15 : processing data chapter 16 : displaying data"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p226#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 226, "snippet": "c hapter 15 processing data in this chapter you will learn about : methods for processing data in quantitative studies how to edit data and prepare da"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p227#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 227, "snippet": "figure 15. 1 steps in data processing these procedures are the same whether your study is quantitative or qualitative, but what you do within each pro"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p228#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 228, "snippet": "editing irrespective of the method of data collection, the information collected is called raw data or simply data. the first step in processing your "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p229#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 229, "snippet": "considerations : 1. the way a variable has been measured ( measurement scale ) in your research instrument ( e. g. if a response to a question is desc"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p230#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 230, "snippet": "methods to communicate your findings. this is your choice, and it is based on your impression of the preference of your readers. for coding quantitati"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p231#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 231, "snippet": "format to illustrate how to develop a code book. the fixed format stipulates that a piece of information obtained from a respondent is entered in a sp"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p232#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 232, "snippet": "you feel that the responses are being repeated and you are getting no or very few new ones – that is, when you have reached a saturation point. table "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p233#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 233, "snippet": "40 - 11 12 - 13 44 - 15 2 ( b ) stuby 1 stuby 3 not opplicable no response same as in study same as in study same es in study lack of job satisfaction"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p234#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 234, "snippet": "= previous job better work morale ~ current job ~ previous job working in # team - current job ~ previous job work es an individual = current job ~ pr"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p235#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 235, "snippet": "now, one by one, examine the responses to each question to ascertain the similarities and differences. if two or more responses are similar in meaning"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p236#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 236, "snippet": "the same question. two responses to questions are commonly repeated : ‘ not applicable ’ and ‘ no response ’. you should select a number that can be u"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p237#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 237, "snippet": "in this question a respondent is asked to indicate the area in which s / he has achieved a tertiary qualification. the question asks for two aspects :"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p238#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 238, "snippet": "themes. the data then can also be analysed to determine the frequency of the themes if so desired. it is also possible to analyse the themes in relati"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p239#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 239, "snippet": "this is a highly structured question asking respondents to compare on a five - point ordinal scale their level of satisfaction with various areas of t"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p240#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 240, "snippet": "assigned and entered in columns 10 – 11. since there is only one qualification, study2 and study3 are not applicable ; therefore, a code of 88 was ent"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p241#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 241, "snippet": "figure 15. 4 some questions from a survey – respondent 3"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p242#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 242, "snippet": "figure 15. 5 some questions from a survey – respondent 59"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p243#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 243, "snippet": "figure 15. 6 some questions from a survey – respondent 81"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p244#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 244, "snippet": "figure 15. 7 an example of coded data on a code sheet step iv : verifying the coded data once the data is coded, select a few research instruments at "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p245#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 245, "snippet": "to concepts ) ; which variables are to be subjected to which statistical procedures. to illustrate, let us take the example from the survey used in th"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p246#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 246, "snippet": "age ; ms ; tedu ; study ; difwk. these determine whether job satisfaction before and after redeployment is affected by age, marital status, education,"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p247#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 247, "snippet": "analysing quantitative data manually coded data can be analysed manually or with the help of a computer. if the number of respondents is reasonably sm"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p248#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 248, "snippet": "part two : data processing in qualitative studies how you process and analyse data in a qualitative study depends upon how you plan to communicate the"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p249#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 249, "snippet": "responses. there are others who count how frequently a theme has occurred, and then provide a sample of the responses. it entirely depends upon the wa"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p250#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 250, "snippet": "and, to some extent, to take control of their situations themselves. it seems that in ‘ preparing a plan for a child under this model, the family of t"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p251#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 251, "snippet": "close collaboration with the family, extended family and other appropriate stakeholders, which makes them ( decisions and solutions ) more acceptable "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p252#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 252, "snippet": "prevention of removal of children some respondents felt that the model actually prevented kids from being removed from their families. according to on"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p253#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 253, "snippet": "dynamics, to think about the dynamics and it allows the families to participate in whatever they want to ’, said a participant of a focus group. fewer"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p254#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 254, "snippet": "and push it to the family. that is another good thing : they all get the same information, and we get and give the same information to them … and it i"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p255#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 255, "snippet": "they were holding on to cases. here it is more organised ’, one participant observed. with the old structure, ‘ case workers were very stressed ; they"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p256#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 256, "snippet": "in a way, the answer to the first question forms the basis for the second. statistics can play a very important role in answering your research questi"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p257#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 257, "snippet": "desirable but not essential for a study. the extent of their application depends upon the purpose of the study. statistics primarily help you to make "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p258#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 258, "snippet": "c hapter 16 displaying data in this chapter you will learn about : methods of communicating and displaying analysed data in quantitative and qualitati"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p259#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 259, "snippet": "because of the nature and purpose of investigation in qualitative research, text becomes the dominant and usually the sole mode of communication. in q"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p260#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 260, "snippet": "to identify a table is by the chapter number followed by the sequential number of the table in the chapter – the procedure adopted in this book. the m"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p261#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 261, "snippet": "figure 16. 1 the structure of a table table 16. 1 respondents by age ( frequency table for one population – hypothetical data ) age no. of respondents"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p262#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 262, "snippet": "note : figures in parentheses are percentages ( * rounding error ). table 16. 3 respondents by attitude towards uranium mining and age ( cross - tabul"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p263#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 263, "snippet": "what would have been the expected number of respondents in each subcategory had there been 100 respondents. percentages in a univariate table play a m"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p264#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 264, "snippet": "that a greater proportion of female than male respondents between the ages of 25 and 34 hold a strongly unfavourable attitude towards uranium mining. "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p265#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 265, "snippet": "them, each representing the frequency of a category or subcategory ( figures, 16. 2a, b, c ). their height is in proportion to the frequency they repr"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p266#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 266, "snippet": "figure 16. 2b three - dimensional histogram figure 16. 2c two - dimensional histogram with two variables"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p267#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 267, "snippet": "figure 16. 3 bar charts the stacked bar chart a stacked bar chart is similar to a bar chart except that in the former each bar shows information about"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p268#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 268, "snippet": "figure 16. 4 the stacked bar chart the 100 per cent bar chart the 100 per cent bar chart ( figure 16. 5 ) is very similar to the stacked bar chart. in"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p269#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 269, "snippet": "figure 16. 5 the 100 per cent bar chart the frequency polygon the frequency polygon is very similar to a histogram. a frequency polygon is drawn by jo"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p270#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 270, "snippet": "figure 16. 6 the frequency polygon figure 16. 7 the cumulative frequency polygon the stem - and - leaf display the stem - and - leaf display is an eff"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p271#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 271, "snippet": "figure 16. 8 the stem - and - leaf display the pie chart the pie chart is another way of representing data graphically ( figure 16. 9 ), this time as "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p272#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 272, "snippet": "the area chart for variables measured on an interval or a ratio scale, information about the subcategories of a variable can also be presented in the "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p273#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 273, "snippet": "for a scattergram, both the variables must be measured either on interval or ratio scales and the data on both the variables needs to be available in "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p274#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 274, "snippet": "before using statistical measures, make sure the data lends itself to the application of statistical measures, you have sufficient knowledge about the"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p275#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 275, "snippet": "s tep viii writing a research report this operational step includes one chapter : chapter 17 : writing a research report"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p276#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 276, "snippet": "c hapter 17 writing a research report in this chapter you will learn about : how to write a research report how to develop an outline for your researc"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p277#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 277, "snippet": "findings, though not essential, will make the information more easily understood by readers. as stated in the previous chapter, whether or not graphs "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p278#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 278, "snippet": "attitudes towards foster - care payments : suggested contents of chapter 1 chapter 1 introduction introduction the development of foster care foster c"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p279#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 279, "snippet": "2. it helps to identify the variance within a group ; for example, you may want to examine how the level of satisfaction of the consumers of a service"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p280#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 280, "snippet": "adequacy of foster - care payment should be presented here. ) adequacy by age ( cross - tabulation, i. e. responses to the question on adequacy of fos"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p281#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 281, "snippet": "role of community development funding officers advantages and disadvantages of the case management model satisfaction of staff with the model the mode"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p282#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 282, "snippet": "have formulated a hypothesis, state it here. ) what has your study found out? ( provide the hard data from your study here, as tables, graphs or text."}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p283#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 283, "snippet": "referencing the report should follow an academic style of referencing. according to butcher ( 1981 : 226 ), there are four referencing systems from wh"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p284#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 284, "snippet": "contents. the chapters should be written around the main themes of the study and for this your subobjectives are of immense help. when providing speci"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p285#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 285, "snippet": "c hapter 18 research methodology and practice evaluation in this chapter you will learn about : what evaluation is and why it is done the process for "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p286#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 286, "snippet": "many social, economic, health, education and political programmes. the very first question that may come to your mind, as a beginner, is : what is eva"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p287#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 287, "snippet": "figure 18. 1 the concept of evaluation and both processes are designed to collect and analyse information in order to answer research questions. in ev"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p288#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 288, "snippet": "you have a professional and ethical responsibility to provide a good quality of service to your clients. to ensure its effectiveness and efficiency, y"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p289#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 289, "snippet": "figure 18. 2 the intervention – development – evaluation model the development of an intervention usually starts with an assessment of the needs of a "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p290#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 290, "snippet": "perspectives in the classification of evaluation studies the various types of evaluation can be looked at from two perspectives : the focus of the eva"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p291#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 291, "snippet": "from the perspective of the focus of evaluation there are four types of evaluation : programme / intervention planning, process / monitoring, impact /"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p292#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 292, "snippet": "interest groups are represented in a community forum, it can provide a reasonable picture of the demand for a service. community forums are comparativ"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p293#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 293, "snippet": "community surveys and social indicators tend to be quantitative, whereas the others tend to be qualitative. thus they give you different types of info"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p294#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 294, "snippet": "evaluating participation of the target population in an evaluation study designed to examine the process of delivering an intervention, it is importan"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p295#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 295, "snippet": "as client satisfaction, problems and issues with the service, or how to improve its efficiency and effectiveness. how well you do this survey is depen"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p296#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 296, "snippet": "are used in process evaluation. the purpose for which you are going to use the findings should determine whether you adopt a quantitative or qualitati"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p297#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 297, "snippet": "analysis study which involved two - minute observations of activities of health workers in a community health programme. panel of experts – another me"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p298#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 298, "snippet": "this theory of causality is of particular relevance to impact assessment studies. in determining the impact of an intervention, it is important to rea"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p299#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 299, "snippet": "strengths of this design is that it enables you to isolate the impact of independent and extraneous variables. however, it adds the problem of compara"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p300#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 300, "snippet": "replicated cross - sectional design – the replicated cross - sectional design studies clients at different stages of an intervention, and is appropria"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p301#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 301, "snippet": "benefit analysis. this is primarily because of the difficulties in accurately identifying and measuring inputs and outputs, and then converting them t"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p302#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 302, "snippet": "evaluation, are the best judges of a programme. client - centred evaluations, again, may use qualitative or quantitative methods to find out how clien"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p303#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 303, "snippet": "identify and solve problems in the delivery process of a service. increase efficiency of the service delivery manner. determine the impacts of the int"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p304#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 304, "snippet": "evaluating a programme : example one for the first evaluation, after having initial discussions with various stakeholders, it was discovered that unde"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p305#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 305, "snippet": "the … model main objective : to evaluate the effectiveness of the … ( name of the model ) developed by … ( name of the office ). subobjectives : 1. to"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p306#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 306, "snippet": "subjective – descriptive impressions to objective – measurable – discrete changes. if you are inclined more towards qualitative studies, you may use i"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p307#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 307, "snippet": "second will provide a soft one. similarly, a change in the number of children, if asked as an opinion question, will be treated as a soft indicator. f"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p308#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 308, "snippet": "evaluation? how will the needed information be collected? how will you take care of the ethical issues confronting your evaluation? how will you maint"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p309#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 309, "snippet": "you adhere to ethical principles and the professional code of conduct. as you have seen, the process of a research study and that of an evaluation is "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p310#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 310, "snippet": "any pressure from anyone. surrendering to such pressure is unethical. summary in this chapter some of the aspects of evaluation research are discussed"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p311#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 311, "snippet": "a ppendix developing a research project : a set of exercises for beginners application is the essence of knowledge. however, there always remains a ga"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p312#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 312, "snippet": "care ; community health ; community needs ; foster care ; or the relationship between unemployment and street crime. chapter 4 of this book will help "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p313#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 313, "snippet": "5. _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ to investigate all these subareas is neither advisable nor feasible. select only those subareas that would "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p314#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 314, "snippet": "( b ) _ _ _ _ _ _ _ _ _ _ ( c ) _ _ _ _ _ _ _ _ _ _ ( d ) _ _ _ _ _ _ _ _ _ _ ( e ) _ _ _ _ _ _ _ _ _ _ the research questions to be answered through "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p315#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 315, "snippet": "clear about what tasks are involved, what time is realistically required and what skills you need to develop in order to conduct your study. consider "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p316#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 316, "snippet": "this part of the exercise is designed to help you operationalise the major concepts used in your study. refer to chapter 5 for additional information "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p317#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 317, "snippet": "hypothesis was true. otherwise, you conclude your hypothesis to be false. disproving a hypothesis is as important as, or more important than, proving "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p318#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 318, "snippet": "step ii having selected your main research question or broad area of study, list all questions that you want to find answers to. also list all issues "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p319#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 319, "snippet": "a : answers to the following questions will help you to develop your study design ( step ii ). 1. is the design that you propose to adopt to conduct y"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p320#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 320, "snippet": "7 ( a ) in either case, explain the reasons for your decision. 8. how will you collect data from your respondents ( e. g. interview, questionnaire )? "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p321#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 321, "snippet": "( iv ) if there are queries, how should respondents get in touch with you? b : on the basis of the above information, describe your study design. ( fo"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p322#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 322, "snippet": "9. how will you collect the required information? list all methods that you plan to use. exercise iii : developing a research instrument the construct"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p323#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 323, "snippet": "step ii formulate the questions, * preferably on a separate piece of paper, giving particular attention to their wording and order. in your own mind y"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p324#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 324, "snippet": "exercise iv : selecting a sample the accuracy of what you find through your research endeavour, in addition to many other things, depends upon the way"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p325#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 325, "snippet": "3. how will you select your sample? ( what sampling design are you proposing? ) 4. why did you select this sampling design? ( what are its strengths? "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p326#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 326, "snippet": "exercise v : developing a frame of analysis for both quantitative and qualitative studies in general terms describe the strategy you plan to use for d"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p327#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 327, "snippet": "5. how do you plan to operationalise or construct the main concepts through combining responses to different questions ( e. g. satisfaction index, eff"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p328#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 328, "snippet": "developing an outline for the structure of the report is extremely useful. as a beginner it is important that you think through carefully the contents"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p329#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 329, "snippet": "glossary 100 per cent bar chart : the 100 per cent bar chart is very similar to the stacked bar chart. the only difference is that in the former the s"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p330#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 330, "snippet": "area chart : for variables measured on an interval or a ratio scale, information about the sub - categories of a variable can also be presented in the"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p331#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 331, "snippet": "categorical variables are those where the unit of measurement is in the form of categories. on the basis of presence or absence of a characteristic, a"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p332#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 332, "snippet": "modalities. in such situations a comparative design is used. with a comparative design, as with most other designs, a study can be carried out either "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p333#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 333, "snippet": "the contents of interviews or observational field notes in order to identify the main themes that emerge from the responses given by your respondents "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p334#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 334, "snippet": "dependent or attribute and dependent, to determine if there is a relationship between them. the subcategories of both the variables are cross - tabula"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p335#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 335, "snippet": "elevation effect : some observers when using a scale to record an observation may prefer to use certain section ( s ) of the scale in the same way tha"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p336#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 336, "snippet": "it with the objectives of the study, thus providing a justification for its inclusion in the instrument, the process is called face validity. feasibil"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p337#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 337, "snippet": "influence his / her rating of that student ’ s performance in another. this type of effect is known as the halo effect. hawthorne effect : when indivi"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p338#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 338, "snippet": "intervention, programme or policy. it establishes causality between an intervention and its impact, and estimates the magnitude of this change ( s ). "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p339#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 339, "snippet": "develop intervention strategies to meet these needs, implement the interventions and then evaluate them for making informed decisions to incorporate c"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p340#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 340, "snippet": "of the pool. the two groups thus formed through the matching process are supposed to be comparable thus ensuring uniform impact of different sets of v"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p341#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 341, "snippet": "programme or a service is achieving its objectives or goals. observation is one of the methods for collecting primary data. it is a purposeful, system"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p342#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 342, "snippet": "findings more relevant to their needs. pie chart : the pie chart is another way of representing data graphically. as there are 360 degrees in a circle"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p343#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 343, "snippet": "select the sample in such a way that each element in the study population has an equal and independent chance of selection in the sample, the process "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p344#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 344, "snippet": "quota sampling : the main consideration directing quota sampling is the researcher ’ s ease of access to the sample population. in addition to conveni"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p345#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 345, "snippet": "mere expression of the attitude in response to a questionnaire or interview has caused them to think about and alter their attitude towards the mean a"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p346#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 346, "snippet": "research questions and research objectives is the way they are worded. research questions take the form of questions whereas research objectives are s"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p347#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 347, "snippet": "residents of a community, members of a group, people belonging to an organisation about whom you want to find out about through your research endeavou"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p348#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 348, "snippet": "snowball sampling is a process of selecting a sample using networks. to start with, a few individuals in a group or organisation are selected using pu"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p349#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 349, "snippet": "background, discipline, philosophy, experience and skills. bias is a deliberate attempt to change or highlight something which in reality is not there"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p350#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 350, "snippet": "trend studies : these studies involve selecting a number of data observation points in the past, together with a picture of the present or immediate p"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p351#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 351, "snippet": "bibliography ackroyd, stephen & john hughes, 1992, data collection in context, new york, longman. alkin, marvin c. & lewis c. solomon ( eds ), 1983, t"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p352#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 352, "snippet": "crotty, michael, 1998, the foundations of social research, st leonards, nsw, allen & unwin. cunningham, j. barton, 1993, action research and organizat"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p353#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 353, "snippet": "brooks / cole. longyear, marie ( eds ), 1983, the mcgraw - hill style manual, a concise guide for writers and editors, new york, mcgraw - hill. lundbe"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p354#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 354, "snippet": "simon, julian l., 1969, basic research methods in social sciences : the art of empirical investigation, new york, random house. smith, herman w., 1991"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p355#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 355, "snippet": "index 100 percent bar chart 301 – 2 abi / inform 36 accidental sampling 201 action research 131 active variables 71 after - only designs 215 alkin, m."}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p356#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 356, "snippet": "coding quantitative data 255 – 77 cohen, m. r. 63 cohen and nagel 63 cohort studies 125 column percentage 295 – 7 community forums 129, 160, 330 commu"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p357#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 357, "snippet": "editing, data 255 – 6 elevation effect 142 equal - appearing interval scale 174 eric 36 error of central tendency 142 ethics in research 241 – 8 conce"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p358#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 358, "snippet": "guba, e. g. 185 guttman scale 175 halo effect 143 harm, caused by research 245 hawthorne effect 141 healthrom 36 histogram 298 holistic research 129 h"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p359#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 359, "snippet": "katz, d. 18 kerlinger, f. n. 8, 44, 62, 82, 94, 98, 178 leading questions 155 likert scale 170 – 4 limitations, research 236 – 7 lincoln and guda 185 "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p360#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 360, "snippet": "pilot study 11 placebo effect 122 planning a research study 23 – 6 poincare 18, 73 polytomous variables 72 – 3 positive statements 172 positivist para"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p361#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 361, "snippet": "report writing bibliography 321 outline 314 – 19 referencing 320 about a variable 319 – 20 research applications 4 – 6 characteristics 8 – 9 definitio"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p362#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 362, "snippet": "frame 194 judgemental 207 non - probability 198, 206 – 8 non - random 198, 206 – 8 population 193 – 4 principles 194 – 7 probability 199 – 206 proport"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p363#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 363, "snippet": "double - control designs 119 – 20 experimental 113 – 14 longitudinal 110 matched - control designs 121 non - experimental 113 – 14 placebo design 122 "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p364#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 364, "snippet": "connecting 69 – 70 continuous 72 – 3 definition 62 – 3 dependent 66 – 70 dichotomous 72 – 3 extraneous 66 – 8 independent 66 – 70 intervening 69 – 70 "}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p365#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 365, "snippet": "research methods books from sage thing roition research discovering statistics design using spss regetieien andy field. research deskgn 7 the qualitat"}
{"id": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf#p366#c1", "source": "RM/Ranjit_Kumar-Research_Methodology_A_Step-by-Step_G(1) (1).pdf", "page": 366, "snippet": "research methods books from sage interpreting sa qualitative qualitative data - esearch & egvaluation heat — — methods ae : 2 constructing 3 grounded "}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p1#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 1, "snippet": "you said : optimization techniques in research methodology chatgpt said : chatgpt optimization techniques in research methodology refer to strategies "}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p2#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 2, "snippet": "• time management : organizing the research timeline to prioritize important tasks and allocate resources effectively, minimizing delays and inefficie"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p3#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 3, "snippet": "sequential uniform sampling is a two - parameter optimization method used to explore and optimize complex functions or systems. it is typically used i"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p4#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 4, "snippet": "4. advantages • simplicity : the method is straightforward to implement and does not require complex calculations or assumptions about the problem. • "}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p5#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 5, "snippet": "monte carlo optimization refers to a class of optimization techniques that utilize random sampling to find the best solution to an optimization proble"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p6#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 6, "snippet": "• as the temperature decreases, the algorithm becomes less likely to accept worse solutions, leading to convergence toward an optimal or near - optima"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p7#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 7, "snippet": "disadvantages of monte carlo optimization : 1. computationally expensive : monte carlo methods require a large number of random samples to converge to"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p8#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 8, "snippet": "the objective function and constraints are linear. the method iterates over the vertices of the feasible region ( the set of possible solutions define"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p9#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 9, "snippet": "maximize z = 3x1 + 2x2z = 3x1 + 2x2 subject to : x1 + x2≤42x1 + x2≤5x1, x2≥0x1 + x22x1 + x2x1, x2≤4≤5≥0 step 1 : convert into standard form add slack "}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p10#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 10, "snippet": "1. potential for cycling : in rare cases, the algorithm might cycle and revisit the same solutions without converging. however, this issue can be avoi"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p11#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 11, "snippet": "• basic idea : gradient descent is the most basic form of gradient - based optimization. it updates the parameters by moving in the direction opposite"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p12#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 12, "snippet": "• disadvantages : • noisy updates can lead to a less stable convergence process. • more iterations are required to converge to the optimum. 3. mini - "}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p13#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 13, "snippet": "• can still struggle with very complex, highly non - convex objective functions. 5. adagrad ( adaptive gradient algorithm ) : • basic idea : adagrad a"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p14#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 14, "snippet": "2. convergence : gradient - based methods can converge quickly to an optimal solution if the learning rate is appropriately set and the problem is wel"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p15#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 15, "snippet": "• gradient descent ( and its variants like stochastic gradient descent, momentum, and adam ) can be applied to multi - parameter optimization problems"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p16#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 16, "snippet": "where : • xikxik is the position of particle ii at iteration kk, • vikvik is the velocity, • pikpik is the best - known position of particle ii, • gkg"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p17#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 17, "snippet": "• multi - parameter optimization : for multi - parameter optimization, bayesian optimization is particularly useful when the objective function is exp"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p18#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 18, "snippet": "• does not require gradient information, making it useful for non - differentiable or noisy functions. • relatively simple to implement. • disadvantag"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p19#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 19, "snippet": "c ( x ) = = 1n ( yi−y ^ i ) 2c ( x ) = n1i = ( yi−y ^ i ) 2 where yiyi are the true values and y ^ iy ^ i are the predicted values. • engineering : in"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p20#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 20, "snippet": "3. non - linear cost function : many real - world problems involve non - linear cost functions, where the relationship between the variables is not a "}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p21#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 21, "snippet": "the goal of the optimization algorithm ( e. g., gradient descent ) is to find the model parameters that minimize this cost function. summary : • the c"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p22#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 22, "snippet": "• objective : the goal was to optimize the ad auction system to balance the bidding prices, ad relevance, and the user experience. advertisers bid for"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p23#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 23, "snippet": "• optimization tool : deepmind ai for energy management • objective : google partnered with deepmind to reduce the energy consumption in its data cent"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p24#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 24, "snippet": "• optimization tool : machine learning and time series forecasting • objective : the goal was to predict future flight prices with high accuracy, allo"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p25#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 25, "snippet": "design process involved multiple engineering disciplines, including aerodynamics, structural analysis, and materials science, each of which required s"}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p26#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 26, "snippet": "• objective function : the overall objective was to minimize the total drag while maximizing lift and ensuring the wing ’ s weight remained as low as "}
{"id": "RM/RM Unit 4 (Chatgpt).pdf#p27#c1", "source": "RM/RM Unit 4 (Chatgpt).pdf", "page": 27, "snippet": "• performance improvements : the optimized wing design led to significant improvements in aerodynamic efficiency, reducing drag and fuel consumption. "}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p1#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 1, "snippet": "research methodology ( class : mtech )"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p2#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 2, "snippet": "# ›"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p3#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 3, "snippet": "ethics ethics ( also moral philosophy ) is the branch of philosophy that involves systematic, defending, and recommending concepts of right and wrong "}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p4#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 4, "snippet": "the following is a rough and general summary of some ethical principals that various codes address : honesty strive for honesty in all scientific comm"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p5#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 5, "snippet": "carefulness avoid careless errors and negligence ; carefully and critically examine your own work and the work of your peers. keep good records of res"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p6#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 6, "snippet": "confidentiality protect confidential communications, such as papers or grants submitted for publication, personnel records, trade or military secrets,"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p7#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 7, "snippet": "respect for colleagues respect your colleagues and treat them fairly. social responsibility strive to promote socially good and prevent or mitigate so"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p8#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 8, "snippet": "three codes of ethics acm codes of ethics and professional conduct : http : / / www. acm. org / about / code - of - ethics acm / ieee - cs software en"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p9#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 9, "snippet": "case study 1 kylie turville owns her own consulting business, and has several people working for her. kylie is currently designing a database manageme"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p10#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 10, "snippet": "case study 1 : relevant clauses principle 1. public software engineers shall act consistently with the public interest. in particular, software engine"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p11#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 11, "snippet": "principle 2. client and employer software engineers shall act in a manner that is in the best interests of their client and employer, consistent with "}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p12#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 12, "snippet": "case study 1 : relevant clauses principle 3. product software engineers shall ensure that their products and related modifications meet the highest pr"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p13#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 13, "snippet": "case study 2 george smith is an experienced software developer working for acme software company. mr. smith is now working on a project for the u. s. "}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p14#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 14, "snippet": "plagiarism plagiarism is taking another ’ s work and passing it off as your own. in a broad sense we are all guilty of plagiarism many times each day."}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p15#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 15, "snippet": "plagiarism in research is usually quite different true plagiarism is, quite bluntly, stealing. sometimes a person just copies text word for word from "}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p16#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 16, "snippet": "plagiarism extends to more than words one should always provide references for any statistics graphs tables numbers, etc. that one wishes to use in on"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p17#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 17, "snippet": "copy direct quotations make sure that you write it precisely, word - for - word as in the original. also essential that you enclose the quoted text in"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p18#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 18, "snippet": "paraphrasing is often preferable to taking direct quotations it ’ s often preferable to take down the substance of an author ’ s idea in your own word"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p19#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 19, "snippet": "“ plagiarism ” sometimes not intentional many instances of plagiarism stem from sloppy research rather than through a deliberate desire to cheat. many"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p20#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 20, "snippet": "“ common knowledge ” and plagiarism it is not necessary to document every single statement. one need not give a reference for stating that president j"}
{"id": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx#p21#c1", "source": "RM/Unit 1 -IEEE-ACM Code of Ethics (1).pptx", "page": 21, "snippet": "thank you # ›"}
{"id": "DGM/DGM Unit 3 (chatgpt).pdf#p1#c1", "source": "DGM/DGM Unit 3 (chatgpt).pdf", "page": 1, "snippet": "you said : what are sequences of unequallength in recurrent neural networks 2 / 2 chatgpt said : chatgpt in the context of recurrent neural networks ("}
{"id": "DGM/DGM Unit 3 (chatgpt).pdf#p2#c1", "source": "DGM/DGM Unit 3 (chatgpt).pdf", "page": 2, "snippet": "• sequences are packed into a compact representation using pack _ padded _ sequence, which efficiently skips padded values during computation. • after"}
{"id": "DGM/DGM Unit 3 (chatgpt).pdf#p3#c1", "source": "DGM/DGM Unit 3 (chatgpt).pdf", "page": 3, "snippet": "• mathematical formulation : • at time tt, given an input xtxt : ht = f ( wxhxt + whhht−1 + bh ) ht = f ( wxhxt + whhht−1 + bh ) where : • htht : hidd"}
{"id": "DGM/DGM Unit 3 (chatgpt).pdf#p4#c1", "source": "DGM/DGM Unit 3 (chatgpt).pdf", "page": 4, "snippet": "2. gradient clipping • prevent exploding gradients by capping them to a predefined threshold. 3. regularization • techniques like dropout, weight deca"}
{"id": "DGM/DGM Unit 3 (chatgpt).pdf#p5#c1", "source": "DGM/DGM Unit 3 (chatgpt).pdf", "page": 5, "snippet": ", adding feedback loops and unfolding chatgpt said : chatgpt adding feedback loops and unfolding in recurrent neural networks recurrent neural network"}
{"id": "DGM/DGM Unit 3 (chatgpt).pdf#p6#c1", "source": "DGM/DGM Unit 3 (chatgpt).pdf", "page": 6, "snippet": "2. each unrolled layer corresponds to one time step of the sequence, with shared weights across all layers. 3. the unfolded network allows standard gr"}
{"id": "DGM/DGM Unit 3 (chatgpt).pdf#p7#c1", "source": "DGM/DGM Unit 3 (chatgpt).pdf", "page": 7, "snippet": "3. shared weights : ensure the weights across the unfolded network are shared, maintaining the temporal consistency of the model. by incorporating fee"}
{"id": "DGM/DGM Unit 3 (chatgpt).pdf#p8#c1", "source": "DGM/DGM Unit 3 (chatgpt).pdf", "page": 8, "snippet": "2. normalize input ( for numerical data ) : • scale time series or other numerical data to a standard range ( e. g., 0 - 1 ). 3. handle unequal length"}
{"id": "DGM/DGM Unit 3 (chatgpt).pdf#p9#c1", "source": "DGM/DGM Unit 3 (chatgpt).pdf", "page": 9, "snippet": "import torch. optim as optim criterion = nn. crossentropyloss ( ) optimizer = optim. adam ( model. parameters ( ) ) # training loop for epoch in range"}
{"id": "DGM/DGM Unit 3 (chatgpt).pdf#p10#c1", "source": "DGM/DGM Unit 3 (chatgpt).pdf", "page": 10, "snippet": "• useful for tasks like translation or summarization. 5. practical considerations • batch processing : handle sequences of varying lengths using paddi"}
{"id": "DGM/DGM Unit 3 (chatgpt).pdf#p11#c1", "source": "DGM/DGM Unit 3 (chatgpt).pdf", "page": 11, "snippet": "step 1 : preprocess the data python copy code from tensorflow. keras. preprocessing. text import tokenizer from tensorflow. keras. utils import pad _ "}
{"id": "DGM/DGM Unit 3 (chatgpt).pdf#p12#c1", "source": "DGM/DGM Unit 3 (chatgpt).pdf", "page": 12, "snippet": "4. results • training accuracy / loss : check the model ' s accuracy and loss over epochs. • generated text example : starting with the seed \" the qui"}
{"id": "DGM/DGM Unit 3 (chatgpt).pdf#p13#c1", "source": "DGM/DGM Unit 3 (chatgpt).pdf", "page": 13, "snippet": "gated recurrent units ( grus ) are a type of recurrent neural network ( rnn ) that address the vanishing gradient problem and efficiently capture long"}
{"id": "DGM/DGM Unit 3 (chatgpt).pdf#p14#c1", "source": "DGM/DGM Unit 3 (chatgpt).pdf", "page": 14, "snippet": "• faster training and inference. 2. effective for long sequences : • manages long - term dependencies well with its gating mechanisms. 3. lower comput"}
{"id": "DGM/DGM Unit 3 (chatgpt).pdf#p15#c1", "source": "DGM/DGM Unit 3 (chatgpt).pdf", "page": 15, "snippet": "3. speech processing : • speech recognition and text - to - speech systems. 4. anomaly detection : • detect unusual patterns in sequential data, such "}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p1#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 1, "snippet": "convolutional neural architecture chatgpt said : chatgpt convolutional neural architectures ( cnns ) are a class of deep neural networks primarily use"}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p2#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 2, "snippet": "applications • image classification : assigning labels to images ( e. g., identifying objects ). • object detection : locating objects within images ("}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p3#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 3, "snippet": "o each filter learns to detect a specific feature, such as edges, corners, or textures. 3. feature maps : o each filter generates its own feature map,"}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p4#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 4, "snippet": "4o mini you said : feature maps chatgpt said : chatgpt feature maps are a crucial concept in convolutional neural networks ( cnns ), representing the "}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p5#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 5, "snippet": "let ’ s illustrate with a simple example. consider a 3×33 \\ times 33×3 filter applied to a 5×55 \\ times 55×5 input image with no padding and a stride "}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p6#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 6, "snippet": "insights into how cnns interpret input data. if you ’ d like to explore specific examples or applications of feature maps further, let me know! 4o min"}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p7#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 7, "snippet": "o similar to global average pooling, but it takes the maximum value from the entire feature map instead of the average. benefits of pooling • dimensio"}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p8#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 8, "snippet": "chatgpt batch normalization is a technique used in training deep neural networks to improve convergence speed, stabilize learning, and enhance overall"}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p9#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 9, "snippet": "batch normalization is typically applied after the activation function of a layer, but it can also be placed before it. in many deep learning framewor"}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p10#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 10, "snippet": "o these layers reduce the spatial dimensions of the feature maps, retaining important information while decreasing computational complexity. common po"}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p11#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 11, "snippet": "5. output layer • layer 10 : final convolution ( e. g., 1×11 \\ times 11×1 ) to produce output with the same spatial dimensions as the input, with soft"}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p12#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 12, "snippet": "o input size : 32×3232 \\ times 3232×32 grayscale images ( the mnist digits are typically 28×2828 \\ times 2828×28 but are zero - padded to 32×3232 \\ ti"}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p13#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 13, "snippet": "• optimization : lenet was trained using stochastic gradient descent ( sgd ) with backpropagation to minimize the cross - entropy loss function. • lea"}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p14#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 14, "snippet": "alexnet consists of eight layers : five convolutional layers followed by three fully connected layers. here ’ s a breakdown of its architecture : 1. i"}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p15#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 15, "snippet": "o convolutional filters : 256256256 filters of size 3×33 \\ times 33×3, with padding of 111. o activation function : relu. o output size : 13×13×25613 "}
{"id": "DGM/DGM Unit 2 Chatgpt.pdf#p16#c1", "source": "DGM/DGM Unit 2 Chatgpt.pdf", "page": 16, "snippet": "• top - 5 accuracy : alexnet achieved a top - 5 error rate of 15. 3 % 15. 3 \\ % 15. 3 % on the imagenet test set, significantly outperforming the prev"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p1#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 1, "snippet": "you said : deep learning : introduction chatgpt said : chatgpt deep learning : introduction deep learning is a subset of machine learning, which itsel"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p2#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 2, "snippet": "• relu ( rectified linear unit ) : most commonly used. it outputs zero for negative values and the input value itself for positive ones. • sigmoid : s"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p3#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 3, "snippet": "deep learning has become one of the most powerful tools in ai, enabling breakthroughs in fields such as computer vision, language understanding, and m"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p4#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 4, "snippet": "deep learning has a significant impact on healthcare by enhancing diagnostic accuracy and enabling personalized treatment. • disease detection : ai mo"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p5#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 5, "snippet": "generative models create new data that mimics the distribution of training data, leading to applications in several creative fields. • generative adve"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p6#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 6, "snippet": "• climate modeling : ai is used to simulate climate change models, predicting environmental changes more accurately. • wildlife monitoring : ai system"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p7#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 7, "snippet": "• compute the error between the predicted output and the actual ground truth using a loss function ( e. g., cross - entropy for classification, mean s"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p9#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 9, "snippet": "here is the diagram illustrating the workflow of training and evaluating a deep learning model. it visually represents the steps involved, from data c"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p10#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 10, "snippet": "• in each training iteration, a subset of neurons is temporarily removed from the network, and the model must adapt to different neuron configurations"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p11#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 11, "snippet": "• adding random noise to the input data or to the hidden layers during training can act as a regularizer, forcing the model to learn more robust featu"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p12#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 12, "snippet": "2. effect on training : • with different neurons deactivated at each iteration, the network has to rely on a wider set of features for decision - maki"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p13#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 13, "snippet": "copy code from tensorflow. keras. models import sequential from tensorflow. keras. layers import dense, dropout model = sequential ( [ dense ( 128, ac"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p14#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 14, "snippet": "key types of activation functions : 1. sigmoid function • the sigmoid function is an s - shaped curve that maps input values to a range between 0 and "}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p15#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 15, "snippet": "• the relu function is the most widely used activation function in deep learning because of its simplicity and effectiveness. • formula : f ( x ) = ma"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p16#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 16, "snippet": "• output range : ( - ∞, ∞ ) • use cases : • applied in deep networks where better flexibility is needed during training to handle negative inputs. • a"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p17#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 17, "snippet": "• disadvantages : • slightly more computationally expensive than relu. choosing the right activation function : • hidden layers : • relu and its varia"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p18#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 18, "snippet": "• overshooting : if the learning rate is too high, the updates to the model ' s weights are too large, causing the model to overshoot the minimum of t"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p19#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 19, "snippet": "• rmsprop : similar to adam, it scales the learning rate based on the moving average of the squared gradients. • adagrad : adjusts the learning rate f"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p20#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 20, "snippet": "1. vanishing gradient problem in the vanishing gradient problem, the gradients ( partial derivatives of the loss function with respect to the network "}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p21#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 21, "snippet": "in the exploding gradient problem, the gradients become excessively large as they are propagated back through the layers of the network. this leads to"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p22#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 22, "snippet": "the gradients grow exponentially as they are propagated backward, leading to instability in the weight updates. rust copy code forward pass : input - "}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p23#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 23, "snippet": "2. batch size : the number of training examples used in one forward and backward pass. 3. number of layers / neurons : determines the architecture of "}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p24#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 24, "snippet": "• cons : • may miss the best combination since it doesn ’ t explore every option. • example : python copy code from sklearn. model _ selection import "}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p25#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 25, "snippet": "• example tools : hypergrad 5. evolutionary algorithms ( genetic algorithms ) • how it works : these algorithms use concepts from natural selection to"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p26#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 26, "snippet": "• when validation performance plateaus. 3. cross - validation • cross - validation can be used during hyperparameter tuning to avoid overfitting on a "}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p27#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 27, "snippet": "2. select a framework / library • popular deep learning libraries include : • tensorflow / keras ( high - level api with ease of use ) • pytorch ( mor"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p28#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 28, "snippet": "• the compilation step specifies the optimizer, the loss function, and the metrics to monitor during training. • example for binary classification : p"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p29#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 29, "snippet": "save the model : python copy code model. save ( ' my _ model. h5 ' ) load the model : python copy code from tensorflow. keras. models import load _ mo"}
{"id": "DGM/DGM Unit 1 (chatgpt).pdf#p30#c1", "source": "DGM/DGM Unit 1 (chatgpt).pdf", "page": 30, "snippet": "4. hyperparameter tuning : experiment with learning rates, batch sizes, optimizer choices, and network architectures to find the best performing model"}
{"id": "DGM/unit 4 DGM (chatgpt).pdf#p1#c1", "source": "DGM/unit 4 DGM (chatgpt).pdf", "page": 1, "snippet": "you said : autoencoder chatgpt said : chatgpt an autoencoder is a type of artificial neural network used to learn efficient data representations in an"}
{"id": "DGM/unit 4 DGM (chatgpt).pdf#p2#c1", "source": "DGM/unit 4 DGM (chatgpt).pdf", "page": 2, "snippet": "4. convolutional autoencoder : • uses convolutional layers instead of fully connected layers, ideal for image data. 5. contractive autoencoder : • add"}
{"id": "DGM/unit 4 DGM (chatgpt).pdf#p3#c1", "source": "DGM/unit 4 DGM (chatgpt).pdf", "page": 3, "snippet": "how vaes work 1. encoder : • maps input xx to two parameters : • μ ( x ) μ ( x ) : mean of the latent distribution. • σ ( x ) σ ( x ) : standard devia"}
{"id": "DGM/unit 4 DGM (chatgpt).pdf#p4#c1", "source": "DGM/unit 4 DGM (chatgpt).pdf", "page": 4, "snippet": "5. image synthesis : • create variations of existing images ( e. g., generating faces with different expressions ). advantages of vaes • probabilistic"}
{"id": "DGM/unit 4 DGM (chatgpt).pdf#p5#c1", "source": "DGM/unit 4 DGM (chatgpt).pdf", "page": 5, "snippet": "• uses convolutional layers rather than recurrent networks for modeling dependencies, making it computationally efficient. • ensures that each pixel i"}
{"id": "DGM/unit 4 DGM (chatgpt).pdf#p6#c1", "source": "DGM/unit 4 DGM (chatgpt).pdf", "page": 6, "snippet": "• useful for class - conditional image generation. pixelcnn architecture 1. input layer : • processes the image with masked convolutional filters to r"}
{"id": "DGM/unit 4 DGM (chatgpt).pdf#p7#c1", "source": "DGM/unit 4 DGM (chatgpt).pdf", "page": 7, "snippet": "• images are generated pixel by pixel, making the process computationally expensive. 2. local context dependency : • limited to capturing dependencies"}
{"id": "DGM/unit 4 DGM (chatgpt).pdf#p8#c1", "source": "DGM/unit 4 DGM (chatgpt).pdf", "page": 8, "snippet": "calculated by chaining the transformations and summing the log - determinants : logp ( x ) = logp ( zk ) + = ( x ) = logp ( zk ) + k = types of transf"}
{"id": "DGM/unit 4 DGM (chatgpt).pdf#p9#c1", "source": "DGM/unit 4 DGM (chatgpt).pdf", "page": 9, "snippet": "• efficient for density estimation but slower for sampling. 4. neural spline flows : • introduces invertible splines for more flexible transformations"}
{"id": "DGM/unit 4 DGM (chatgpt).pdf#p10#c1", "source": "DGM/unit 4 DGM (chatgpt).pdf", "page": 10, "snippet": "2. computational cost : • computing jacobians and determinants for large transformations can be resource - intensive. 3. expressiveness vs. efficiency"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p1#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 1, "snippet": "undergraduate topics in computer science sandro skansi introduction to deep learning from logical calculus to artificial intelligence"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p2#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 2, "snippet": "undergraduate topics in computer science series editor ian mackie advisory editors samson abramsky, university of oxford, oxford, uk chris hankin, imp"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p3#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 3, "snippet": "undergraduate topics in computer science ( utics ) delivers high - quality instruc - tional content for undergraduates studying in all areas of comput"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p4#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 4, "snippet": "sandro skansi introduction to deep learning from logical calculus to artificial intelligence 123"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p5#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 5, "snippet": "sandro skansi university of zagreb zagreb croatia issn 1863 - 7310 issn 2197 - 1781 ( electronic ) undergraduate topics in computer science isbn 978 -"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p6#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 6, "snippet": "preface this textbook contains no new scienti ﬁc results, and my only contribution was to compile existing knowledge and explain it with my examples a"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p7#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 7, "snippet": "there is an old idea from kendo 1 which seems to ﬁnd its way to the new world of cutting - edge technology. the idea is that you learn a martial art i"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p8#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 8, "snippet": "artiﬁcial intelligence as a discipline can be considered to be a sort of ‘ philo - sophical engineering ’. what i mean by this is that ai is a process"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p9#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 9, "snippet": "engineering side ). this novelty can be better results than the last result on that problem, 7 the formulation of a new problem 8 or results below the"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p10#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 10, "snippet": "references 1. i. goodfellow, y. bengio, a. courville, deep learning ( mit press, cambridge, 2016 ) 2. a. gulli, s. pal, deep learning with keras ( pac"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p11#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 11, "snippet": "contents 1 from logic to cognitive science............................ 1 1. 1 the beginnings of arti ﬁcial neural networks.............. 1 1. 2 the xo"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p12#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 12, "snippet": "4. 5 from the logistic neuron to backpropagation.............. 89 4. 6 backpropagation..................................... 93 4. 7 a complete feedfor"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p13#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 13, "snippet": "9. 4 walking through the word - space : an idea that has eluded symbolic ai.............................. 171 references.............................."}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p14#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 14, "snippet": "1from logic to cognitive science 1. 1 the beginnings of artiﬁcial neural networks artiﬁcial intelligence has it roots in two philosophical ideas of go"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p15#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 15, "snippet": "2 1 from logic to cognitive science called logical psychologism, is still researched only in philosophical logic, 1 but even in philosophical logic it"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p16#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 16, "snippet": "1. 1 the beginnings of artiﬁcial neural networks 3 logical rules. but there was another direction which is characteristic of philosophical logic : cou"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p17#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 17, "snippet": "4 1 from logic to cognitive science walter pitts was an interesting person, and, one could argue, the father of artiﬁcial neural networks. at the age "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p18#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 18, "snippet": "1. 1 the beginnings of artiﬁcial neural networks 5 course taught by professor wilfrid rall ( the pioneer of computational neuroscience ), and rall rem"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p19#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 19, "snippet": "6 1 from logic to cognitive science by the air force ofﬁce of scientiﬁc research ) which implemented neural networks called snarc ( stochastic neural "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p20#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 20, "snippet": "1. 2 the xor problem 7 funding was abundant, but many technically inclined researchers underestimated the linguistic complexities involved in extracti"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p21#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 21, "snippet": "8 1 from logic to cognitive science 1. 3 from cognitive science to deep learning but the idea of neural networks lingered on in the minds of only a ha"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p22#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 22, "snippet": "1. 3 from cognitive science to deep learning 9 in history, and its scope was roughly deﬁned. it was christopher longuet - higgins, fellow of the royal"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p23#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 23, "snippet": "10 1 from logic to cognitive science to avoid problems. 16 after graduating ( 1978 ), he came to san diego as a visiting scholar to the cognitive scie"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p24#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 24, "snippet": "1. 3 from cognitive science to deep learning 11 for an exhaustive treatment of the history of neural networks, we point the reader to the paper by jur"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p25#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 25, "snippet": "12 1 from logic to cognitive science fig. 1. 1 v ertical and horizontal components of ai because it misses an important aspect. recall the good old - "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p26#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 26, "snippet": "1. 3 from cognitive science to deep learning 13 imitation of any mental process taking place in the human cortex. philosophy also wants to abstract aw"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p27#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 27, "snippet": "14 1 from logic to cognitive science the paper that exposed this major philosophical issue in artiﬁcial neural networks and connectionism, is the semi"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p28#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 28, "snippet": "1. 5 philosophical and cognitive aspects 15 models. this is perhaps surprising since in the normal cognitive setting ( undoubtedly under the inﬂuence "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p29#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 29, "snippet": "16 1 from logic to cognitive science 26. d. b. parker, learning - logic. technical report no. 47 ( mit center for computational research in economics "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p30#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 30, "snippet": "2mathematical and computational prerequisites 2. 1 derivations and function minimization in this chapter, we give most of the mathematical preliminari"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p31#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 31, "snippet": "18 2 mathematical and computational prerequisites a set does not remember the order of elements or repetitions of one element. if we have a set that r"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p32#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 32, "snippet": "2. 1 derivations and function minimization 19 multiplication and exponentiation. they in turn can be expressed from simpler func - tions, but we will "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p33#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 33, "snippet": "20 2 mathematical and computational prerequisites a function f is called monotone if for every x and y from the domain ( for which the function is deﬁ"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p34#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 34, "snippet": "2. 1 derivations and function minimization 21 • the logarithmic function has the properties : log c 1 = 0, log c c = 1, log c ( xy ) = logc x + logc y"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p35#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 35, "snippet": "22 2 mathematical and computational prerequisites polynomial functions. rational functions 9 are continuous everywhere except where the value of the d"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p36#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 36, "snippet": "2. 1 derivations and function minimization 23 i. e. that x1 < x2 and y1 < y2. the slope is then equal to y2−y1 x2−x1, which is the ratio of the vertic"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p37#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 37, "snippet": "24 2 mathematical and computational prerequisites on calculus. one of the most basic things regarding derivatives is that the derivative of a constant"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p38#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 38, "snippet": "2. 1 derivations and function minimization 25 • const : constant rule c ′ = 0. • chainexp : chain rule for exponents [ ef ( x ) ] ′ = ef ( x ) · f ′ ("}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p39#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 39, "snippet": "26 2 mathematical and computational prerequisites question is a bit vague but the answer is yes. if we take three 16 vectors e1 = ( 1, 0, 0 ), e2 = ( "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p40#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 40, "snippet": "2. 2 vectors, matrices and linear programming 27 which are a natural extension of vectors. a matrix is a structure similar to a table as it is made by"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p41#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 41, "snippet": "28 2 mathematical and computational prerequisites row vector and vice versa. transposition is used a lot in deep learning to keep all operations runni"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p42#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 42, "snippet": "2. 2 vectors, matrices and linear programming 29 element comes from the k - th row of a and the m - th column of b. an example will make it clear : ab"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p43#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 43, "snippet": "30 2 mathematical and computational prerequisites • c23 = 4 · 0 + 5 · 3 + 6 · 6 + 7 · 9 = 114 before continuing, we must deﬁne two more classes of mat"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p44#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 44, "snippet": "2. 2 vectors, matrices and linear programming 31 of f ( x, y ). in symbols, we calculated dfy ( x ) dx, and the corresponding partial derivative is de"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p45#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 45, "snippet": "32 2 mathematical and computational prerequisites we will be making a series of steps towards the x which will produce a minimal f ( x ) ( or more pre"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p46#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 46, "snippet": "2. 3 probability distributions 33 weight, education, foot size, interests, etc. statistics then analyses the population ’ s properties, such as for ex"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p47#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 47, "snippet": "34 2 mathematical and computational prerequisites that we need an odd number of elements in the sequence, but we can easily modify the median a bit to"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p48#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 48, "snippet": "2. 3 probability distributions 35 desired outcome will occur and dividing it by the number of all possible outcomes. please note one interesting thing"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p49#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 49, "snippet": "36 2 mathematical and computational prerequisites but let us see what is happening in the background when we talk about the expected value. we are act"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p50#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 50, "snippet": "2. 3 probability distributions 37 finally, we can deﬁne the conditional probability of two events. the conditional probability of a given b ( or in lo"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p51#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 51, "snippet": "38 2 mathematical and computational prerequisites it is quite a weird equation, but the main thing about the gaussian distribution is not the elegance"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p52#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 52, "snippet": "2. 4 logic and turing machines 39 2. 4 logic and turing machines we have already encountered logic in the very beginnings of artiﬁcial neural net - wo"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p53#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 53, "snippet": "40 2 mathematical and computational prerequisites we can also a quick look at fuzzy ﬁrst - order logic. here, we have a predicate p ( suppose p ( x ) "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p54#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 54, "snippet": "2. 4 logic and turing machines 41 line and close one, closing the other will not produce a 1 ). this is a strong case for intuitionistic logic where t"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p55#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 55, "snippet": "42 2 mathematical and computational prerequisites the command prompt is opened. if you close it, or restart your computer, you must type again activat"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p56#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 56, "snippet": "2. 6 a brief overview of python programming 43 2. 6 a brief overview of python programming in the last section, we have discussed installation of pyth"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p57#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 57, "snippet": "44 2 mathematical and computational prerequisites the second line begins with four. they denote whitespace ( the character which the space bar puts in"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p58#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 58, "snippet": "2. 6 a brief overview of python programming 45 indexing iterables with ints from 0 onwards, and this means that to get the ﬁrst element of the iterabl"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p59#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 59, "snippet": "46 2 mathematical and computational prerequisites as the elements in a list. y ou can put anything as a value, but there are restric - tions on what c"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p60#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 60, "snippet": "2. 6 a brief overview of python programming 47 if condition = = 1 : return 1 elif condition = = 0 : print ( \" invalid input \" ) else : print ( \" error"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p61#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 61, "snippet": "48 2 mathematical and computational prerequisites them out. the last non - indented line of the code will simply show you the last ( and current ) val"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p62#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 62, "snippet": "2. 6 a brief overview of python programming 49 y ou can additionally specify a path to the ﬁle, so you can write skansi / desktop / myfile. json. if y"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p63#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 63, "snippet": "3machine learning basics machine learning is a subﬁeld of artiﬁcial intelligence and cognitive science. in arti - ﬁcial intelligence, it is divided in"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p64#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 64, "snippet": "52 3 machine learning basics make a portfolio of penny stocks, we can reformulate it to be a classiﬁcation problem of the form : ‘ winner! will rise 4"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p65#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 65, "snippet": "3. 1 elementary classiﬁcation problem 53 x and o in fig. 3. 1a ). we will never be able to distinguish them, and if a new animal were to have this len"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p66#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 66, "snippet": "54 3 machine learning basics fig. 3. 2 different hyperplanes useful separation. hyperplane e is even worse than hyperplane a, but to deﬁne it we just "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p67#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 67, "snippet": "3. 1 elementary classiﬁcation problem 55 fig. 3. 3 feature engineering actually separate the two classes by a simple straight plane in 3d. when it is "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p68#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 68, "snippet": "56 3 machine learning basics length weight colour label 34 7 black dog 59 15 white dog 54 17 brown dog 78 28 white dog … … … … and convert it so that "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p69#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 69, "snippet": "3. 2 evaluating classiﬁcation results 57 3. 2 evaluating classiﬁcation results in the previous section, we have explored the basics of classiﬁcation a"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p70#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 70, "snippet": "58 3 machine learning basics number of true positives, added to the number of true negatives and divided by the total number of datapoints. in our cas"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p71#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 71, "snippet": "3. 2 evaluating classiﬁcation results 59 called splitting the dataset in training and testing sets or simply the train – test split. the test set is d"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p72#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 72, "snippet": "60 3 machine learning basics time buy morning no afternoon yes evening yes morning yes morning yes afternoon yes evening no evening yes morning no aft"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p73#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 73, "snippet": "3. 3 a simple classiﬁer : naive bayes 61 we also know that p ( no | morning ) = 1 − p ( yes | morning ) = 0. 4. this means that the datapoint gets the"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p74#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 74, "snippet": "62 3 machine learning basics fig. 3. 5 schematic view of logistic regression logistic regression was ﬁrst introduced in 1958 by d. r. cox [ 6 ], and a"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p75#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 75, "snippet": "3. 4 a simple neural network : logistic regression 63 if we join them and tidy up a bit, we have simply y = σ ( b + w1x1 + w2x2 + w3x3 ) now, let us c"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p76#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 76, "snippet": "64 3 machine learning basics the weights so we can forget about the bias knowing it will be taken care of and it will become one of the weights. let u"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p77#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 77, "snippet": "3. 4 a simple neural network : logistic regression 65 do we need such weird notation now and a bit later how to dispense with it. let us calculate our"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p78#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 78, "snippet": "66 3 machine learning basics to represent the whole training set. this is important in the computational sense as well since most deep learning librar"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p79#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 79, "snippet": "3. 4 a simple neural network : logistic regression 67 this matrix can be equivalently represented as ( 0. 66, 0. 1, 0. 35, 0. 7 ), but we will use the"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p80#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 80, "snippet": "68 3 machine learning basics fig. 3. 7 a single mnist datapoint 3. 5 introducing the mnist dataset the mnist dataset is a modiﬁcation of the national "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p81#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 81, "snippet": "3. 5 introducing the mnist dataset 69 fig. 3. 8 greyscale for all colours, red channel, green channel and blue channel component ‘ images ’ called cha"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p82#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 82, "snippet": "70 3 machine learning basics 3. 6 learning without labels : k - means we now turn our attention to two algorithms for unsupervised learning, the k - m"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p83#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 83, "snippet": "3. 6 learning without labels : k - means 71 fig. 3. 9 two complete cycles of k - means with two centroids associating all datapoints from centroids. c"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p84#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 84, "snippet": "72 3 machine learning basics here, d ( i, j ) is the euclidean distance between centroids i and j and din ( c ) is the intra - cluster distance which "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p85#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 85, "snippet": "3. 7 learning different representations : pca 73 the details in chap. 9. 32 pca has the following form : z = xq, ( 3. 17 ) where x is the input matrix"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p86#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 86, "snippet": "74 3 machine learning basics fig. 3. 10 v ariance under rotation of the coordinate system fig. 3. 10, we have compared the black ( original coordinate"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p87#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 87, "snippet": "3. 8 learning language : the bag of words representation 75 3. 8 learning language : the bag of words representation so far we have addressed numerica"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p88#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 88, "snippet": "76 3 machine learning basics user you dont know as if i what likes s. a 1 1 1 0 0 0 0 22 f. f 1 0 1 1 1 0 0 13 s. a 0 0 2 0 0 2 1 9 p. h 0 0 1 0 0 1 0"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p89#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 89, "snippet": "references 77 references 1. r. tibshirani, t. hastie, the elements of statistical learning : data mining, inference, and prediction, 2nd edn. ( spring"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p90#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 90, "snippet": "4feedforward neural networks 4. 1 basic concepts and t erminology for neural networks backpropagation is the core method of learning for deep learning"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p91#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 91, "snippet": "80 4 feedforward neural networks directed, and this is its output ( this is also a neuron ). the same holds for a simple neural network, but it can ha"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p92#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 92, "snippet": "4. 1 basic concepts and terminology for neural networks 81 layer can take a single output. it is possible to have less input values than input neurons"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p93#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 93, "snippet": "82 4 feedforward neural networks w23 21 y22 + w23 31 y23 + w23 41 y24. the same is done for z32, and then by applying the chosen nonlinearity to z31 a"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p94#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 94, "snippet": "4. 2 representing network components with vectors and matrices 83 second neuron in layer 1 and the third neuron in layer 2 with a variable named w23. "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p95#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 95, "snippet": "84 4 feedforward neural networks 4. 3 the perceptron rule as we noted before, the learning process in the neurons is simply the modiﬁcation or update "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p96#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 96, "snippet": "4. 3 the perceptron rule 85 the perceptron is trained as follows ( this is the perceptron learning rule 3 ) : 1. choose a training case. 2. if the pre"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p97#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 97, "snippet": "86 4 feedforward neural networks the other ) but numerically they are not. if the representation given to an algorithm is in pixels, and it can be rot"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p98#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 98, "snippet": "4. 3 the perceptron rule 87 the inequality ( d ) is derived in a similar fashion to ( c ). by adding ( a ) and ( b ) we get w1 + w2 ≥ 2b, and by addin"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p99#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 99, "snippet": "88 4 feedforward neural networks estimations so that you are off by less and less as the meals pass, and hopefully, this will lead you to a good appro"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p100#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 100, "snippet": "4. 4 the delta rule 89 where wi is a weight, xi is the input and t − y is the residual error. the η is called the learning rate. its default value sho"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p101#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 101, "snippet": "90 4 feedforward neural networks to absolute values ( albeit larger in magnitude, but this is not a problem, since we want to use it in relative, not "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p102#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 102, "snippet": "4. 5 from the logistic neuron to backpropagation 91 part ∂z ∂wi which is equal to xi since z = i wixi ( we absorbed the bias ). by the same argument ∂"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p103#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 103, "snippet": "92 4 feedforward neural networks which by derdifvar becomes − −1 · e−z ( 1 + e−z ) 2 we tidy up the signs and get e−z ( 1 + e−z ) 2 therefore, dy dz ="}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p104#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 104, "snippet": "4. 5 from the logistic neuron to backpropagation 93 by applying ld we get 1 2 de dy ( t − y ) 2 by applying exp we get 1 2 · 2 · ( t − y ) · de dy ( t"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p105#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 105, "snippet": "94 4 feedforward neural networks needed. backpropagation of errors is basically just gradient descent. mathematically speaking, backpropagation is : w"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p106#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 106, "snippet": "4. 6 backpropagation 95 if we formalize this approach we will get a method called ﬁnite difference approx - imation16 : 1. each weight wi, 1 ≤ i ≤ k i"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p107#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 107, "snippet": "96 4 feedforward neural networks as we have e = 1 2 o∈output ( to − yo ) 2 the ﬁrst thing we need to do is turn the difference between the output and "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p108#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 108, "snippet": "4. 6 backpropagation 97 ∂e ∂who = ∂zo ∂who ∂e ∂zo = yi ∂e ∂zj the rule for updating weights is quite straightforward, and we call it the general weigh"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p109#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 109, "snippet": "98 4 feedforward neural networks fig. 4. 5 backpropagation in a complete simple neural network the usual procedure is to name them by referring to the"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p110#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 110, "snippet": "4. 6 backpropagation 99 e = 1 2 ( t − y ) 2 = 1 2 ( 1 − 0. 6155 ) 2 = 0. 0739 now we are all set to calculate the derivatives. we will explain how to "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p111#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 111, "snippet": "100 4 feedforward neural networks ∂e ∂w5 = ∂e ∂yf · ∂yf ∂zf · ∂zf ∂w5 = − 0. 3844 · 0. 2365 · 0. 5868 = − 0. 0533 we repeat the same process 19 to get"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p112#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 112, "snippet": "4. 6 backpropagation 101 using the general weight update rule we have : wnew 3 = 0. 4 − ( 0. 7 · ( −0. 0035 ) ) = 0. 4024 we use the same steps ( acro"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p113#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 113, "snippet": "102 4 feedforward neural networks we can now make another forward pass with the new weights to make sure that the error has decreased : ynew c = σ ( 0"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p114#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 114, "snippet": "4. 7 a complete feedforward neural network 103 includes _ a _ book, purchase _ after _ 21, total, user _ action 1, 1, 13. 43, 1 1, 0, 23. 45, 1 0, 0, "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p115#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 115, "snippet": "104 4 feedforward neural networks mask = np. random. rand ( len ( raw _ data ) ) < train _ test _ split tr _ dataset = raw _ data [ mask ] te _ datase"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p116#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 116, "snippet": "4. 7 a complete feedforward neural network 105 metrics = ffnn. evaluate ( te _ data, te _ labels, verbose = 1 ) print ( \" % s : %. 2f % % \" % ( ffnn. "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p117#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 117, "snippet": "5modiﬁcations and extensions to a feed - forward neural network 5. 1 the idea of regularization let us recall the ideas of variance and bias. if we ha"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p118#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 118, "snippet": "108 5 modiﬁcations and extensions to a feed - forward neural network to include only the relevant tokens in the deﬁnition ). therefore, overﬁtting can"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p119#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 119, "snippet": "5. 1 the idea of regularization 109 effect will be similar to the points becoming actually little circles. in this way, some of the choices for the hy"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p120#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 120, "snippet": "110 5 modiﬁcations and extensions to a feed - forward neural network batch used ( to account for the fact that we want it to be proportional ), so the"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p121#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 121, "snippet": "5. 2 l1 and l2 regularization 111 they are missing ). this means that there are a number of useful applications of l1 regularization in signal process"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p122#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 122, "snippet": "112 5 modiﬁcations and extensions to a feed - forward neural network procedure how to do it. to do this, we must revisit the idea of splitting the dat"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p123#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 123, "snippet": "5. 3 learning rate, momentum and dropout 113 fig. 5. 2 gradient bowl friction or inertia and, perhaps the most counterintuitive, ( c ) our gravity is "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p124#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 124, "snippet": "114 5 modiﬁcations and extensions to a feed - forward neural network ‘ unfreezes ’ the situation, and again the general direction towards of the curva"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p125#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 125, "snippet": "5. 3 learning rate, momentum and dropout 115 fig. 5. 4 local minimum how much of the move to keep in the present step, while momentum controls how muc"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p126#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 126, "snippet": "116 5 modiﬁcations and extensions to a feed - forward neural network fig. 5. 5 dropout with π = 0. 5 the l1 and l2 regularizations ‘ numerical regular"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p127#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 127, "snippet": "5. 4 stochastic gradient descent and online learning 117 ﬁnished one epoch of training. we may do this for as many epochs we want. usually, we want to"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p128#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 128, "snippet": "118 5 modiﬁcations and extensions to a feed - forward neural network 5. 5 problems for multiple hidden layers : vanishing and exploding gradients let "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p129#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 129, "snippet": "5. 5 problems for multiple hidden layers : vanishing and exploding gradients 119 if we change radically our approach we would be blown in the opposite"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p130#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 130, "snippet": "120 5 modiﬁcations and extensions to a feed - forward neural network 8. j. wen, j. l. zhao, s. w. luo, z. han, the improvements of bp neural network l"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p131#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 131, "snippet": "6c o n v o l u t i o n a ln e u r a ln e t w o r k s 6. 1 a third visit to logistic regression in this chapter, we explore convolutional neural networ"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p132#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 132, "snippet": "122 6 convolutional neural networks fig. 6. 1 building a 1d convolutional layer with a logistic regression convolutional layers which will not be call"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p133#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 133, "snippet": "6. 1 a third visit to logistic regression 123 in − 1, 0 and 11 components to get the output vector to have the same size as the input vector is called"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p134#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 134, "snippet": "124 6 convolutional neural networks receptive ﬁeld to scan a 10 by 10 image, as the output from the local receptive ﬁeld we will get an 8 by 8 array ("}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p135#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 135, "snippet": "6. 2 feature maps and pooling 125 6. 2 feature maps and pooling now that we know how a convolutional neural network works, we can use a trick. recall "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p136#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 136, "snippet": "126 6 convolutional neural networks peculiar images. y ou can try to modify the code in the section below to print out feature maps which come out of "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p137#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 137, "snippet": "6. 2 feature maps and pooling 127 through a regular feed - forward fully connected network is highly sequential, since we need to have the derivatives"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p138#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 138, "snippet": "128 6 convolutional neural networks by 28 dimension of the images, and the last one is the channel. it could be rgb, but mnist is in greyscale, so thi"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p139#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 139, "snippet": "6. 3 a complete convolutional network 129 tensor. this is a generalized version of the process which we described for translating ﬁxed - size matrices"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p140#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 140, "snippet": "130 6 convolutional neural networks print ( predictions. shape ) to see the dimensionality of the array stored in predictions. these 29 lines of code "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p141#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 141, "snippet": "6. 4 using a convolutional network to classify text 131 were a vector. the ‘ width ’ of the local receptive ﬁeld remains a hyperparameter, as does the"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p142#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 142, "snippet": "132 6 convolutional neural networks a 0 0 1 0 0 1 0 b 0 0 0 0 0 0 1 c 0 0 0 1 0 0 0 d 1 1 0 0 0 0 0 s 0 0 0 0 1 0 0 if on the other hand we had the re"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p143#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 143, "snippet": "references 133 2. d. h. hubel, t. n. wiesel, receptive ﬁelds and functional architecture of monkey striate cortex. j. physiol. 195 ( 1 ), 215 – 243 ( "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p144#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 144, "snippet": "7recurrent neural networks 7. 1 sequences of unequal length let us take bird ’ s eye view of things. feedforward neural networks can process vectors, "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p145#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 145, "snippet": "136 7 recurrent neural networks the problem of varying dimensionality can be seen as the problem of learning sequences of unequal length, and audio pr"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p146#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 146, "snippet": "7. 2 the three settings of learning with recurrent neural networks 137 ( i ) p ( a ) ≥ 0, ( ii ) p ( / omega1 ) = 1, where / omega1is the possibility "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p147#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 147, "snippet": "138 7 recurrent neural networks • ( ‘ $ ’, ‘ all ’ ) • ( ‘ $ all ’, ‘ i ’ ) • ( ‘ $ all i ’, ‘ want ’ ) • ( ‘ $ all i want ’, ‘ for ’ ) • ( ‘ $ all i "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p148#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 148, "snippet": "7. 3 adding feedback loops and unfolding a neural network 139 7. 3 adding feedback loops and unfolding a neural network let us now see how recurrent n"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p149#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 149, "snippet": "140 7 recurrent neural networks fig. 7. 2 unfolding a recurrent neural network the proper and detailed notation used in the recurrent neural network l"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p150#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 150, "snippet": "7. 4 elman networks 141 calculates the output y at a ﬁnal time t. the calculation can be unfolded to the following recursive structure ( which makes i"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p151#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 151, "snippet": "142 7 recurrent neural networks 7. 5 long short - t erm memory in this section, we will give a graphical illustration of the workings of the long shor"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p152#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 152, "snippet": "7. 5 long short - term memory 143 fig. 7. 4 cell state ( a ), forget gate ( b ), input gate ( c ) and output gate ( d ) cell state is emphasized on fi"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p153#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 153, "snippet": "144 7 recurrent neural networks of basic ‘ building blocks ’ to be assembled together like lego® bricks, and then each block should have its own set o"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p154#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 154, "snippet": "7. 5 long short - term memory 145 have no way to ‘ force ’ this interpretation on the lstm other than with the sequence of calculations or ﬂow of info"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p155#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 155, "snippet": "146 7 recurrent neural networks let us take a minute and see what we are using. the variable hidden _ neurons simply states how many hidden units are "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p156#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 156, "snippet": "7. 6 using a recurrent neural network for predicting following words 147 code should be changed to reﬂect the appropriate ﬁle encoding. note that most"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p157#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 157, "snippet": "148 7 recurrent neural networks individual words. let us explain a bit of the idea. suppose we have a tiny text ‘ why would anyone ever eat anything b"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p158#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 158, "snippet": "7. 6 using a recurrent neural network for predicting following words 149 for i, input _ w in enumerate ( input _ words ) : for j, w in enumerate ( inp"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p159#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 159, "snippet": "150 7 recurrent neural networks we have the predict - next setting, which does not have labels, so we have to adopt a different approach. the idea is "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p160#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 160, "snippet": "7. 6 using a recurrent neural network for predicting following words 151 let us see how this works in a whole example. say, we want to calculate the g"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p161#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 161, "snippet": "152 7 recurrent neural networks which we will discuss in a different context can also be used as a loss or error function in neural network training. "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p162#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 162, "snippet": "8autoencoders 8. 1 learning representations in this and the next chapter we turn our attention to unsupervised deep learning, also known as learning d"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p163#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 163, "snippet": "154 8 autoencoders we will denote the covariance matrix of the matrix x as / xi1 ( x ). this is not a standard notation, but ( unlike the standard not"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p164#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 164, "snippet": "8. 1 learning representations 155 follow the usual matric entry naming conventions ) : v = ( 1, 2,..., d ) = v ( 1 ) 1 v ( 1 ) 2 · · · v ( 1 ) d v ( 2"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p165#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 165, "snippet": "156 8 autoencoders we now have to choose a matrix q so that we get what we want ( correlation zero and features ordered according to variance ). we si"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p166#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 166, "snippet": "8. 2 different autoencoder architectures 157 time we will produce a large hidden layer vector. this large hidden layer vector is a ( very large ) dist"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p167#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 167, "snippet": "158 8 autoencoders tribution of latent variables which hopefully are the objective latent variables and learning will conclude when they are very simi"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p168#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 168, "snippet": "8. 3 stacking autoencoders 159 ( 13, 7, 13 ). notice that if they want to process the same data they have to have the same input ( and output ) size. "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p169#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 169, "snippet": "160 8 autoencoders introduces a new array, of the size of the x _ train array populated with a gaussian random variable with loc = 0. 0 ( which is act"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p170#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 170, "snippet": "8. 3 stacking autoencoders 161 y ou should also increase the number of epochs once you get the code to work. finally we get to the last part of the au"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p171#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 171, "snippet": "162 8 autoencoders as the number of input neurons ), which makes it an autoencoder. result neurons are found in the middle part of the autoencoder. th"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p172#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 172, "snippet": "8. 4 recreating the cat paper 163 then sends them back to the repository so that other instances running asynchronous sgd can use them. the minibatch "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p173#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 173, "snippet": "9neural language models 9. 1 word embeddings and word analogies neural language models are distributed representations of words and sentences. they ar"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p174#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 174, "snippet": "166 9 neural language models vector in a way which will convey information about the meaning of the word ( i. e. its use in our language ). if we repr"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p175#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 175, "snippet": "9. 2 cbow and word2vec 167 context word ‘ are ’ ‘ who ’ ‘ who ’, ‘ you ’ ‘ are ’ ‘ are ’, ‘ that ’ ‘ you ’ ‘ you ’, ‘ you ’ ‘ that ’ ‘ that ’, ‘ do ’ "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p176#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 176, "snippet": "168 9 neural language models fig. 9. 1 cbow word2vec architecture 9. 3 word2vec in code in this and the next section, we give an example of a cbow wor"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p177#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 177, "snippet": "9. 3 word2vec in code 169 word2index = dict ( ( w, i ) for i, w in enumerate ( distinct _ words ) ) index2word = dict ( ( i, w ) for i, w in enumerate"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p178#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 178, "snippet": "170 9 neural language models word2vec = sequential ( ) word2vec. add ( dense ( embedding _ size, input _ shape = ( number _ of _ words, ), activation "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p179#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 179, "snippet": "9. 4 walking through the word - space : an idea that has eluded symbolic ai 171 9. 4 walking through the word - space : an idea that has eluded symbol"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p180#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 180, "snippet": "172 9 neural language models embedding _ weight _ matrix through pca and keep just the ﬁrst two dimen - sions, 3 and then simply draw the results to a"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p181#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 181, "snippet": "9. 4 walking through the word - space : an idea that has eluded symbolic ai 173 for the word ‘ someword ’. to recreate the classic example, take w2v ("}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p182#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 182, "snippet": "10an overview of different neural network architectures 10. 1 energy - based models energy - based models are a speciﬁc class of neural networks. the "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p183#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 183, "snippet": "176 10 an overview of different neural network architectures fig. 10. 1 hopﬁeld networks c = ( −1, −1, 1 ). using the equation above, we calculate the"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p184#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 184, "snippet": "10. 1 energy - based models 177 fig. 10. 2 boltzmann machines and restricted boltzmann machines we turn to a subclass of boltzmann machines, called re"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p185#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 185, "snippet": "178 10 an overview of different neural network architectures 10. 2 memory - based models the ﬁrst memory - based model we will explore are neural turi"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p186#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 186, "snippet": "10. 2 memory - based models 179 fig. 10. 3 neural turing - machines ( components range from 0 to 1 ) which indicates how much of each of those locatio"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p187#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 187, "snippet": "180 10 an overview of different neural network architectures networks even more aligned with the spirit of connectionism than neural turing - machines"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p188#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 188, "snippet": "10. 3 the kernel of general connectionist intelligence : the babi dataset 181 10. 3 the kernel of general connectionist intelligence : the babi datase"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p189#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 189, "snippet": "182 10 an overview of different neural network architectures 1. single supporting fact : 100 % 2. two supporting facts : 100 % 3. three supporting fac"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p190#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 190, "snippet": "references 183 4. g. e. hinton, s. osindero, y. - w. teh, a fast learning algorithm for deep belief nets. neural comput. 18 ( 7 ), 1527 – 1554 ( 2006 "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p191#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 191, "snippet": "11conclusion 11. 1 an incomplete overview of open research questions we conclude this book with a list of open research questions. a similar list, fro"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p192#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 192, "snippet": "186 11 conclusion 7. the approximation of the gradient is good enough for neural networks, but it is currently computationally less efﬁcient than symb"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p193#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 193, "snippet": "11. 2 the spirit of connectionism and philosophical ties 187 can be seen as a gradual transition from philosophy to mathematics. this can chart one ’ "}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p194#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 194, "snippet": "index a accidental properties, 108 accuracy, 57 activation function, 67, 81 adaptable learning rate, 97 analogical reasoning, 14 artiﬁcial intelligenc"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p195#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 195, "snippet": "190 index euclidean norm, 109 expected value, 35 f false negative, 57 false positive, 57 feature engineering, 55 feature maps, 125 features, 33, 52 fe"}
{"id": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf#p196#c1", "source": "DGM/Books/Introduction to Deep Learning_ From Logical Calculus to Artificial Intelligence.pdf", "page": 196, "snippet": "index 191 p padding, 123 parameters, 63 parity, 86 partial derivative, 30 pca, 70, 72, 172 perceptron, 6, 84, 87 positive - deﬁnite matrix, 154 precis"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p1#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 1, "snippet": "neural networks and deep learning charu c. aggarwal a textbook"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p2#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 2, "snippet": "neural networks and deep learning"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p3#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 3, "snippet": "charu c. aggarwal neural networks and deep learning a t extbook 123"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p4#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 4, "snippet": "charu c. aggarwal ibm t. j. w atson research center international business machines y orktown heights, ny, usa isbn 978 - 3 - 319 - 94462 - 3 isbn 978"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p5#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 5, "snippet": "to my wife lata, my daughter sayani, and my late parents dr. prem sarup and mrs. pushplata aggarwal."}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p6#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 6, "snippet": "preface “ any a. i. smart enough to pass a turing test is smart enough to know to fail it. ” — ian mcdonald neural networks were developed to simulate"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p7#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 7, "snippet": "viii preface expected to increase rapidly over time, and fundamentally more powerful paradigms like quantum computing are on the horizon, the computat"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p8#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 8, "snippet": "preface ix point are usually n - dimensional column vectors. an example is the n - dimensional column vector y of class variables of n data points. an"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p9#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 9, "snippet": "acknowledgments i would like to thank my family for their love and support during the busy time spent in writing this book. i would also like to thank"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p10#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 10, "snippet": "xii acknowledgments i would like to thank lata aggarwal for helping me with some of the ﬁgures created using powerpoint graphics in this book. my daug"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p11#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 11, "snippet": "contents 1 an introduction to neural networks 1 1. 1 introduction.................................... 1 1. 1. 1 humans versus computers : stretching t"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p12#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 12, "snippet": "xiv contents 1. 6 common neural architectures......................... 3 7 1. 6. 1 simulating basic machine learning with shallow models...... 3 7 1. "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p13#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 13, "snippet": "contents xv 2. 5. 7 recommender systems : row index to row value prediction.... 8 3 2. 5. 8 discussion................................. 8 6 2. 6 word2"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p14#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 14, "snippet": "xvi contents 3. 5. 3. 4 adadelta............................ 1 3 9 3. 5. 3. 5 adam.............................. 1 4 0 3. 5. 4 and higher - order inst"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p15#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 15, "snippet": "contents xvii 4. 10 regularization in unsupervised applications................. 2 0 1 4. 10. 1 value - based penalization : sparse autoencoders......"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p16#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 16, "snippet": "xviii contents 6. 5 applications of restricted boltzmann machines................ 2 5 1 6. 5. 1 dimensionality reduction and data reconstruction......"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p17#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 17, "snippet": "contents xix 8 convolutional neural networks 315 8. 1 introduction.................................... 3 1 5 8. 1. 1 historical perspective and biolog"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p18#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 18, "snippet": "xx contents 9. 3. 2 simple reinforcement learning for tic - tac - toe............ 3 8 0 9. 3. 3 role of deep learning and a straw - man algorithm....."}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p19#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 19, "snippet": "contents xxi 10. 6 limitations of neural networks......................... 4 5 3 10. 6. 1 an aspirational goal : one - shot learning............... 4 "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p20#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 20, "snippet": "author biography charu c. aggarwal is a distinguished research member ( drsm ) at the ibm t. j. watson research center in yorktown heights, new york. "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p21#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 21, "snippet": "chapter 1 an introduction to neural networks “ thou shalt not make a machine to counterfeit a human mind. ” — frank herbert 1. 1 introduction artiﬁcia"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p22#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 22, "snippet": "2 chapter 1. an introduction to neural networks role as the strengths of synaptic connections in biological organisms. each input to a neuron is scale"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p23#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 23, "snippet": "1. 1. introduction 3 accuracy amount of data deep learning conventional machine learning figure 1. 2 : an illustrative comparison of the accuracy of a"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p24#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 24, "snippet": "4 chapter 1. an introduction to neural networks complexity of a model by adding or removing neurons from the architecture according to the availabilit"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p25#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 25, "snippet": "1. 2. the basic architecture of neural networks 5 input nodes output node y w1 w2 w3 w4 x4 x3 x2 x1 x5 w5 input nodes output node w1 w2 w3 w4 w5 b + 1"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p26#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 26, "snippet": "6 chapter 1. an introduction to neural networks the architecture of the perceptron is shown in figure 1. 3 ( a ), in which a single input layer transm"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p27#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 27, "snippet": "1. 2. the basic architecture of neural networks 7 taining feature - label pairs : minimize w l = ( x, y ) ∈d ( y − ) 2 = ( x, y ) ∈d y − sign { w · x "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p28#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 28, "snippet": "8 chapter 1. an introduction to neural networks when errors are made in prediction. in mini - batch stochastic gradient descent, the aforemen - tioned"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p29#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 29, "snippet": "1. 2. the basic architecture of neural networks 9 the general goal was to minimize the number of classiﬁcation errors with a heuristic update process "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p30#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 30, "snippet": "10 chapter 1. an introduction to neural networks w w − α∇w li. the modiﬁed loss function to enable gradient computation of a non - function is also re"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p31#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 31, "snippet": "1. 2. the basic architecture of neural networks 11 which points are lossless and should not cause an update. the relationship between the perceptron c"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p32#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 32, "snippet": "12 chapter 1. an introduction to neural networks break up h = ( w x ). ah post - activation value pre - activation value = w x. { x w h = ( a h ) { x "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p33#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 33, "snippet": "1. 2. the basic architecture of neural networks 13 −2 −1. 5 −1 −0. 5 0 0. 5 1 1. 5 2−2 −1. 5 −1 −0. 5 0 0. 5 1 1. 5 2 −2 −1. 5 −1 −0. 5 0 0. 5 1 1. 5 "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p34#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 34, "snippet": "14 chapter 1. an introduction to neural networks x4 x3 x2 x1 input layer hidden layer x5 p ( y = green ) outputs p ( y = blue ) p ( y = red ) v1 v2 v3"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p35#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 35, "snippet": "1. 2. the basic architecture of neural networks 15 requires a simple squared loss of the form ( y − ) 2 for a single training instance with target y a"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p36#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 36, "snippet": "16 chapter 1. an introduction to neural networks −2 −1. 5 −1 −0. 5 0 0. 5 1 1. 5 2−2 −1. 5 −1 −0. 5 0 0. 5 1 1. 5 2 −2 −1. 5 −1 −0. 5 0 0. 5 1 1. 5 2 "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p37#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 37, "snippet": "1. 2. the basic architecture of neural networks 17 the key point is that this sigmoid can be written more conveniently in terms of the outputs : ∂o ∂v"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p38#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 38, "snippet": "18 chapter 1. an introduction to neural networks contains three layers. note that the input layer is often not counted, because it simply transmits th"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p39#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 39, "snippet": "1. 2. the basic architecture of neural networks 19 the weights of the connections between the input layer and the ﬁrst hidden layer are contained in a"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p40#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 40, "snippet": "20 chapter 1. an introduction to neural networks the convolutional neural network architecture ( cf. chapter 8 ), in which the architecture is careful"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p41#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 41, "snippet": "1. 3. training a neural network with backpropagation 21 with access to these building blocks. the analyst is able to specify the number and type of un"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p42#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 42, "snippet": "22 chapter 1. an introduction to neural networks w f ( w ) g ( y ) h ( z ) k ( p, q ) o = k ( p, q ) = k ( g ( f ( w ) ), h ( f ( w ) ) ) ugly composi"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p43#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 43, "snippet": "1. 3. training a neural network with backpropagation 23 the computation of ∂hr ∂w ( hr−1, hr ) on the right - hand side is straightforward and will be"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p44#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 44, "snippet": "24 chapter 1. an introduction to neural networks it is noteworthy that the dynamic programming recursion of equation 1. 26 can be computed in multiple"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p45#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 45, "snippet": "1. 4. practical issues in neural network training 25 1. 4. 1 the problem of overﬁtting the problem of overﬁtting refers to the fact that ﬁtting a mode"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p46#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 46, "snippet": "26 chapter 1. an introduction to neural networks take - away from the notion of bias - variance trade - is that one does not always win with more powe"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p47#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 47, "snippet": "1. 4. practical issues in neural network training 27 single - layer perceptron 3 because it can sometimes cause overly rapid forgetting with a small n"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p48#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 48, "snippet": "28 chapter 1. an introduction to neural networks even though deep networks have fewer problems with respect to overﬁtting, they come with a family of "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p49#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 49, "snippet": "1. 4. practical issues in neural network training 29 of adaptive learning rates and conjugate gradient methods can help in many cases. further - more,"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p50#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 50, "snippet": "30 chapter 1. an introduction to neural networks although algorithmic advancements have played a role in the recent excitement around deep learning, a"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p51#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 51, "snippet": "1. 5. the secrets to the power of function composition 31 then, we have the following recurrence condition for multi - layer networks : h1 = φ ( w t 1"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p52#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 52, "snippet": "32 chapter 1. an introduction to neural networks nonnegative structure in which extremes of the data distribution always saturate to zero density, and"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p53#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 53, "snippet": "1. 5. the secrets to the power of function composition 33 not linearly separable a b c x2 x1 ( - 1, 1 ) ( 0, 1 ) ( 1, 1 ) first layer transform h2 h1 "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p54#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 54, "snippet": "34 chapter 1. an introduction to neural networks there are many alternative choices of weights that can make the hidden representation linearly separa"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p55#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 55, "snippet": "1. 5. the secrets to the power of function composition 35 to the output. as long as the function to be learned is regular in some way, it is often pos"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p56#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 56, "snippet": "36 chapter 1. an introduction to neural networks tational units are often deﬁned by squashing functions applied to linear combinations of input. the h"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p57#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 57, "snippet": "1. 6. common neural architectures 37 architectures allow modeling with greater structure. unlike traditional neural networks in which nonlinearity is "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p58#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 58, "snippet": "38 chapter 1. an introduction to neural networks that the former gains its power from expanding the size of the feature space rather than depth. this "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p59#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 59, "snippet": "1. 6. common neural architectures 39 form x1... xn, w h e r ext is a d - dimensional point received at the time - stamp t. for example, the vector xt "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p60#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 60, "snippet": "40 chapter 1. an introduction to neural networks state at each time - stamp and the self - loop has been unfurled into a feed - forward network. this "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p61#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 61, "snippet": "1. 6. common neural architectures 41 depth in terms of the number of layers. in the input layer, these features correspond to the color channels like "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p62#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 62, "snippet": "42 chapter 1. an introduction to neural networks of the convolutional neural network, this insight was obtained by observing the biological workings o"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p63#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 63, "snippet": "1. 6. common neural architectures 43 input layer w x = 0 hidden layers learn features that are friendly to machine learning algorithms like classifica"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p64#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 64, "snippet": "44 chapter 1. an introduction to neural networks approach is often used for image data, in which the imagenet data set ( cf. section 1. 8. 2 ) is used"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p65#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 65, "snippet": "1. 7. advanced topics 45 the ﬁeld of neural networks, the strong complementarity of the two ﬁelds has brought them together. deep learning methods can"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p66#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 66, "snippet": "46 chapter 1. an introduction to neural networks 1. 8 two notable benchmarks the benchmarks used in the neural network literature are dominated by dat"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p67#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 67, "snippet": "1. 8. two notable benchmarks 47 database is used by neural network researchers in much the same way as biologists use fruit ﬂies for early and quick r"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p68#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 68, "snippet": "48 chapter 1. an introduction to neural networks convolutional neural networks are often trained on this data set ; the pretrained network can be used"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p69#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 69, "snippet": "1. 10. bibliographic notes 49 published a book on perceptrons [ 330 ], which was largely negative about the potential of being able to properly train "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p70#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 70, "snippet": "50 chapter 1. an introduction to neural networks makes the neural network harder to train from an optimization point of view. a discussion on some of "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p71#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 71, "snippet": "1. 11. exercises 51 1. 11 exercises 1. consider the case of the xor function in which the two points { ( 0, 0 ), ( 1, 1 ) } belong to one class, and t"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p72#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 72, "snippet": "52 chapter 1. an introduction to neural networks node applies relu activation to the sum of its two inputs. write the output of this neural network in"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p73#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 73, "snippet": "chapter 2 machine learning with shallow neural networks “ simplicity is the ultimate sophistication. ” — leonardo da vinci 2. 1 introduction conventio"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p74#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 74, "snippet": "54 chapter 2. machine learning with shallow neural networks learning models in data - lean settings as these models are more interpretable. on the oth"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p75#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 75, "snippet": "2. 2. neural architectures for binary classification models 55 this chapter will primarily discuss two classes of models for machine learning : 1. sup"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p76#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 76, "snippet": "56 chapter 2. machine learning with shallow neural networks continuous score output yloss = max ( 0, - y [ w x ] ) linear activation perceptron criter"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p77#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 77, "snippet": "2. 2. neural architectures for binary classification models 57 output node sign activation non - differentiable loss y loss = ( y - sign [ w x ] ) 2 x"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p78#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 78, "snippet": "58 chapter 2. machine learning with shallow neural networks 2. 2. 2 least - squares regression in least - squares regression, the training data contai"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p79#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 79, "snippet": "2. 2. neural architectures for binary classification models 59 one can rewrite the above update as follows : w w + α ( yi − ) x ( 2. 8 ) it is possibl"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p80#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 80, "snippet": "60 chapter 2. machine learning with shallow neural networks would be given by equation 2. 8, except for in how ( yi − ) is computed. in the case of th"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p81#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 81, "snippet": "2. 2. neural architectures for binary classification models 61 the second form of the update is helpful in relating it to perceptron and svm updates, "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p82#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 82, "snippet": "62 chapter 2. machine learning with shallow neural networks hyperplane. moving xi in either direction from the hyperplane results in signs of w · xi a"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p83#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 83, "snippet": "2. 2. neural architectures for binary classification models 63 just as the perceptron and the widrow - algorithms use the magnitudes of the mistakes t"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p84#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 84, "snippet": "64 chapter 2. machine learning with shallow neural networks in order to explain the in loss functions between the perceptron, widrow -, logistic regre"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p85#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 85, "snippet": "2. 3. neural architectures for multiclass models 65 svms [ 448 ]. the reader should also convince herself is that this update is identical to that of "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p86#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 86, "snippet": "66 chapter 2. machine learning with shallow neural networks ∂li ∂wr = − xi if r = c ( i ) xi if r = c ( i ) is most misclassiﬁed prediction 0 otherwis"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p87#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 87, "snippet": "2. 3. neural architectures for multiclass models 67 2. 3. 2 weston - watkins svm the weston - watkins svm [ 529 ] varies on the multiclass perceptron "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p88#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 88, "snippet": "68 chapter 2. machine learning with shallow neural networks for training instances xi in which the loss li is zero, the above update can be shown to s"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p89#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 89, "snippet": "2. 3. neural architectures for multiclass models 69 ∂li ∂wr = j ∂vj ∂wr = ∂li ∂vr ∂vr ∂wr xi ( 2. 35 ) in the above simpliﬁcation, we used the fact th"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p90#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 90, "snippet": "70 chapter 2. machine learning with shallow neural networks 2. 4 backpropagated saliency for interpretability and feature selection one of the common "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p91#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 91, "snippet": "2. 5. matrix factorization with autoencoders 71 one of our goals will be to demonstrate how small changes to the underlying building blocks of the neu"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p92#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 92, "snippet": "72 chapter 2. machine learning with shallow neural networks kth layer are tied to those incoming to the ( m − k ) th layer in many architectures. for "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p93#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 93, "snippet": "2. 5. matrix factorization with autoencoders 73 ( without worrying about neural networks at all ), our goal here is to capture this optimization probl"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p94#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 94, "snippet": "74 chapter 2. machine learning with shallow neural networks this result is easy to show at least for non - degenerate cases in which the rows of matri"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p95#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 95, "snippet": "2. 5. matrix factorization with autoencoders 75 decoder. this is also referred to as tying the weights. in particular, the autoencoder has an inherent"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p96#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 96, "snippet": "76 chapter 2. machine learning with shallow neural networks the sharing of weights does require some changes to the backpropagation algorithm during t"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p97#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 97, "snippet": "2. 5. matrix factorization with autoencoders 77 for text feature engineering is a logistic matrix factorization method, when one examines it more clos"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p98#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 98, "snippet": "78 chapter 2. machine learning with shallow neural networks simplistic compared to what is considered typical in kernel methods, in reality multiple h"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p99#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 99, "snippet": "2. 5. matrix factorization with autoencoders 79 1. many nonlinear dimensionality reduction methods have a very hard time mapping out - of - sample dat"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p100#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 100, "snippet": "80 chapter 2. machine learning with shallow neural networks spiral mapped to a linear hyperplane, clariﬁes the reason for this behavior. in many cases"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p101#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 101, "snippet": "2. 5. matrix factorization with autoencoders 81 2. 5. 5 when the hidden layer is broader than the input layer so far, we have only discussed cases in "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p102#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 102, "snippet": "82 chapter 2. machine learning with shallow neural networks 2. one can allow only the top - r activations in the hidden layer to be nonzero for r ≤ k."}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p103#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 103, "snippet": "2. 5. matrix factorization with autoencoders 83 with that of an adversarial discriminator in order to create generative samples of a data set. generat"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p104#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 104, "snippet": "84 chapter 2. machine learning with shallow neural networks small subset of ratings from a row may be available. as a practical matter, one might cons"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p105#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 105, "snippet": "2. 5. matrix factorization with autoencoders 85 item - wise ratings predictions for the rth user. therefore, all feature values are reconstructed in o"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p106#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 106, "snippet": "86 chapter 2. machine learning with shallow neural networks here, α is the step - size, and λ is the regularization parameter. these updates are ident"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p107#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 107, "snippet": "2. 6. word2vec : an application of simple neural architectures 87 2. 6 word2vec : an application of simple neural archi - tectures neural network meth"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p108#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 108, "snippet": "88 chapter 2. machine learning with shallow neural networks two indices corresponding to contextual position and word identiﬁer. speciﬁcally, the inpu"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p109#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 109, "snippet": "2. 6. word2vec : an application of simple neural architectures 89 words, we have the following : hq = i = 1 j = 1 ujq xij ∈ { 1... p } ( 2. 46 ) many "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p110#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 110, "snippet": "90 chapter 2. machine learning with shallow neural networks the probability of making a mistake in prediction on the jth word in the lexicon is deﬁned"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p111#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 111, "snippet": "2. 6. word2vec : an application of simple neural architectures 91 from 1 to d ( lexicon size ). each yij ∈ { 0, 1 } indicates whether the ith contextu"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p112#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 112, "snippet": "92 chapter 2. machine learning with shallow neural networks x1x2x3 xd h1 h2 hp y11 y12 y13 y1d yj1 yj2 yj3 yjd ym1 ym2 ym3 ymd u = [ ujq ] v = [ vqj ]"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p113#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 113, "snippet": "2. 6. word2vec : an application of simple neural architectures 93 note that the value outside the logarithm is a ground - truth binary value, whereas "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p114#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 114, "snippet": "94 chapter 2. machine learning with shallow neural networks misspellings, and it is hard to create a meaningful embedding for them without overﬁtting."}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p115#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 115, "snippet": "2. 6. word2vec : an application of simple neural architectures 95 the basic skip - gram model discussed earlier. sgns is not only, but it also provide"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p116#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 116, "snippet": "96 chapter 2. machine learning with shallow neural networks these are relatively superﬁcial, and one can still use logistic matrix factorization to re"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p117#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 117, "snippet": "2. 6. word2vec : an application of simple neural architectures 97 it is also easy to verify from equation 2. 58 that lij is p ( bij = 1 ) for positive"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p118#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 118, "snippet": "98 chapter 2. machine learning with shallow neural networks 2. 6. 4 vanilla skip - gram is multinomial matrix factorization since we have already show"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p119#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 119, "snippet": "2. 7. simple neural architectures for graph embeddings 99 node index 0 1 0 0 0 u neighbor indicator 1 0 1 1 0 vt 1 one - hot encoded input sigmoid act"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p120#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 120, "snippet": "100 chapter 2. machine learning with shallow neural networks 2. 7. 1 handling arbitrary edge counts the aforementioned discussion assumes that the wei"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p121#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 121, "snippet": "2. 9. bibliographic notes 101 2. 8 summary this chapter discusses a number of neural models supervised and unsupervised learning. one of the goals was"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p122#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 122, "snippet": "102 chapter 2. machine learning with shallow neural networks discusses the problem of recoding between input and output patterns. both classiﬁcation a"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p123#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 123, "snippet": "2. 10. exercises 103 2. 10 exercises 1. consider the following loss function for training pair ( x, y ) : l = m a x { 0, a − y ( w · x ) } the test in"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p124#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 124, "snippet": "104 chapter 2. machine learning with shallow neural networks 11. simulating a model combination ensemble : in machine learning, a model com - bination"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p125#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 125, "snippet": "chapter 3 training deep neural networks “ i hated every minute of training, but i said, ‘ don ’ t quit. now and live the rest of your life as a champi"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p126#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 126, "snippet": "106 chapter 3. training deep neural networks in the early years, methods for training multilayer networks were not known. in their inﬂuential book, mi"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p127#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 127, "snippet": "3. 2. backpropagation : the gory details 107 chapter organization this chapter is organized as follows. the next section reviews the backpropagation a"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p128#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 128, "snippet": "108 chapter 3. training deep neural networks forbidding from the perspective of computing partial derivatives. therefore, we need some kind of iterati"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p129#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 129, "snippet": "3. 2. backpropagation : the gory details 109 each local gradient only needs to worry about its speciﬁc input and output, which simpliﬁes the computati"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p130#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 130, "snippet": "110 chapter 3. training deep neural networks summing these products over all paths. ∂o ∂w = p ∈p ( i, j ) ∈p z ( i, j ) ( 3. 4 ) this lemma can be eas"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p131#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 131, "snippet": "3. 2. backpropagation : the gory details 111 the pathwise aggregation lemma implies that the value of ∂o ∂w is the product of the local derivatives ( "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p132#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 132, "snippet": "112 chapter 3. training deep neural networks let a ( i ) be the set of nodes at the end points of outgoing edges from node i. w ec a n compute the agg"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p133#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 133, "snippet": "3. 2. backpropagation : the gory details 113 3. 2. 3 backpropagation with post - activation variables in this section, we show how to instantiate the "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p134#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 134, "snippet": "114 chapter 3. training deep neural networks computed as the initial point of the recursion. subsequently, this computation is propagated in the backw"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p135#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 135, "snippet": "3. 2. backpropagation : the gory details 115 order to add up their local gradients and execute mini - batch stochastic gradient descent. this enhancem"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p136#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 136, "snippet": "116 chapter 3. training deep neural networks variables in the chain rule, whereas the recurrence for δ ( hr, o ) = ∂l ∂ahr uses the hidden values befo"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p137#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 137, "snippet": "3. 2. backpropagation : the gory details 117 4. use the computed partial derivatives of loss function with respect to weights in order to perform stoc"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p138#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 138, "snippet": "118 chapter 3. training deep neural networks chapter 1, the softmax converts k real - valued predictions v1... v k into output probabilities o1... o k"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p139#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 139, "snippet": "3. 2. backpropagation : the gory details 119 however, in many real implementations, the linear computations and the activation com - putations are dec"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p140#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 140, "snippet": "120 chapter 3. training deep neural networks a hidden variable in the ith layer. then, we have the following : zi + 1 = w t zi [ forward propagation ]"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p141#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 141, "snippet": "3. 2. backpropagation : the gory details 121 3. 2. 7 loss functions on multiple output nodes and hidden nodes for simplicity, the discussion above has"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p142#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 142, "snippet": "122 chapter 3. training deep neural networks common in machine learning algorithms. in this section, we provide a justiﬁcation for this choice along w"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p143#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 143, "snippet": "3. 2. backpropagation : the gory details 123 in this case, updating the full gradient with respect to all the points sums up the individual point - sp"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p144#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 144, "snippet": "124 chapter 3. training deep neural networks 2. in a recurrent neural network for text ( cf. chapter 7 ), the weights in temporal layers are shared, b"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p145#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 145, "snippet": "3. 3. setup and initialization issues 125 let the backpropagation - determined derivative be denoted by ge, and the aforemen - tioned estimation be de"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p146#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 146, "snippet": "126 chapter 3. training deep neural networks the hyperparameters are tested in order to determine the optimal choice. one issue with this procedure is"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p147#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 147, "snippet": "3. 3. setup and initialization issues 127 1. additive preprocessing and mean - centering : it can be useful to mean - center the data in order to remo"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p148#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 148, "snippet": "128 chapter 3. training deep neural networks dominated by numerical errors in the computation. it is a bad idea to include dimensions in which the var"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p149#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 149, "snippet": "3. 4. the vanishing and exploding gradient problems 129 successively weaker or successively stronger. the is exponentially related to the depth of the"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p150#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 150, "snippet": "130 chapter 3. training deep neural networks backpropagation update to show the following relationship : ∂l ∂ht = φ ′ ( ht + 1 ) · wt + 1 · ∂l ∂ht + 1"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p151#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 151, "snippet": "3. 4. the vanishing and exploding gradient problems 131 are encountered in almost any convex optimization problem. therefore, in this section, we will"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p152#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 152, "snippet": "132 chapter 3. training deep neural networks figure 3. 9 ( b ) occurs in almost any optimization problem using steepest descent, the case of the vanis"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p153#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 153, "snippet": "3. 4. the vanishing and exploding gradient problems 133 3. 4. 2 a partial fix with activation function choice the speciﬁc choice of activation functio"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p154#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 154, "snippet": "134 chapter 3. training deep neural networks although α is a hyperparameter chosen by the user, it is also possible to learn it. therefore, at negativ"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p155#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 155, "snippet": "3. 5. gradient - descent strategies 135 3. 5. 1 learning rate decay a constant learning rate is not desirable because it poses a dilemma to the analys"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p156#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 156, "snippet": "136 chapter 3. training deep neural networks 3. 5. 2 momentum - based learning momentum - based techniques recognize that zigzagging is a result of hi"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p157#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 157, "snippet": "3. 5. gradient - descent strategies 137 the use of momentum will often cause the solution to slightly overshoot in the direction where velocity is pic"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p158#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 158, "snippet": "138 chapter 3. training deep neural networks was the delta - bar - delta method [ 217 ]. this approach tracks whether the sign of each partial derivat"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p159#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 159, "snippet": "3. 5. gradient - descent strategies 139 instead of simply adding the squared gradients to estimate ai, i tu s e s exponential averaging. since one use"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p160#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 160, "snippet": "140 chapter 3. training deep neural networks updates in previous iterations. consider the update of rmsprop, which is repeated below : wi wi − α√ai ∂w"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p161#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 161, "snippet": "3. 5. gradient - descent strategies 141 there are two key from the rmsprop algorithm. first, the gradient is replaced with its exponentially smoothed "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p162#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 162, "snippet": "142 chapter 3. training deep neural networks slope. as a result, a small learning rate will lead to very slow learning, whereas increasing the learnin"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p163#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 163, "snippet": "3. 5. gradient - descent strategies 143 the of the two types of clipping is very similar. by clipping, one can achieve a better conditioning of the va"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p164#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 164, "snippet": "144 chapter 3. training deep neural networks one interesting characteristic of this update is that it is directly obtained from an opti - mality condi"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p165#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 165, "snippet": "3. 5. gradient - descent strategies 145 that one can take larger steps without causing harm to the optimization. pre - multiplication with the inverse"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p166#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 166, "snippet": "146 chapter 3. training deep neural networks a quadratic and convex loss function l ( w ) has an ellipsoidal contour plot of the type s h o w ni nf i "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p167#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 167, "snippet": "3. 5. gradient - descent strategies 147 a diagonal hessian hq = qt hq. however, hq is not a true diagonalization of h because qt q = i. nevertheless, "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p168#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 168, "snippet": "148 chapter 3. training deep neural networks hessian along particular directions ; we will see that these can be computed indirectly using the method "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p169#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 169, "snippet": "3. 5. gradient - descent strategies 149 the above update can be improved with an optimized learning rate αt for non - quadratic loss functions working"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p170#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 170, "snippet": "150 chapter 3. training deep neural networks minimum ( or maximum ). a saddle point is an inﬂection point, which appears to be either a minimum or a m"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p171#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 171, "snippet": "3. 5. gradient - descent strategies 151 point from the perspective of a newton update, even though it is not an extremum. saddle points occur frequent"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p172#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 172, "snippet": "152 chapter 3. training deep neural networks optimization problems, and they represent the simplest case of optimization. in general, however, the obj"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p173#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 173, "snippet": "3. 6. batch normalization 153 batch normalization is able to reduce this. in batch normalization, the idea is to add additional “ normalization layers"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p174#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 174, "snippet": "154 chapter 3. training deep neural networks by using the linear transformation deﬁned by the vector w i ( and biases if any ). for a particular batch"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p175#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 175, "snippet": "3. 6. batch normalization 155 programming recursion will be complete because one can then use these values of ∂l ∂ar j. o n e can compute the value of"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p176#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 176, "snippet": "156 chapter 3. training deep neural networks by plugging in the partial derivatives of the loss with respect to the mean and variance in equation 3. 6"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p177#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 177, "snippet": "3. 7. practical tricks for acceleration and compression 157 phone, which is highly constrained both in terms of memory and computational power. theref"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p178#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 178, "snippet": "158 chapter 3. training deep neural networks help the speed of computation. in such cases, the memory transfer cannot keep up with the speed of the pr"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p179#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 179, "snippet": "3. 7. practical tricks for acceleration and compression 159 communication. there are several ways in which one can partition the work across processor"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p180#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 180, "snippet": "160 chapter 3. training deep neural networks parameters to the parameter server without worrying about locks. in this case, would still be caused by o"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p181#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 181, "snippet": "3. 7. practical tricks for acceleration and compression 161 the weights will have zero values anyway because of the natural mathematical properties of"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p182#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 182, "snippet": "162 chapter 3. training deep neural networks for example, imagine a situation where we have a weight matrix of size 100 × 100 with 10 4 entries. in su"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p183#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 183, "snippet": "3. 9. bibliographic notes 163 3. the original training data contains targets with 0 / 1 values, whereas the newly created training contains soft targe"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p184#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 184, "snippet": "164 chapter 3. training deep neural networks hyperparameter optimization is discussed in [ 37 ]. the use of bayesian optimization for hyperparameter t"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p185#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 185, "snippet": "3. 10. exercises 165 compression with regularization is discussed in [ 168, 169 ]. a related model compression method is proposed in [ 213 ]. the use "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p186#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 186, "snippet": "166 chapter 3. training deep neural networks 6. consider the loss function l = x2 + y10. implement a simple steepest - descent algorithm to plot the c"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p187#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 187, "snippet": "3. 10. exercises 167 ( b ) show that ∇l ( w t + 1 ) is orthogonal to each qi for i ≤ t. [ t h ep r o o ff o rt h ec a s e when i = t is trivial becaus"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p188#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 188, "snippet": "chapter 4 teaching deep learners to generalize “ all generalizations are dangerous, even this one. ” — alexandre dumas 4. 1 introduction neural networ"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p189#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 189, "snippet": "170 chapter 4. teaching deep learners to generalize one is always looking to use the labeled images in order to learn captions for images that the lea"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p190#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 190, "snippet": "4. 1. introduction 171 linear simplification true model x = 2 x = 2 x = 2 polynomial prediction at x = 2 linear prediction at x = 2 x = 2 figure 4. 2 "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p191#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 191, "snippet": "172 chapter 4. teaching deep learners to generalize because of the large gaps between training and test error, models are often tested on unseen porti"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p192#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 192, "snippet": "4. 1. introduction 173 5. continuation and curriculum methods : these methods perform more by ﬁrst training simple models, and then making them more c"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p193#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 193, "snippet": "174 chapter 4. teaching deep learners to generalize chapter organization this chapter is organized as follows. the next section introduces the bias - "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p194#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 194, "snippet": "4. 2. the bias - variance trade - off 175 ure 4. 2. if there had been no noise, all points in the scatter plot would overlap with the curved line repr"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p195#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 195, "snippet": "176 chapter 4. teaching deep learners to generalize all prediction functions of learning models ( including neural networks ) are examples of the esti"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p196#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 196, "snippet": "4. 2. the bias - variance trade - off 177 squared error model complexity overall error optimal complexity figure 4. 3 : the trade - between bias and v"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p197#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 197, "snippet": "178 chapter 4. teaching deep learners to generalize 4. 3 generalization issues in model tuning and evaluation there are several practical issues in th"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p198#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 198, "snippet": "4. 3. generalization issues in model tuning and evaluation 179 contaminated with knowledge from the testing data. the idea that one is allowed to look"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p199#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 199, "snippet": "180 chapter 4. teaching deep learners to generalize can also be used for the second - level division into model building and validation portions. this"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p200#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 200, "snippet": "4. 4. penalty - based regularization 181 allowed to run to completion. one reason that such an approach works well is because the vast majority of the"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p201#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 201, "snippet": "182 chapter 4. teaching deep learners to generalize more than small values, because small values do not the prediction signiﬁcantly. what kind of pena"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p202#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 202, "snippet": "4. 4. penalty - based regularization 183 random vector, in which each entry oi is independently drawn from the standard normal distribution with zero "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p203#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 203, "snippet": "184 chapter 4. teaching deep learners to generalize penalties on the parameters. a common approach is l1 - regularization in which the squared penalty"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p204#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 204, "snippet": "4. 4. penalty - based regularization 185 and the l1 - regularizer acts as a feature selector. therefore, one can use l1 - regularization to estimate w"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p205#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 205, "snippet": "186 chapter 4. teaching deep learners to generalize note that the above update is based on equation 3. 26 of chapter 3. once the value of δ ( hr, n ( "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p206#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 206, "snippet": "4. 5. ensemble methods 187 values. this is the equivalent of using the geometric means of the probabilities. for discrete predictions, arithmetically "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p207#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 207, "snippet": "188 chapter 4. teaching deep learners to generalize try combinations of parameters and model choices. the selection that provides the highest accuracy"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p208#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 208, "snippet": "4. 5. ensemble methods 189 impossible to provide a prediction and compute the loss function. in some cases, the input nodes are sampled with a probabi"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p209#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 209, "snippet": "190 chapter 4. teaching deep learners to generalize means results in a situation where the probabilities over the classes do not sum to 1. therefore, "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p210#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 210, "snippet": "4. 5. ensemble methods 191 dependencies on other features, unless these other features are truly useful. to understand this point, consider a situatio"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p211#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 211, "snippet": "192 chapter 4. teaching deep learners to generalize 4. 6 early stopping neural networks are trained using variations of gradient - descent methods. in"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p212#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 212, "snippet": "4. 7. unsupervised pretraining 193 indirect manifestation of the variance in prediction created by a particular training data set. training data sets "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p213#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 213, "snippet": "194 chapter 4. teaching deep learners to generalize in order to provide robust initializations [ 196 ]. this initialization is achieved by training th"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p214#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 214, "snippet": "4. 7. unsupervised pretraining 195 consider the autoencoder and classiﬁer architectures shown in figure 4. 7. since these architectures have multiple "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p215#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 215, "snippet": "196 chapter 4. teaching deep learners to generalize during the early years, pretraining was often seen as a more stable way to train a deep network in"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p216#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 216, "snippet": "4. 7. unsupervised pretraining 197 digits have strokes that are curved in a particular way. the decoder reconstructs the digits by putting together th"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p217#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 217, "snippet": "198 chapter 4. teaching deep learners to generalize in at least some settings [ 113, 31 ]. this does not mean that supervised pretraining is never hel"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p218#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 218, "snippet": "4. 8. continuation and curriculum learning 199 layer is partially supervised. subsequent layers are trained using the autoencoder approach only. the i"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p219#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 219, "snippet": "200 chapter 4. teaching deep learners to generalize li as a smoothed version of li + 1. solving each li brings the solution closer to the basin of opt"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p220#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 220, "snippet": "4. 10. regularization in unsupervised applications 201 can be obtained when one has a good idea of how a particular computational node relates to the "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p221#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 221, "snippet": "202 chapter 4. teaching deep learners to generalize data set. therefore, one tends to hear fewer complaints about overﬁtting in unsupervised applicati"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p222#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 222, "snippet": "4. 10. regularization in unsupervised applications 203 3. salt - and - pepper noise : in this case, a fraction f of the inputs are set to either their"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p223#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 223, "snippet": "204 chapter 4. teaching deep learners to generalize of figure 4. 10. the true manifold is a more concise representation of the data as compared to the"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p224#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 224, "snippet": "4. 10. regularization in unsupervised applications 205 in the original paper [ 397 ], the sigmoid nonlinearity is used in the hidden layer, in which c"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p225#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 225, "snippet": "206 chapter 4. teaching deep learners to generalize some interesting relationships exist between the de - noising autoencoder and the contrac - tive a"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p226#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 226, "snippet": "4. 10. regularization in unsupervised applications 207 in a contractive autoencoder, the gradients are deterministic, and therefore it is also easier "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p227#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 227, "snippet": "208 chapter 4. teaching deep learners to generalize and lower standard deviations than that of the standard normal distribution ( which is like a prio"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p228#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 228, "snippet": "4. 10. regularization in unsupervised applications 209 traditional neural network. the only is that one needs to backpropagate across the unusual form"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p229#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 229, "snippet": "210 chapter 4. teaching deep learners to generalize decoder network gaussian samples n ( 0, i ) generated image figure 4. 13 : generating samples from"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p230#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 230, "snippet": "4. 10. regularization in unsupervised applications 211 * * * * * * * * * * * * o oo o o o o o o oo o o oo * * * * + + + + + + + + + + + + +......... +"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p231#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 231, "snippet": "212 chapter 4. teaching deep learners to generalize it is important to understand that the generated objects are often similar to but not exactly the "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p232#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 232, "snippet": "4. 11. summary 213 images are needed, and therefore the encoder and decoder are able to learn how the con - text relates to the images being generated"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p233#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 233, "snippet": "214 chapter 4. teaching deep learners to generalize reﬁned solutions. other related techniques include curriculum and continuation methods, which also"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p234#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 234, "snippet": "4. 13. exercises 215 compared to unsupervised pretraining, the of supervised pretraining is limited [ 31 ]. a detailed discussion of why unsupervised "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p235#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 235, "snippet": "216 chapter 4. teaching deep learners to generalize 2. consider a situation in which you have four attributes x1... x 4, and the dependent variable y "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p236#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 236, "snippet": "chapter 5 radial basis function networks “ two birds disputed about a kernel, when a third swooped down and carried it. ” — african proverb 5. 1 intro"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p237#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 237, "snippet": "218 chapter 5. radial basis function networks one can characterize the in the functionality of the hidden and output layers as follows : 1. the hidden"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p238#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 238, "snippet": "5. 1. introduction 219 then, for any input training point x, the activation φ i ( x ) o ft h e ith hidden unit is deﬁned as follows : hi = φ i ( x ) ="}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p239#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 239, "snippet": "220 chapter 5. radial basis function networks an unsupervised way, whereas those of the output layer are learned in a supervised way with gradient des"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p240#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 240, "snippet": "5. 2. training an rbf network 221 layer, our discussion will be restricted to unsupervised methods. in the following, we will ﬁrst discuss the trainin"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p241#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 241, "snippet": "222 chapter 5. radial basis function networks the selection of the prototype vectors is somewhat more complex. in particular, the following choices ar"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p242#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 242, "snippet": "5. 2. training an rbf network 223 the predictions of the n training points are given by the elements of the n - dimensional column vector hw t. ideall"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p243#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 243, "snippet": "224 chapter 5. radial basis function networks one can also choose to use mini - batch gradient descent in which the matrix h in the above update can b"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p244#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 244, "snippet": "5. 2. training an rbf network 225 data set. however, the procedure with which the prototype is added is far more. a set of orthogonal vectors are cons"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p245#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 245, "snippet": "226 chapter 5. radial basis function networks 1. an attractive characteristic of rbfs is that they are to train, if unsuper - vised methods are used. "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p246#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 246, "snippet": "5. 3. variations and special cases of rbf networks 227 then, for each mini - batch s of training instances, the following updates are used for the mis"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p247#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 247, "snippet": "228 chapter 5. radial basis function networks linearly separable in input space not linearly separable in input space but separable in 4 - dimensional"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p248#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 248, "snippet": "5. 4. relationship with kernel methods 229 w with zero error. in such a case, the activations h1... hn represent n - dimensional row vectors. therefor"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p249#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 249, "snippet": "230 chapter 5. radial basis function networks now consider the case in which the prototypes are the same as the training points, and therefore we set "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p250#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 250, "snippet": "5. 5. summary 231 it is instructive to compare this prediction function with that used in kernel svms ( see, for example, [ 6 ] ) with the lagrange mu"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p251#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 251, "snippet": "232 chapter 5. radial basis function networks can be used for classiﬁcation, regression, and linear interpolation by changing the nature of the loss f"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p252#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 252, "snippet": "5. 7. exercises 233 4. discuss how you can extend the three multi - class models discussed in chapter 2 to rbf networks. in particular discuss the ext"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p253#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 253, "snippet": "chapter 6 restricted boltzmann machines “ available energy is the main object at stake in the struggle for existence and the evolution of the world. ”"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p254#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 254, "snippet": "236 chapter 6. restricted boltzmann machines at their core, rbms are unsupervised models that generate latent feature representa - tions of the data p"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p255#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 255, "snippet": "6. 2. hopfield networks 237 the process of stacking multiple restricted boltzmann machines in order to create deep networks is discussed in section 6."}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p256#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 256, "snippet": "238 chapter 6. restricted boltzmann machines a hopﬁeld network, one is implicitly memorizing the training examples, although there is a relatively con"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p257#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 257, "snippet": "6. 2. hopfield networks 239 this value must be larger than 0 in order for a ﬂip of state si f r o m0t o1t ob ea t t r a c t i v e. therefore, one obta"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p258#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 258, "snippet": "240 chapter 6. restricted boltzmann machines output ( or recalled output ). in attribute completion, the state vector is initialized by set - ting obs"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p259#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 259, "snippet": "6. 2. hopfield networks 241 example contains d bits, it follows that the hopﬁeld network can store only about 0. 15 d2 bits. this is not an form of st"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p260#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 260, "snippet": "242 chapter 6. restricted boltzmann machines some function of the energy gap ( e. g., sigmoid ) in order to create probabilistic estimations. furtherm"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p261#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 261, "snippet": "6. 3. the boltzmann machine 243 6. 3 the boltzmann machine throughout this section, we assume that the boltzmann machine contains a total of q = ( m +"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p262#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 262, "snippet": "244 chapter 6. restricted boltzmann machines factor gets canceled out from the computation. for example, the conditional probability of equation 6. 7 "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p263#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 263, "snippet": "6. 3. the boltzmann machine 245 even generating a set of data points with the boltzmann machine is a more complicated process compared to many other p"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p264#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 264, "snippet": "246 chapter 6. restricted boltzmann machines 2. model samples : the second type of sample does not put any constraints on states, and one simply wants"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p265#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 265, "snippet": "6. 4. restricted boltzmann machines 247 6. 4 restricted boltzmann machines in the boltzmann machine, the connections among hidden and visible units ca"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p266#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 266, "snippet": "248 chapter 6. restricted boltzmann machines value of 1 can be written directly as a logistic function of the values of visible units. in other words,"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p267#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 267, "snippet": "6. 4. restricted boltzmann machines 249 ice - creams she will receive on future days. even the weights of the model can be learned by alice from examp"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p268#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 268, "snippet": "250 chapter 6. restricted boltzmann machines samples. however, it turns out that it is possible to run the monte carlo sampling for only as h o r tt i"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p269#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 269, "snippet": "6. 5. applications of restricted boltzmann machines 251 6. 4. 3 practical issues and improvisations there are several practical issues in training the"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p270#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 270, "snippet": "252 chapter 6. restricted boltzmann machines node is a binary value sampled from the bernoulli probabilities deﬁned by equations 6. 15 and 6. 17. on t"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p271#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 271, "snippet": "6. 5. applications of restricted boltzmann machines 253 hidden states visible states hidden states visible states equivalence w ww t ( a ) equivalence"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p272#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 272, "snippet": "254 chapter 6. restricted boltzmann machines although does represent the expected value of the jth hidden unit, applying the sig - moid function again"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p273#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 273, "snippet": "6. 5. applications of restricted boltzmann machines 255 0 0 0 1 0 e. t. ( rating = 4 ) 0 0 0 0 1 shrek ( rating = 5 ) hidden units h1 h2 ( a ) rbm arc"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p274#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 274, "snippet": "256 chapter 6. restricted boltzmann machines the states of the hidden units, which are binary, are deﬁned with the use of the sigmoid function : p ( h"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p275#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 275, "snippet": "6. 5. applications of restricted boltzmann machines 257 softmax units for the input and output layers. although the original paper [ 414 ] d o e sn o "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p276#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 276, "snippet": "258 chapter 6. restricted boltzmann machines there is, however, another alternative approach to perform classiﬁcation with the rbm, which integrates r"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p277#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 277, "snippet": "6. 5. applications of restricted boltzmann machines 259 2. the hidden layer contains m binary units. the hidden units are denoted by h1... h m. multin"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p278#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 278, "snippet": "260 chapter 6. restricted boltzmann machines this approach is a direct extension from collaborative ﬁltering. however, the main problem is that this g"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p279#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 279, "snippet": "6. 5. applications of restricted boltzmann machines 261 binary hidden states multinomial visible states lexicon size d is typically larger than docume"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p280#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 280, "snippet": "262 chapter 6. restricted boltzmann machines the architecture of the rbm is illustrated in figure 6. 8. based on the architecture of the rbm, one can "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p281#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 281, "snippet": "6. 6. using rbms beyond binary data types 263 visible states ( text ) visible states ( image ) hidden states ( shared ) wts wis ( a ) a simple rbm for"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p282#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 282, "snippet": "264 chapter 6. restricted boltzmann machines with an ordered attribute, when the number of discrete values of that attribute is small. however, these "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p283#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 283, "snippet": "6. 7. stacking restricted boltzmann machines 265 rbm 3 rbm 2 rbm 1 copy copy stacked representation w1 w2 w3 the parameter matrices w1, w2, and w3 are"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p284#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 284, "snippet": "266 chapter 6. restricted boltzmann machines weights can be used to deﬁne a related neural network that performs directed computation in the continuou"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p285#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 285, "snippet": "6. 7. stacking restricted boltzmann machines 267 together. better results are obtained by using a pretraining approach. each of the three rbms in figu"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p286#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 286, "snippet": "268 chapter 6. restricted boltzmann machines that we can merge the nodes in adjacent nodes of two rbms into a single layer of nodes. furthermore, obse"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p287#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 287, "snippet": "6. 9. bibliographic notes 269 with real - valued activations and derived weights from the trained boltzmann machine is often used for prediction. othe"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p288#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 288, "snippet": "270 chapter 6. restricted boltzmann machines are discussed in [ 437 ]. although these methods are potentially more powerful that traditional boltzmann"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p289#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 289, "snippet": "chapter 7 recurrent neural networks “ democracy is the recurrent suspicion that more than half the people are right more than half the time. ” — the n"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p290#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 290, "snippet": "272 chapter 7. recurrent neural networks the individual values in a sequence can be either real - valued or symbolic. real - valued sequences are also"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p291#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 291, "snippet": "7. 1. introduction 273 network. the goal of such an approach would be to reduce the parameter requirements with increasing sequence length ; recurrent"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p292#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 292, "snippet": "274 chapter 7. recurrent neural networks problem. this problem is particularly prevalent in the context of deep networks like recurrent neural network"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p293#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 293, "snippet": "7. 2. the architecture of recurrent neural networks 275 network to change after the input of each word in the sequence. in practice, one only works wi"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p294#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 294, "snippet": "276 chapter 7. recurrent neural networks no missing inputs or outputs [ example : forecasting, language modeling ] missing inputs [ example : image ca"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p295#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 295, "snippet": "7. 2. the architecture of recurrent neural networks 277 here, the “ tanh ” notation is used in a relaxed way, in the sense that the function is applie"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p296#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 296, "snippet": "278 chapter 7. recurrent neural networks whh wxh why wxh why wxh why wxh why whh whh the cat chased the cat chased the mouse 1 0 0 0 0 1 0 0 0 0 1 0 1"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p297#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 297, "snippet": "7. 2. the architecture of recurrent neural networks 279 king richard ii : do cantant, - ’ for neight here be with hand her, - eptar the home that valy"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p298#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 298, "snippet": "280 chapter 7. recurrent neural networks 7. 2. 2 backpropagation through time the negative logarithms of the softmax probability of the correct words "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p299#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 299, "snippet": "7. 2. the architecture of recurrent neural networks 281 ( iii ) we add all the ( shared ) weights corresponding to instantiations of an edge in time. "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p300#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 300, "snippet": "282 chapter 7. recurrent neural networks ( cf. chapter 2 ). this approach is a form of pretraining. the speciﬁc advantage of using this type of pretra"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p301#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 301, "snippet": "7. 2. the architecture of recurrent neural networks 283 7. 2. 3 bidirectional recurrent networks one disadvantage of recurrent networks is that the st"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p302#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 302, "snippet": "284 chapter 7. recurrent neural networks backwards direction. at this point, the output states are computed from the hidden states in the two directio"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p303#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 303, "snippet": "7. 2. the architecture of recurrent neural networks 285 an example of a deep network containing three layers is shown in figure 7. 6. n o t et h a t n"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p304#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 304, "snippet": "286 chapter 7. recurrent neural networks in this case, the size of the matrix w ( k ) is p × ( p + p ) = p × 2p. the transformation from hidden to out"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p305#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 305, "snippet": "7. 3. the challenges of training recurrent networks 287 activation function, for which the derivative φ ′ ( · ) is almost always less than 1, tends to"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p306#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 306, "snippet": "288 chapter 7. recurrent neural networks are often hidden near or other regions of unpredictable change in the topography of the loss function, which "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p307#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 307, "snippet": "7. 3. the challenges of training recurrent networks 289 finally, a number of variants of recurrent neural networks are used to address the van - ishin"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p308#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 308, "snippet": "290 chapter 7. recurrent neural networks note that at is a vector with as many components as the number of units in the hidden layer ( which we have c"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p309#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 309, "snippet": "7. 4. echo - state networks 291 as in traditional recurrent networks, the hidden - to - hidden layers have nonlinear activa - tions such as the logist"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p310#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 310, "snippet": "292 chapter 7. recurrent neural networks t that correspond to the time - series input values at t + k, w h e r ek is the lookahead required for foreca"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p311#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 311, "snippet": "7. 5. long short - term memory ( lstm ) 293 of figure 7. 6 in which we change the recurrence conditions of how the hidden states h ( k ) t are propaga"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p312#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 312, "snippet": "294 chapter 7. recurrent neural networks here, the element - wise product of vectors is denoted by “, ” and the notation “ sigm ” denotes a sigmoid op"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p313#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 313, "snippet": "7. 6. gated recurrent units ( grus ) 295 finally, the hidden states h ( k ) t are updated using leakages from the cell state. the hidden state is upda"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p314#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 314, "snippet": "296 chapter 7. recurrent neural networks in the case of the gru, we use two matrices w ( k ) and v ( k ) of sizes 5 2p × 2p and p × 2p, respectively. "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p315#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 315, "snippet": "7. 7. applications of recurrent neural networks 297 backward gradient ﬂow is multiplied with this factor. here, the term z ∈ ( 0, 1 ) helps in passing"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p316#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 316, "snippet": "298 chapter 7. recurrent neural networks the following material will provide an overview of the numerous applications of recurrent neural networks. in"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p317#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 317, "snippet": "7. 7. applications of recurrent neural networks 299 ht = t a n h ( wxhxt + whhht−1 ) ≥ 2 yt = whyht an important point here is that the convolutional "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p318#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 318, "snippet": "300 chapter 7. recurrent neural networks in the following, we provide a simple solution to machine translation with recurrent neural networks, althoug"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p319#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 319, "snippet": "7. 7. applications of recurrent neural networks 301 with the use of the backpropagation algorithm. since only the nodes in rnn2 have outputs, only the"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p320#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 320, "snippet": "302 chapter 7. recurrent neural networks there is a greater level of reasoning in the latter, which typically requires an understanding of the relatio"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p321#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 321, "snippet": "7. 7. applications of recurrent neural networks 303 the paraphrased question can be learned with sequence - to - sequence learning, although the work "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p322#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 322, "snippet": "304 chapter 7. recurrent neural networks 7. 7. 4 token - level classiﬁcation with linguistic features the numerous applications of token - level class"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p323#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 323, "snippet": "7. 7. applications of recurrent neural networks 305 in some variations, it might also be helpful to concatenate the linguistic and token - wise featur"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p324#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 324, "snippet": "306 chapter 7. recurrent neural networks relationship with autoregressive models an autoregressive model models the values of a time - series as a lin"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p325#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 325, "snippet": "7. 7. applications of recurrent neural networks 307 7. 7. 6 temporal recommender systems several solutions [ 465, 534, 565 ] have been proposed in rec"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p326#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 326, "snippet": "308 chapter 7. recurrent neural networks in several cases, explicit ratings are not available, but implicit feedback data is available corresponding t"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p327#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 327, "snippet": "7. 7. applications of recurrent neural networks 309 neural network is e1... et, which was used to predict the outputs o1... ot. the output at the time"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p328#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 328, "snippet": "310 chapter 7. recurrent neural networks further features such as a feature indicating whether the pen is touching the writing surface, the angles bet"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p329#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 329, "snippet": "7. 9. bibliographic notes 311 in [ 368, 369 ]. recurrent neural networks ( and their advanced variations ) began to become more attractive after about"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p330#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 330, "snippet": "312 chapter 7. recurrent neural networks works like deeplearning4j provide implementations of lstms [ 617 ]. implementations of sentiment analysis wit"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p331#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 331, "snippet": "7. 10. exercises 313 if only a small subset of points in the training data set are re - scaled? would the learned weights in either normalization meth"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p332#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 332, "snippet": "chapter 8 convolutional neural networks “ the soul never thinks without a picture. ” — aristotle 8. 1 introduction convolutional neural networks are d"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p333#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 333, "snippet": "316 chapter 8. convolutional neural networks an important deﬁning characteristic of convolutional neural networks is an operation, which is referred t"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p334#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 334, "snippet": "8. 1. introduction 317 neural networks have been consistent winners of this contest since 2012. in fact, the domi - nance of convolutional neural netw"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p335#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 335, "snippet": "318 chapter 8. convolutional neural networks 8. 2 the basic structure of a convolutional network in convolutional neural networks, the states in each "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p336#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 336, "snippet": "8. 2. the basic structure of a convolutional network 319 raw pixels. furthermore, the value of dq is much larger than three for the hidden layers beca"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p337#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 337, "snippet": "320 chapter 8. convolutional neural networks ﬁlter tries to identify a particular type of spatial pattern in a small rectangular region of the image, "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p338#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 338, "snippet": "8. 2. the basic structure of a convolutional network 321 the expression above seems notationally complex, although the underlying convolutional operat"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p339#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 339, "snippet": "322 chapter 8. convolutional neural networks parameters across the entire convolution is that the presence of a particular shape in any part of the im"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p340#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 340, "snippet": "8. 2. the basic structure of a convolutional network 323 634 474 702 5 8 8 0 64 1 37 03 5 254 1 06 430 0 45 0 04 0 34 5 5 1 0 0 2 7 2 4 3 634 474 702 "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p341#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 341, "snippet": "324 chapter 8. convolutional neural networks input dimensions in the original image are lq and bq, the padded spatial dimensions in the input volume b"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p342#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 342, "snippet": "8. 2. the basic structure of a convolutional network 325 of the image to create the training data. the number of ﬁlters in each layer is often a power"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p343#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 343, "snippet": "326 chapter 8. convolutional neural networks 8. 2. 5 pooling the pooling operation is, however, quite. the pooling operation works on small grid regio"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p344#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 344, "snippet": "8. 2. the basic structure of a convolutional network 327 other types of pooling ( like average - pooling ) are possible but rarely used. in the ear - "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p345#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 345, "snippet": "328 chapter 8. convolutional neural networks more signiﬁcantly is that the number of activations are multiplied by mini - batch size while tracking va"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p346#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 346, "snippet": "8. 2. the basic structure of a convolutional network 329 footprint of the activation maps because it uses strides that are larger than 1. it is also p"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p347#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 347, "snippet": "330 chapter 8. convolutional neural networks units in the ﬁnal layer ( cf. chapter 5 ), in which the prototype of each unit was compared to its input "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p348#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 348, "snippet": "8. 2. the basic structure of a convolutional network 331 these n ﬁlters at a particular spatial position ( x, y ) a r eg i v e nb y a1... a n. then, e"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p349#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 349, "snippet": "332 chapter 8. convolutional neural networks in a later section, we will show visualizations of how smaller portions of real - world image activate hi"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p350#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 350, "snippet": "8. 3. training a convolutional network 333 8. 3. 1 backpropagating through convolutions the backpropagation through convolutions is also not very from"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p351#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 351, "snippet": "334 chapter 8. convolutional neural networks note that the approach above uses simple linear accumulation of gradients like tradi - tional backpropaga"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p352#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 352, "snippet": "8. 3. training a convolutional network 335 case, let the 5 - dimensional tensor corresponding to the backpropagation ﬁlters from layer q + 1 t o l a y"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p353#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 353, "snippet": "336 chapter 8. convolutional neural networks key in representing the convolution as a matrix multiplication. a matrix of size ao × ai is deﬁned in whi"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p354#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 354, "snippet": "8. 3. training a convolutional network 337 the matrix - centric approach is very useful for performing backpropagation because one can also propagate "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p355#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 355, "snippet": "338 chapter 8. convolutional neural networks although most data augmentation methods are quite, some forms of transfor - mation that use principal com"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p356#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 356, "snippet": "8. 4. case studies of convolutional architectures 339 224 224 3 11 11 55 55 5 5 96 256 27 27 3 3 13 3 3 384 13 3 3 384 13 13 256 13 13 4096 4096 1000 "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p357#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 357, "snippet": "340 chapter 8. convolutional neural networks layers in figure 8. 9 ( b ), which leads to some between figure 8. 9 ( a ) and ( b ) in terms of the actu"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p358#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 358, "snippet": "8. 4. case studies of convolutional architectures 341 many design choices used in the architecture became standard in later architectures. a speciﬁc e"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p359#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 359, "snippet": "342 chapter 8. convolutional neural networks table 8. 1 : comparison of alexnet and zfnet alexnet zfnet volume : 224 × 224 × 3 224 × 224 × 3 operation"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p360#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 360, "snippet": "8. 4. case studies of convolutional architectures 343 an important innovation of vgg is that it reduced ﬁlter sizes but increased depth. it is importa"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p361#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 361, "snippet": "344 chapter 8. convolutional neural networks table 8. 2 : conﬁgurations used in vgg. the term c3d64 refers to the case in which convo - lutions are pe"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p362#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 362, "snippet": "8. 4. case studies of convolutional architectures 345 connected layer has dense connections between 4096 neurons and a 7 ×7×512 volume. as we will see"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p363#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 363, "snippet": "346 chapter 8. convolutional neural networks filter concatenation 1 x 1 convolutions previous layer 3 x 3 convolutions 5x 5 convolutions 3x 3 max - po"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p364#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 364, "snippet": "8. 4. case studies of convolutional architectures 347 parameters in the former is less by an order of magnitude. this is primarily because of the use "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p365#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 365, "snippet": "348 chapter 8. convolutional neural networks weight layer weight layer relu + f ( x ) x relu f ( x ) + x identity x 7x7 conv, 64, / 2 3x3 conv, 64 poo"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p366#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 366, "snippet": "8. 4. case studies of convolutional architectures 349 layers of the architecture is shown in figure 8. 11 ( b ). this particular snapshot is based on "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p367#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 367, "snippet": "350 chapter 8. convolutional neural networks table 8. 3 : the number of layers in various top - performing ilsvrc contest entries name year number of "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p368#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 368, "snippet": "8. 4. case studies of convolutional architectures 351 closely correlated with improved error rates. therefore, an important focus of the research in r"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p369#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 369, "snippet": "352 chapter 8. convolutional neural networks orientation to represent them ( captured in early layers ), but a feature corresponding to the wheel of a"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p370#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 370, "snippet": "8. 5. visualization and unsupervised learning 353 8. 5. 1 visualizing the features of a trained network consider a neural network that has already bee"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p371#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 371, "snippet": "354 chapter 8. convolutional neural networks figure 8. 12 : examples of portions of speciﬁc images activated by particular class labels. these images "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p372#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 372, "snippet": "8. 5. visualization and unsupervised learning 355 that cause this hidden feature to be activated. the grayscale portion of the visualization correspon"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p373#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 373, "snippet": "356 chapter 8. convolutional neural networks 2 4 1 - 1 - 2 3 - 2 13 traditional backpropagation forward pass relu 2 4 10 03 0 13 - 1 2 - 3 2 - 1 2 1 2"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p374#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 374, "snippet": "8. 5. visualization and unsupervised learning 357 synthesized images that activate a feature the above examples tell us the portions of a particular i"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p375#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 375, "snippet": "358 chapter 8. convolutional neural networks figure 8. 15 : examples of activation visualizations in layers based on zeiler and fer - gus ’ s work [ 5"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p376#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 376, "snippet": "8. 5. visualization and unsupervised learning 359 figure 8. 16 : examples of synthesized images with respect to particular class labels. these example"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p377#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 377, "snippet": "360 chapter 8. convolutional neural networks table 8. 4 : the relationship between backpropagation and decoders linear operation traditional neural ne"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p378#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 378, "snippet": "8. 5. visualization and unsupervised learning 361 the ( i, j, k ) th entry of the pth ﬁlter in the ﬁrst layer has weight w ( p, 1 ) ijk. this notation"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p379#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 379, "snippet": "362 chapter 8. convolutional neural networks like traditional autoencoders, the loss function is deﬁned by the reconstruction error over all l1 × l1 ×"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p380#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 380, "snippet": "8. 6. applications of convolutional networks 363 figure 8. 18 : example of image classiﬁcation / localization in which the class “ ﬁsh ” i si d e n t "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p381#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 381, "snippet": "364 chapter 8. convolutional neural networks convolution layers ( weights fixed for both classification and regression ) softmax fully connected class"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p382#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 382, "snippet": "8. 6. applications of convolutional networks 365 figure 8. 20 : example of object detection. here, four objects are identiﬁed together with their boun"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p383#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 383, "snippet": "366 chapter 8. convolutional neural networks object detection is generally a more problem than that of localization because of the variable number of "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p384#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 384, "snippet": "8. 6. applications of convolutional networks 367 with this approach is that the use of one - hot encoding increases the number of channels, and theref"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p385#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 385, "snippet": "368 chapter 8. convolutional neural networks 8. 7 summary this chapter discusses the use of convolutional neural networks with a primary focus on imag"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p386#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 386, "snippet": "8. 8. bibliographic notes 369 in the construction of an autoencoder because one can use a convolutional layer with a fractional stride within the deco"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p387#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 387, "snippet": "370 chapter 8. convolutional neural networks video data can be considered the spatiotemporal generalization of image data from the perspective of conv"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p388#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 388, "snippet": "8. 9. exercises 371 in the data set has size 32 × 32 × 3. it is noteworthy that the cifar - 10 data set is a small subset of the tiny images data set "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p389#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 389, "snippet": "chapter 9 deep reinforcement learning “ the reward of is experience. ” — harry s. truman 9. 1 introduction human beings do not learn from a concrete n"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p390#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 390, "snippet": "374 chapter 9. deep reinforcement learning a reward - driven trial - and - error process, in which a system learns to interact with a complex environm"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p391#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 391, "snippet": "9. 2. stateless algorithms : multi - armed bandits 375 reinforcement learning is appropriate for tasks that are simple to evaluate but hard to specify"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p392#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 392, "snippet": "376 chapter 9. deep reinforcement learning cursor in a particular direction has a reward that heavily depends on the state of the video game. there ar"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p393#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 393, "snippet": "9. 3. the basic framework of reinforcement learning 377 one can consider the upper bound ui of testing a slot machine i as the sum of expected reward "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p394#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 394, "snippet": "378 chapter 9. deep reinforcement learning point a few movies back, and actions since then might have had no bearing on the reward. furthermore, the r"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p395#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 395, "snippet": "9. 3. the basic framework of reinforcement learning 379 examples although a system state refers to a complete description of the environment, many pra"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p396#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 396, "snippet": "380 chapter 9. deep reinforcement learning in the following sections, we will introduce a simple reinforcement learning algorithm and discuss the role"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p397#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 397, "snippet": "9. 3. the basic framework of reinforcement learning 381 xo o x playing x here assures victory with optimal play x o playing x here assures victory wit"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p398#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 398, "snippet": "382 chapter 9. deep reinforcement learning a minuscule fraction of the valid positions. in fact, the algorithm of section 9. 3. 2 is a reﬁned form of "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p399#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 399, "snippet": "9. 4. bootstrapping for value function learning 383 9. 4 bootstrapping for value function learning the simple generalization of the o - greedy algorit"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p400#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 400, "snippet": "384 chapter 9. deep reinforcement learning 9. 4. 1 deep learning models as function approximators for ease in discussion, we will work with the atari "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p401#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 401, "snippet": "9. 4. bootstrapping for value function learning 385 ahead in order to create an improved estimate of q ( st, a t ). it is important to set ( st + 1, a"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p402#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 402, "snippet": "386 chapter 9. deep reinforcement learning 4. one can now use backpropagation on this loss function in order to update the weight vector w. even thoug"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p403#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 403, "snippet": "9. 4. bootstrapping for value function learning 387 and 1, respectively. the convolutional layers were followed by two fully connected layers. the num"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p404#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 404, "snippet": "388 chapter 9. deep reinforcement learning the function f ( ·, ·, · ) is deﬁned in the same way as the previous section. the weight vector is updated "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p405#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 405, "snippet": "9. 4. bootstrapping for value function learning 389 9. 4. 4 modeling states versus state - action pairs a minor variation of the theme in the previous"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p406#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 406, "snippet": "390 chapter 9. deep reinforcement learning during the training phase, one needs to shift the weights so as to push g ( xt, w ) t o w a r d s the impro"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p407#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 407, "snippet": "9. 5. policy gradient methods 391 convolutional neural network observed state ( previous four screens of pixels ) probability of “ up ” softmax probab"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p408#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 408, "snippet": "392 chapter 9. deep reinforcement learning for each action a, observed state representation xt, and current parameter w, the neural network is able to"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p409#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 409, "snippet": "9. 5. policy gradient methods 393 we can create an s - dimensional column vector y = [ ∆ j1... ∆ js ] t of the changes in the objective function and a"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p410#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 410, "snippet": "394 chapter 9. deep reinforcement learning discount ). consider a game containing at most h moves. since multiple roll - outs are used, we get a whole"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p411#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 411, "snippet": "9. 5. policy gradient methods 395 that have greater state - speciﬁc impact will be recognized with a higher advantage ( within a single game ). as a r"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p412#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 412, "snippet": "396 chapter 9. deep reinforcement learning decisions are outside its control ( beyond its role as a critic ). therefore, the policy network is the act"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p413#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 413, "snippet": "9. 5. policy gradient methods 397 single value - network is enough. this is because we can use rt + γ p ( st + 1 ) in lieu of ( st, a t ). this result"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p414#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 414, "snippet": "398 chapter 9. deep reinforcement learning are known to be suboptimal ( such as guessing games ) due to being able to be exploited by the opponent. q "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p415#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 415, "snippet": "9. 7. case studies 399 computable. monte carlo simulations are used to estimate the value of the newly added leaf node s ′. speciﬁcally, monte carlo r"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p416#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 416, "snippet": "400 chapter 9. deep reinforcement learning exploring the minimax tree of moves up to a certain depth can perform signiﬁcantly better than the best hum"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p417#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 417, "snippet": "9. 7. case studies 401 some additional features about the status of junctions or the number of moves since a stone was played. multiple such spatial m"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p418#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 418, "snippet": "402 chapter 9. deep reinforcement learning value networks this network was also a convolutional neural network, which uses the state of the network as"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p419#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 419, "snippet": "9. 7. case studies 403 learning bootstraps state values, this approach bootstraps visit counts for learning policies. the predicted probability of mon"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p420#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 420, "snippet": "404 chapter 9. deep reinforcement learning which would sometimes make sense only in hindsight after the victory of the program [ 607, 608 ]. there wer"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p421#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 421, "snippet": "9. 7. case studies 405 biomechanics, graphics, and animation, where fast and accurate simulation is needed with - out having to construct an actual ro"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p422#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 422, "snippet": "406 chapter 9. deep reinforcement learning 9. 7. 2. 2 deep learning of visuomotor skills a second and interesting case of reinforcement learning is pr"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p423#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 423, "snippet": "9. 7. case studies 407 referred to as a feature point. note that each spatial feature map in the convolution layer creates a feature point. the featur"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p424#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 424, "snippet": "408 chapter 9. deep reinforcement learning in the following, we will describe a system built by facebook for end - to - end learning of negotiation sk"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p425#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 425, "snippet": "9. 7. case studies 409 agent a : i want the books and the hats, you get the ball. a g e n tb : g i v em eab o o kt o oa n dw eh a v ead e a l. agent a"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p426#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 426, "snippet": "410 chapter 9. deep reinforcement learning a number of interesting observations were made in [ 290 ] about the performance of the approach. first, the"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p427#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 427, "snippet": "9. 7. case studies 411 figure 9. 10 : the neural network architecture of the control system in the self - driving car discussed in [ 46 ] ( courtesy n"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p428#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 428, "snippet": "412 chapter 9. deep reinforcement learning lutional layers were followed with three fully connected layers. the ﬁnal output value was a control value,"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p429#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 429, "snippet": "9. 8. practical challenges associated with safety 413 the choice of a recurrent network is motivated by the sequential dependence between dif - ferent"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p430#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 430, "snippet": "414 chapter 9. deep reinforcement learning driver or two pedestrians? most humans would save themselves in this setting as a matter of reﬂexive biolog"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p431#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 431, "snippet": "9. 10. bibliographic notes 415 of correlation within a thread, which improves convergence to higher - quality solutions. this type of asynchronous app"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p432#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 432, "snippet": "416 chapter 9. deep reinforcement learning reinforcement learning can also improve deep learning models. this is achieved with the notion of attention"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p433#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 433, "snippet": "9. 11. exercises 417 4. consider the well - known game of rock - paper - scissors. human players often try to use the history of previous moves to gue"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p434#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 434, "snippet": "chapter 10 advanced topics in deep learning “ instead of trying to produce a program to simulate the adult mind, why not rather try to produce one whi"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p435#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 435, "snippet": "420 chapter 10. advanced topics in deep learning it is so tightly integrated with the computations that it is hard to separate data access from comput"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p436#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 436, "snippet": "10. 2. attention mechanisms 421 fovea macula retina ( not drawn to scale ) maximum density of receptors least density of receptors figure 10. 1 : reso"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p437#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 437, "snippet": "422 chapter 10. advanced topics in deep learning part of a house, is there a way of systematically identifying the numbers corresponding to the street"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p438#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 438, "snippet": "10. 2. attention mechanisms 423 glimpse sensor glimpse network lt - 1 lt - 1 gt ρ ( xt, lt - 1 ) ρ ( xt, lt - 1 ) ht - 1 lt - 1 ltat gt ht glimpse net"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p439#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 439, "snippet": "424 chapter 10. advanced topics in deep learning architecture of the neural network is illustrated on the right - hand side of figure 10. 2. note that"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p440#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 440, "snippet": "10. 2. attention mechanisms 425 network should receive a representation of the image in which the attention is on a speciﬁc location. furthermore, as "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p441#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 441, "snippet": "426 chapter 10. advanced topics in deep learning we focus on a method proposed in luong et al. [ 302 ], which is an improvement over the original mech"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p442#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 442, "snippet": "10. 2. attention mechanisms 427 once this new hidden representation h ( 2 ) t is created, it is used in lieu of the original hidden representation h ("}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p443#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 443, "snippet": "428 chapter 10. advanced topics in deep learning these attention values are used in the same way as in the case of dot product similarity. the paramet"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p444#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 444, "snippet": "10. 3. neural networks with external memory 429 uni - directional recurrent neural network, whereas that in bahdanau et al. emphasizes the use of a bi"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p445#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 445, "snippet": "430 chapter 10. advanced topics in deep learning 3 2 1 4 1 2 3 4 location id value at location id 1 ( a ) output screen neural network observed state "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p446#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 446, "snippet": "10. 3. neural networks with external memory 431 this setting is almost identical to the atari video game setting discussed in chapter 9. for example, "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p447#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 447, "snippet": "432 chapter 10. advanced topics in deep learning external input external output controller read heads write heads memory figure 10. 6 : the neural tur"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p448#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 448, "snippet": "10. 3. neural networks with external memory 433 the environment. furthermore, it has an external memory to which it can read and write with the use of"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p449#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 449, "snippet": "434 chapter 10. advanced topics in deep learning if multiple write heads are present, then the order of the addition operations does not matter. howev"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p450#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 450, "snippet": "10. 3. neural networks with external memory 435 sequential access. finally, the softness of the addressing is sharpened with a temperature - like para"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p451#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 451, "snippet": "436 chapter 10. advanced topics in deep learning external memory in the neural turing machine. the states within the neural network are like cpu regis"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p452#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 452, "snippet": "10. 3. neural networks with external memory 437 in addition, the neural turing machine was experimentally shown to be good at the task of associative "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p453#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 453, "snippet": "438 chapter 10. advanced topics in deep learning written to at the tth time - stamp. this precedence relation is used to update the temporal link matr"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p454#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 454, "snippet": "10. 4. generative adversarial networks ( gans ) 439 real and fake examples of car images. the second network is a discriminative network that has been"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p455#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 455, "snippet": "440 chapter 10. advanced topics in deep learning the goal for the discriminator is to correctly classify the real examples to a label of 1, and the sy"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p456#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 456, "snippet": "10. 4. generative adversarial networks ( gans ) 441 sample noise from prior distribution ( e. g., gaussian ) to create m samples synthetic samples cod"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p457#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 457, "snippet": "442 chapter 10. advanced topics in deep learning reason that the training of the generator and discriminator are done simultaneously with interleaving"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p458#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 458, "snippet": "10. 4. generative adversarial networks ( gans ) 443 ( a ) convolution architecture of dcgan ( b ) smooth image transitions caused by changing input no"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p459#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 459, "snippet": "444 chapter 10. advanced topics in deep learning t h ew o r ki n [ 384 ] starts with 100 - dimensional gaussian noise, which is the starting point of "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p460#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 460, "snippet": "10. 4. generative adversarial networks ( gans ) 445 artist sketch synthetic samples fusion decoder as generator noise generator conditional input enco"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p461#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 461, "snippet": "446 chapter 10. advanced topics in deep learning the context is more complex than the target output, this universe of target objects tends to shrink, "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p462#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 462, "snippet": "10. 4. generative adversarial networks ( gans ) 447 4. the base object might be a photograph or video in black and white ( e. g., classic movie ), and"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p463#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 463, "snippet": "448 chapter 10. advanced topics in deep learning synthetic samples fusion decoder as generator neural network with single probabilistic output ( e. g."}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p464#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 464, "snippet": "10. 5. competitive learning 449 10. 5 competitive learning most of the learning methods discussed in this book are based on updating the weights in th"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p465#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 465, "snippet": "450 chapter 10. advanced topics in deep learning 10. 5. 1 vector quantization vector quantization is the simplest application of competitive learning."}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p466#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 466, "snippet": "10. 5. competitive learning 451 wk wi wj k i j wk k i jwjwi ( a ) rectangular ( b ) hexagonal figure 10. 11 : an example of a 5 × 5 lattice structure "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p467#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 467, "snippet": "452 chapter 10. advanced topics in deep learning instead of the smooth gaussian damping function, one can use a thresholded step kernel, which takes o"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p468#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 468, "snippet": "10. 6. limitations of neural networks 453 music artsliterature drama arts music drama literature ( a ) rectangular lattice ( b ) hexagonal lattice fig"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p469#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 469, "snippet": "454 chapter 10. advanced topics in deep learning to new settings as compared to artiﬁcial neural networks. the general principle of being able to lear"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p470#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 470, "snippet": "10. 6. limitations of neural networks 455 e a r l yw o r ko no n e - s h o tl e a r n i n g [ 116 ] used bayesian frameworks in order to transfer the "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p471#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 471, "snippet": "456 chapter 10. advanced topics in deep learning with far fewer connections. furthermore, the dead neurons that have zero input connections and output"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p472#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 472, "snippet": "10. 8. bibliographic notes 457 10. 8 bibliographic notes early techniques for using attention in neural network training were proposed in [ 59, 266 ]."}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p473#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 473, "snippet": "458 chapter 10. advanced topics in deep learning may be found in [ 249, 250 ]. many variants of this basic architecture, such as neural gas, a r e use"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p474#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 474, "snippet": "bibliography [ 1 ] d. ackley, g. hinton, and t. sejnowski. a learning algorithm for boltzmann machines. cognitive science, 9 ( 1 ), pp. 147 – 169, 198"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p475#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 475, "snippet": "460 bibliography [ 14 ] j. ba, j. kiros, and g. hinton. layer normalization. arxiv : 1607. 06450, 2016. https : / / arxiv. org / abs / 1607. 06450 [ 1"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p476#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 476, "snippet": "bibliography 461 [ 31 ] y. bengio, p. lamblin, d. popovici, and h. larochelle. greedy layer - wise training of deep networks. nips conference, 19, 153"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p477#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 477, "snippet": "462 bibliography [ 52 ] c. browne et al. a survey of monte carlo tree search methods. ieee transactions on com - putational intelligence and ai in gam"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p478#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 478, "snippet": "bibliography 463 [ 71 ] j. chung, c. gulcehre, k. cho, and y. bengio. empirical evaluation of gated recurrent neural networks on sequence modeling. ar"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p479#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 479, "snippet": "464 bibliography [ 88 ] y. dauphin, r. pascanu, c. gulcehre, k. cho, s. ganguli, and y. bengio. identifying and attacking the saddle point problem in "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p480#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 480, "snippet": "bibliography 465 [ 107 ] h. drucker and y. lecun. improving generalization performance using double backpropaga - tion. ieee transactions on neural ne"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p481#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 481, "snippet": "466 bibliography [ 126 ] b. fritzke. a growing neural gas network learns topologies. nips conference, pp. 625 – 632, 1995. [ 127 ] k. fukushima. neoco"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p482#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 482, "snippet": "bibliography 467 [ 144 ] c. goller and a. k¨ uchler. learning task - dependent distributed representations by backprop - agation through structure. ne"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p483#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 483, "snippet": "468 bibliography [ 162 ] i. grondman, l. busoniu, g. a. lopes, and r. babuska. a survey of actor - critic reinforcement learning : standard and natura"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p484#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 484, "snippet": "bibliography 469 [ 180 ] m. havaei et al. brain tumor segmentation with deep neural networks. medical image anal - ysis, 35, pp. 18 – 31, 2017. [ 181 "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p485#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 485, "snippet": "470 bibliography [ 198 ] g. hinton and r. salakhutdinov. reducing the dimensionality of data with neural networks. science, 313, ( 5766 ), pp. 504 – 5"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p486#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 486, "snippet": "bibliography 471 [ 215 ] p. isola, j. zhu, t. zhou, and a. efros. image - to - image translation with conditional adver - sarial networks. arxiv : 161"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p487#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 487, "snippet": "472 bibliography [ 233 ] a. karpathy, j. johnson, and l. fei - fei. visualizing and understanding recurrent networks. arxiv : 1506. 02078, 2015. https"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p488#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 488, "snippet": "bibliography 473 [ 252 ] e. kong and t. dietterich. error - correcting output coding corrects bias and variance. icml conference, pp. 313 – 321, 1995."}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p489#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 489, "snippet": "474 bibliography [ 270 ] q. le et al. building high - level features using large scale unsupervised learning. icassp, 2013. [ 271 ] q. le, n. jaitly, "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p490#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 490, "snippet": "bibliography 475 [ 289 ] w. levy and r. baxter. energy neural codes. neural computation, 8 ( 3 ), pp. 531 – 543, 1996. [ 290 ] m. lewis, d. yarats, y."}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p491#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 491, "snippet": "476 bibliography [ 306 ] d. j. mackay. a practical bayesian framework for backpropagation networks. neural com - putation, 4 ( 3 ), pp. 448 – 472, 199"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p492#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 492, "snippet": "bibliography 477 [ 324 ] t. mikolov. statistical language models based on neural networks. ph. d. thesis, brno uni - versity of technology, 2012. [ 32"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p493#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 493, "snippet": "478 bibliography [ 342 ] j. moody and c. darken. fast learning in networks of locally - tuned processing units. neural computation, 1 ( 2 ), pp. 281 –"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p494#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 494, "snippet": "bibliography 479 [ 361 ] m. oquab, l. bottou, i. laptev, and j. sivic. learning and transferring mid - level image representations using convolutional"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p495#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 495, "snippet": "480 bibliography [ 381 ] d. pomerleau. alvinn, an autonomous land vehicle in a neural network. technical report, carnegie mellon university, 1989. [ 3"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p496#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 496, "snippet": "bibliography 481 [ 399 ] d. rezende, s. mohamed, and d. wierstra. stochastic backpropagation and approximate inference in deep generative models. arxi"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p497#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 497, "snippet": "482 bibliography [ 417 ] r. salakhutdinov and g. hinton. deep boltzmann machines. artiﬁcial intelligence and statis - tics, pp. 448 – 455, 2009. [ 418"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p498#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 498, "snippet": "bibliography 483 [ 436 ] s. sedhain, a. k. menon, s. sanner, and l. xie. autorec : autoencoders meet collaborative ﬁltering. www conference, pp. 111 –"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p499#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 499, "snippet": "484 bibliography [ 454 ] k. simonyan and a. zisserman. very deep convolutional networks for large - scale image recog - nition. arxiv : 1409. 1556, 20"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p500#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 500, "snippet": "bibliography 485 [ 471 ] a. storkey. increasing the capacity of a hopﬁeld network without sacriﬁcing functionality. artiﬁcial neural networks, pp. 451"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p501#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 501, "snippet": "486 bibliography [ 490 ] c. thornton, f. hutter, h. h. hoos, and k. leyton - brown. auto - weka : combined selec - tion and hyperparameter optimizatio"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p502#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 502, "snippet": "bibliography 487 [ 510 ] j. walker, c. doersch, a. gupta, and m. hebert. an uncertain future : forecasting from static images using variational autoen"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p503#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 503, "snippet": "488 bibliography [ 529 ] j. weston and c. watkins. multi - class support vector machines. technical report csd - tr - 98 - 04, department of computer "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p504#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 504, "snippet": "bibliography 489 [ 547 ] w. yu, w. cheng, c. aggarwal, k. zhang, h. chen, and wei wang. netwalk : a ﬂexible deep embedding approach for anomaly detect"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p505#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 505, "snippet": "490 bibliography [ 564 ] c. zhou and r.. anomaly detection with robust deep autoencoders. acm kdd conference, pp. 665 – 674, 2017. [ 565 ] m. zhou, z."}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p506#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 506, "snippet": "bibliography 491 [ 592 ] https : / / www. tensorﬂow. org / tutorials / word2vec / [ 593 ] https : / / github. com / aditya - grover / node2vec [ 594 ]"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p507#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 507, "snippet": "492 bibliography [ 622 ] https : / / github. com / facebookresearch / parlai [ 623 ] https : / / github. com / openai / baselines [ 624 ] https : / / "}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p508#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 508, "snippet": "index l1 - regularization, 183 l2 - regularization, 182 o - greedy algorithm, 376 t - sne, 80 adadelta algorithm, 139 adagrad, 138 adaline, 60 adam al"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p509#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 509, "snippet": "494 index cudnn, 158 curriculum learning, 199 data augmentation, 337 data parallelism, 159 dcgan, 442 de - noising autoencoder, 82, 202 deconvolution,"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p510#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 510, "snippet": "index 495 keras, 50 kernel matrix factorization, 77 kernels for convolution, 319 kohonen self - organizing map, 450 l - bfgs, 148, 149, 164 ladder net"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p511#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 511, "snippet": "496 index q - network, 384 quasi - newton methods, 148 question answering, 301 radial basis function network, 37, 217 rbf network, 37, 217 rbm, 247 re"}
{"id": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf#p512#c1", "source": "DGM/Books/Charu Agarwal 2018_Book_NeuralNetworksAndDeepLearning.pdf", "page": 512, "snippet": "index 497 visual attention, 422 visualization, 80 weight scaling inference rule, 190 weston - watkins svm, 67 whitening, 127 widrow - learning, 59 win"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p1#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 1, "snippet": "manning francois chollet"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p2#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 2, "snippet": "deep learning with python licensed to < null >"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p3#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 3, "snippet": "licensed to < null >"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p4#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 4, "snippet": "deep learning with python francois chollet manning shelter island licensed to < null >"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p5#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 5, "snippet": "for online information and ordering of this and other manning books, please visit www. manning. com. the publisher offers discounts on this book when "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p6#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 6, "snippet": "v brief contents part 1f undamentals of deep learning.................................. 1 1 ■ what is deep learning? 3 2 ■ before we begin : the mathe"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p7#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 7, "snippet": "licensed to < null >"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p8#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 8, "snippet": "vii contents preface xiii acknowledgments xv about this book xvi about the author xx about the cover xxi part 1f undamentals of deep learning........."}
{"id": "DGM/Books/deeplearningwithpython.pdf#p9#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 9, "snippet": "contentsviii 1. 3 why deep learning? why now? 20 hardware 20 ■ data 21 ■ algorithms 21 ■ a new wave of investment 22 ■ the democratization of deep lea"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p10#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 10, "snippet": "contents ix running deep - learning jobs in the cloud : pros and cons 66 what is the best gpu for deep learning? 66 3. 4 classifying movie reviews : a"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p11#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 11, "snippet": "contentsx evaluation protocol 112 ■ preparing your data 112 developing a model that does better than a baseline 113 scaling up : developing a model th"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p12#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 12, "snippet": "contents xi to fight overfitting 216 ■ stacking recurrent layers 217 using bidirectional rnns 219 ■ going even further 222 wrapping up 223 6. 4 sequen"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p13#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 13, "snippet": "contentsxii 8. 4 generating images with variational autoencoders 296 sampling from latent spaces of images 296 ■ concept vectors for image editing 297"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p14#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 14, "snippet": "xiii preface if you ’ ve picked up this book, you ’ re pr obably aware of the extraordinary progress that deep learning has represented for the fi eld"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p15#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 15, "snippet": "prefacexiv simultaneously cover fundamentals of deep learning, keras usage patterns, and deep - learning best practices. this book is my best effort t"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p16#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 16, "snippet": "xv acknowledgments i ’ d like to thank the keras community for making this book possible. keras has grown to have hundreds of open source contributors"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p17#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 17, "snippet": "xvi about this book this book was written for anyone who wishes to explore deep learning from scratch or broaden their understanding of deep learning."}
{"id": "DGM/Books/deeplearningwithpython.pdf#p18#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 18, "snippet": "about this book xvii who should read this book this book is written for people with python programming experience who want to get started with machine"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p19#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 19, "snippet": "about this bookxviii networks to handle classification and re gression tasks, and you ’ ll have a solid idea of what ’ s happening in the background a"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p20#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 20, "snippet": "about this book xix book forum purchase of deep learning with python includes free access to a private web forum run by manning publications where you"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p21#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 21, "snippet": "xx about the author francois chollet works on deep learning at google in moun - tain view, ca. he is the creator of the keras deep - learning library,"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p22#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 22, "snippet": "xxi about the cover the figure on the cover of deep learning with python is captioned “ habit of a persian lady in 1568. ” the illustration is taken f"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p23#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 23, "snippet": "licensed to < null >"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p24#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 24, "snippet": "part 1 fundamentals of deep learning chapters 1 – 4 of this book will give you a foundational understanding of what deep learning is, what it can achi"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p25#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 25, "snippet": "licensed to < null >"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p26#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 26, "snippet": "3 what is deep learning? in the past few years, artificial intelligence ( ai ) has been a subject of intense media hype. machine learning, deep learni"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p27#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 27, "snippet": "4 chapter 1 what is deep learning? 1. 1 artificial intelligen ce, machine learning, and deep learning first, we need to define clearly what we ’ re ta"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p28#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 28, "snippet": "5artificial intelligence, machine learning, and deep learning engine wasn ’ t meant as a general - purpose computer when it was designed in the 1830s "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p29#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 29, "snippet": "6 chapter 1 what is deep learning? although machine learning only started to flourish in the 1990s, it has quickly become the most popular and mo st s"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p30#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 30, "snippet": "7artificial intelligence, machine learning, and deep learning let ’ s make this concrete. consider an x - axis, a y - axis, and some points represente"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p31#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 31, "snippet": "8 chapter 1 what is deep learning? finding these transformations ; they ’ re mere ly searching through a predefined set of operations, called a hypoth"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p32#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 32, "snippet": "9artificial intelligence, machine learning, and deep learning what do the representations learned by a deep - learning algorithm look like? let ’ s ex"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p33#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 33, "snippet": "10 chapter 1 what is deep learning? mapping via a deep sequence of simple data transformations ( layers ) and that these data transformations are lear"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p34#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 34, "snippet": "11artificial intelligence, machine learning, and deep learning the fundamental trick in deep learning is to use this score as a feedback signal to adj"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p35#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 35, "snippet": "12 chapter 1 what is deep learning? improved text - to - speech conversion digital assistants such as google now and amazon alexa near - human - level"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p36#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 36, "snippet": "13artificial intelligence, machine learning, and deep learning we may be currently witnessing the third cycle of ai hype and disappointment — and we ’"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p37#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 37, "snippet": "14 chapter 1 what is deep learning? 1. 2 before deep learning : a brief history of machine learning deep learning has reached a level of pub lic atten"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p38#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 38, "snippet": "15before deep learning : a brief history of machine learning when multiple people independently rediscovered the backpropagation algorithm — a way to "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p39#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 39, "snippet": "16 chapter 1 what is deep learning? hyperplanes in the ne w representation space, you don ’ t have to explicitly compute the coordinates of your point"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p40#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 40, "snippet": "17before deep learning : a brief history of machine learning uses gradient boosting, a way to improve any machine - learning model by iteratively trai"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p41#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 41, "snippet": "18 chapter 1 what is deep learning? problem - solving much easier, because it completely automates what used to be the most crucial step in a machine "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p42#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 42, "snippet": "19before deep learning : a brief history of machine learning in 2016 and 2017, kaggle was dominate d by two approaches : gradient boosting machines an"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p43#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 43, "snippet": "20 chapter 1 what is deep learning? 1. 3 why deep learning? why now? the two key ideas of deep learning for co mputer vision — convolutional neural ne"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p44#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 44, "snippet": "21why deep learning? why now? what happened is that the gaming market subsidized supercomputing for the next generation of artificial intelligence ap "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p45#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 45, "snippet": "22 chapter 1 what is deep learning? using only one or two layers of representations ; thus, they weren ’ t able to shine against more - refined shallo"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p46#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 46, "snippet": "23why deep learning? why now? a s a r e s u l t o f t h i s w a v e o f i n v e s t m e n t, t h e n u m b e r o f p e o p l e w o r k i n g o n d e e"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p47#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 47, "snippet": "24 chapter 1 what is deep learning? complex and powerful mode ls. this also makes deep learning applicable to fairly small datasets. deep learning has"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p48#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 48, "snippet": "25 before we begin : the mathematical building blocks of neural networks understanding deep learning requires familiarity with many simple mathematica"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p49#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 49, "snippet": "26 chapter 2 before we begin : the mathematical building blocks of neural networks that ’ s been introduced, point by point. keep in mind that these c"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p50#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 50, "snippet": "27a first look at a neural network 2. 1 a first look at a neural network let ’ s look at a concrete example of a neural network that uses the python l"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p51#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 51, "snippet": "28 chapter 2 before we begin : the mathematical building blocks of neural networks the images are encoded as numpy arrays, and the labels are an array"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p52#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 52, "snippet": "29a first look at a neural network to make the network ready for training, we need to pick three more things, as part of the compilation step : a loss"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p53#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 53, "snippet": "30 chapter 2 before we begin : the mathematical building blocks of neural networks two quantities are displayed during training : the loss of the netw"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p54#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 54, "snippet": "31data representations for neural networks 2. 2 data representations for neural networks in the previous example, we started from data stored in multi"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p55#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 55, "snippet": "32 chapter 2 before we begin : the mathematical building blocks of neural networks > > > x = np. array ( [ [ 5, 78, 2, 34, 0 ], [ 6, 79, 3, 35, 1 ], ["}
{"id": "DGM/Books/deeplearningwithpython.pdf#p56#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 56, "snippet": "33data representations for neural networks to make this more concrete, let ’ s look back at the data we processed in the mnist example. first, we load"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p57#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 57, "snippet": "34 chapter 2 before we begin : the mathematical building blocks of neural networks 2. 2. 6 manipulating tensors in numpy in the previous example, we s"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p58#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 58, "snippet": "35data representations for neural networks when considering such a batch tensor, the first axis ( axis 0 ) is called the batch axis or batch dimension"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p59#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 59, "snippet": "36 chapter 2 before we begin : the mathematical building blocks of neural networks the time axis is always the second axis ( axis of index 1 ), by con"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p60#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 60, "snippet": "37data representations for neural networks the theano convention, the previous examples would become ( 128, 1, 256, 256 ) and ( 128, 3, 256, 256 ). th"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p61#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 61, "snippet": "38 chapter 2 before we begin : the mathematical building blocks of neural networks 2. 3 the gears of neural ne tworks : tensor operations much as any "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p62#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 62, "snippet": "39the gears of neural networks : tensor operations you do the same for addition : def naive _ add ( x, y ) : assert len ( x. shape ) = = 2 assert x. s"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p63#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 63, "snippet": "40 chapter 2 before we begin : the mathematical building blocks of neural networks repeated 10 times alongside a new axis is a helpful mental model. h"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p64#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 64, "snippet": "41the gears of neural networks : tensor operations z = 0. for i in range ( x. shape [ 0 ] ) : z + = x [ i ] * y [ i ] return z you ’ ll have noticed t"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p65#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 65, "snippet": "42 chapter 2 before we begin : the mathematical building blocks of neural networks to understand dot - product shape compatibility, it helps to visual"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p66#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 66, "snippet": "43the gears of neural networks : tensor operations > > > x = x. reshape ( ( 6, 1 ) ) > > > x array ( [ [ 0. ], [ 1. ], [ 2. ], [ 3. ], [ 4. ], [ 5. ] "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p67#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 67, "snippet": "44 chapter 2 before we begin : the mathematical building blocks of neural networks let ’ s consider a new point, b = [ 1, 0. 25 ], which we ’ ll add t"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p68#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 68, "snippet": "45the gears of neural networks : tensor operations uncrumpling paper balls is what machine lear ning is about : find ing neat representa - tions for c"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p69#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 69, "snippet": "46 chapter 2 before we begin : the mathematical building blocks of neural networks 2. 4 the engine of neural networks : gradient - based optimization "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p70#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 70, "snippet": "47the engine of neural networks : gradient - based optimization would contribute to minimizing the loss. this would have to be repeated for all coeffi"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p71#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 71, "snippet": "48 chapter 2 before we begin : the mathematical building blocks of neural networks points. for instance, the derivative of cos ( x ) is - sin ( x ), t"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p72#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 72, "snippet": "49the engine of neural networks : gradient - based optimization applied to a neural network, that mean s finding analytically the combination of weigh"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p73#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 73, "snippet": "50 chapter 2 before we begin : the mathematical building blocks of neural networks as you can see, intuitively it ’ s important to pick a reasonable v"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p74#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 74, "snippet": "51the engine of neural networks : gradient - based optimization as you can see, around a certai n parameter value, there is a local minimum : around t"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p75#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 75, "snippet": "52 chapter 2 before we begin : the mathematical building blocks of neural networks called backpropagation ( also sometimes called reverse - mode diffe"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p76#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 76, "snippet": "53looking back at our first example 2. 5 looking back at our first example you ’ ve reached the end of this chapter, and you should now have a general"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p77#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 77, "snippet": "54 chapter 2 before we begin : the mathematical building blocks of neural networks accordingly. after these 5 epochs, the network will have performed "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p78#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 78, "snippet": "55looking back at our first example chapter summary learning means finding a combination of model parameters that mini - mizes a loss function for a g"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p79#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 79, "snippet": "56 getting started with neural networks this chapter is designed to get you starte d with using neural networks to solve real problems. you ’ ll conso"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p80#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 80, "snippet": "57 tensorflow, keras, and gpu support. we ’ ll dive into three introductory examples of how to use neural networks to address real problems : classify"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p81#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 81, "snippet": "58 chapter 3 getting started with neural networks 3. 1 anatomy of a neural network as you saw in the previous chapters, training a neural network revo"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p82#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 82, "snippet": "59anatomy of a neural network you can think of layers as the lego bricks of deep learning, a metaphor that is made explicit by frameworks like keras. "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p83#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 83, "snippet": "60 chapter 3 getting started with neural networks picking the right network architecture is more an art than a science ; and although there are some b"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p84#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 84, "snippet": "61introduction to keras 3. 2 introduction to keras throughout this book, the code examples use keras ( https : / / keras. io ). keras is a deep - lear"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p85#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 85, "snippet": "62 chapter 3 getting started with neural networks 3. 2. 1 keras, tensorflow, theano, and cntk keras is a model - level library, providing high - level"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p86#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 86, "snippet": "63introduction to keras 3 configure the learning process by choos ing a loss function, an optimizer, and some metrics to monitor. 4 iterate on your tr"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p87#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 87, "snippet": "64 chapter 3 getting started with neural networks over the next few chapters, you ’ ll build a solid intuition about what type of network architecture"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p88#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 88, "snippet": "65setting up a deep - learning workstation 3. 3 setting up a deep - learning workstation before you can get started developing deep - learning applica"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p89#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 89, "snippet": "66 chapter 3 getting started with neural networks 3. 3. 2 getting keras running : two options to get started in practice, we recommend one of the foll"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p90#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 90, "snippet": "67setting up a deep - learning workstation from this section onward, we ’ ll assume that you ha ve access to a machine with keras and its dependencies"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p91#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 91, "snippet": "68 chapter 3 getting started with neural networks 3. 4 classifying movie reviews : a binary classification example two - class classification, or bina"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p92#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 92, "snippet": "69classifying movie reviews : a binary classification example because you ’ re restricting yourself to th e top 10, 000 most freq uent words, no word "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p93#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 93, "snippet": "70 chapter 3 getting started with neural networks here ’ s what the samples look like now : > > > x _ train [ 0 ] array ( [ 0., 1., 1.,..., 0., 0., 0."}
{"id": "DGM/Books/deeplearningwithpython.pdf#p94#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 94, "snippet": "71classifying movie reviews : a binary classification example indicating how likely the sample is to have the target “ 1 ” : how likely the review is "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p95#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 95, "snippet": "72 chapter 3 getting started with neural networks figure 3. 6 shows what the network looks li ke. and here ’ s the keras implementation, similar to th"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p96#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 96, "snippet": "73classifying movie reviews : a binary classification example binary _ crossentropy loss. it isn ’ t the only viable choice : you could use, for insta"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p97#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 97, "snippet": "74 chapter 3 getting started with neural networks y _ val = y _ train [ : 10000 ] partial _ y _ train = y _ train [ 10000 : ] you ’ ll now train the m"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p98#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 98, "snippet": "75classifying movie reviews : a binary classification example plt. clf ( ) acc _ values = history _ dict [ ' acc ' ] val _ acc _ values = history _ di"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p99#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 99, "snippet": "76 chapter 3 getting started with neural networks as you can see, the training loss decreases with every epoch, and the training accuracy increases wi"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p100#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 100, "snippet": "77classifying movie reviews : a binary classification example as you can see, the network is confident for some samples ( 0. 99 or more, or 0. 01 or l"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p101#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 101, "snippet": "78 chapter 3 getting started with neural networks 3. 5 classifying newswires : a multiclass classification example in the previous section, you saw ho"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p102#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 102, "snippet": "79classifying newswires : a multiclass classification example the label associated with an example is an integer between 0 and 45 — a topic index : > "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p103#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 103, "snippet": "80 chapter 3 getting started with neural networks relevant to the classification problem, this information can never be recovered by later layers : ea"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p104#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 104, "snippet": "81classifying newswires : a multiclass classification example now, let ’ s train the network for 20 epochs. history = model. fit ( partial _ x _ train"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p105#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 105, "snippet": "82 chapter 3 getting started with neural networks the network begins to overfit after nine epochs. let ’ s train a new network from scratch for nine e"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p106#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 106, "snippet": "83classifying newswires : a multiclass classification example 3. 5. 5 generating predictions on new data you can verify that the predict method of the"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p107#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 107, "snippet": "84 chapter 3 getting started with neural networks model = models. sequential ( ) model. add ( layers. dense ( 64, activation = ' relu ', input _ shape"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p108#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 108, "snippet": "85predicting house prices : a regression example 3. 6 predicting house prices : a regression example the two previous examples we re considered classi"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p109#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 109, "snippet": "86 chapter 3 getting started with neural networks 3. 6. 2 preparing the data it would be problematic to feed into a neural network values that all tak"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p110#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 110, "snippet": "87predicting house prices : a regression example note that you compile the network with the mse loss function — mean squared error, the square of the "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p111#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 111, "snippet": "88 chapter 3 getting started with neural networks for i in range ( k ) : print ( ' processing fold # ', i ) val _ data = train _ data [ i * num _ val "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p112#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 112, "snippet": "89predicting house prices : a regression example partial _ train _ targets = np. concatenate ( [ train _ targets [ : i * num _ val _ samples ], train "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p113#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 113, "snippet": "90 chapter 3 getting started with neural networks the result is shown in figure 3. 13. def smooth _ curve ( points, factor = 0. 9 ) : smoothed _ point"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p114#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 114, "snippet": "91predicting house prices : a regression example here ’ s the final result : > > > test _ mae _ score 2. 5532484335057877 you ’ re still off by about "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p115#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 115, "snippet": "92 chapter 3 getting started with neural networks chapter summary you ’ re now able to handle the mo st common kinds of machine - learning tasks on ve"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p116#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 116, "snippet": "93 fundamentals of machine learning after the three practical examples in chapter 3, you should be starting to feel famil - iar with how to approach c"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p117#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 117, "snippet": "94 chapter 4 fundamentals of machine learning 4. 1 four branches of machine learning in our previous examples, you ’ ve become familiar with three spe"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p118#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 118, "snippet": "95four branches of machine learning human - annotated labels — you can think of it as supervised learning without any humans in the loop. there are st"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p119#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 119, "snippet": "96 chapter 4 fundamentals of machine learning ( continued ) prediction error or loss value — a measure of the distance between your model ’ s predicti"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p120#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 120, "snippet": "97evaluating machine - learning models 4. 2 evaluating machine - learning models in the three examples presented in chapter 3, we split the data into "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p121#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 121, "snippet": "98 chapter 4 fundamentals of machine learning if anything about the model has been tuned based on test set performance, then your measure of generaliz"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p122#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 122, "snippet": "99evaluating machine - learning models this is the simplest evaluation protocol, an d it suffers from one flaw : if little data is available, then you"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p123#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 123, "snippet": "100 chapter 4 fundamentals of machine learning validation _ score = np. average ( validation _ scores ) model = get _ model ( ) model. train ( data ) "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p124#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 124, "snippet": "101data preprocessing, feature engineering, and feature learning 4. 3 data preprocessing, feature engineering, and feature learning in addition to mod"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p125#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 125, "snippet": "102 chapter 4 fundamentals of machine learning additionally, the following stricter normal ization practice is common and can help, although it isn ’ "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p126#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 126, "snippet": "103data preprocessing, feature engineering, and feature learning if you choose to use the raw pixels of the image as input data, then you have a diffi"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p127#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 127, "snippet": "104 chapter 4 fundamentals of machine learning 4. 4 overfitting and underfitting in all three examples in the previous chapter — predicting movie revi"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p128#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 128, "snippet": "105overfitting and underfitting we ’ d need only 10 binary parameters for each of the 50, 000 digits. but such a model would be useless for cl assifyi"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p129#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 129, "snippet": "106 chapter 4 fundamentals of machine learning as you can see, the smaller network starts overfitting later than the reference network ( after six epo"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p130#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 130, "snippet": "107overfitting and underfitting the bigger network starts over fitting almost immediately, after just one epoch, and it overfits much more severely. i"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p131#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 131, "snippet": "108 chapter 4 fundamentals of machine learning in keras, weight regulariza tion is added by passing weight regularizer instances to layers as keyword "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p132#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 132, "snippet": "109overfitting and underfitting 4. 4. 3 adding dropout dropout is one of the most effective and mo st commonly used regularization tech - niques for n"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p133#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 133, "snippet": "110 chapter 4 fundamentals of machine learning i figured it must be becaus e it would require cooperation between employees to suc - cessfully defraud"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p134#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 134, "snippet": "111the universal workflow of machine learning 4. 5 the universal workfl ow of machine learning in this section, we ’ ll present a universal blueprint "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p135#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 135, "snippet": "112 chapter 4 fundamentals of machine learning keep in mind that machine learning can only be used to memorize patterns that are present in your train"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p136#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 136, "snippet": "113the universal workflow of machine learning if different features take values in di fferent ranges ( heterogeneous data ), then the data should be n"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p137#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 137, "snippet": "114 chapter 4 fundamentals of machine learning 4. 5. 6 scaling up : developing a model that overfits once you ’ ve obtained a model that has statistic"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p138#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 138, "snippet": "115the universal workflow of machine learning try different hyperparameter s ( such as the number of units per layer or the learning rate of the optim"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p139#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 139, "snippet": "116 chapter 4 fundamentals of machine learning chapter summary define the problem at hand and the data on which you ’ ll train. collect this data, or "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p140#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 140, "snippet": "part 2 deep learning in practice chapters 5 – 9 will help you gain practical intuition about how to solve real - world problems using deep learning, a"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p141#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 141, "snippet": "licensed to < null >"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p142#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 142, "snippet": "119 deep learning for computer vision this chapter introduces convolutiona l neural networks, also known as convnets, a type of deep - learning model "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p143#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 143, "snippet": "120 chapter 5 deep learning for computer vision 5. 1 introduction to convnets we ’ re about to dive into the theory of what convnets are and why they "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p144#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 144, "snippet": "121introduction to convnets as you go deeper in the netw ork. the number of channels is controlled by the first argument passed to the conv2d layers ("}
{"id": "DGM/Books/deeplearningwithpython.pdf#p145#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 145, "snippet": "122 chapter 5 deep learning for computer vision train _ images = train _ images. reshape ( ( 60000, 28, 28, 1 ) ) train _ images = train _ images. ast"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p146#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 146, "snippet": "123introduction to convnets this key characteristic gives convnets two interesting properties : the patterns they learn are translation invariant. aft"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p147#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 147, "snippet": "124 chapter 5 deep learning for computer vision different channels in that depth axis no longer stand for specific colors as in rgb input ; rather, th"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p148#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 148, "snippet": "125introduction to convnets note that the output width and height ma y differ from the input width and height. they may differ for two reasons : borde"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p149#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 149, "snippet": "126 chapter 5 deep learning for computer vision if you want to get an output feature map with the same spatial dimensions as the input, you can use pa"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p150#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 150, "snippet": "127introduction to convnets understanding convolution strides the other factor that can influence output size is the notion of strides. the descriptio"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p151#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 151, "snippet": "128 chapter 5 deep learning for computer vision stride 2, in order to downsample the feature maps by a factor of 2. on the other hand, convolution is "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p152#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 152, "snippet": "129introduction to convnets use average pooling instead of max pooling, where each local input patch is trans - formed by taking the average value of "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p153#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 153, "snippet": "130 chapter 5 deep learning for computer vision 5. 2 training a convnet from scratch on a small dataset having to train an image - classification mode"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p154#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 154, "snippet": "131training a convnet from scratch on a small dataset in the case of computer vision, many pretrained models ( usually trained on the image - net data"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p155#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 155, "snippet": "132 chapter 5 deep learning for computer vision following is the code to do this. import os, shutil original _ dataset _ dir = ' / users / fchollet / "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p156#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 156, "snippet": "133training a convnet from scratch on a small dataset fnames = [ ' dog. { }. jpg '. format ( i ) for i in range ( 1000 ) ] for fname in fnames : src ="}
{"id": "DGM/Books/deeplearningwithpython.pdf#p157#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 157, "snippet": "134 chapter 5 deep learning for computer vision note the depth of the feature maps prog ressively increases in the network ( from 32 to 128 ), whereas"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p158#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 158, "snippet": "135training a convnet from scratch on a small dataset dense _ 2 ( dense ) ( none, 1 ) 513 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p159#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 159, "snippet": "136 chapter 5 deep learning for computer vision target _ size = ( 150, 150 ), batch _ size = 20, class _ mode = ' binary ' ) let ’ s look at the outpu"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p160#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 160, "snippet": "137training a convnet from scratch on a small dataset steps _ per _ epoch gradient descent steps — the fitt ing process will go to the next epoch. in "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p161#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 161, "snippet": "138 chapter 5 deep learning for computer vision these plots are characteristic of overfitting. the training accuracy increases linearly over time, unt"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p162#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 162, "snippet": "139training a convnet from scratch on a small dataset would be exposed to every possible aspect of the data distribution at hand : you would never ove"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p163#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 163, "snippet": "140 chapter 5 deep learning for computer vision x = image. img _ to _ array ( img ) x = x. reshape ( ( 1, ) + x. shape ) i = 0 for batch in datagen. f"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p164#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 164, "snippet": "141training a convnet from scratch on a small dataset model = models. sequential ( ) model. add ( layers. conv2d ( 32, ( 3, 3 ), activation = ' relu '"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p165#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 165, "snippet": "142 chapter 5 deep learning for computer vision let ’ s save the model — you ’ ll use it in section 5. 4. model. save ( ' cats _ and _ dogs _ small _ "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p166#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 166, "snippet": "143using a pretrained convnet 5. 3 using a pretrained convnet a common and highly effective approach to deep learning on small image datasets is to us"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p167#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 167, "snippet": "144 chapter 5 deep learning for computer vision previously trained network, running the new data through it, and training a new clas - sifier on top o"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p168#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 168, "snippet": "145using a pretrained convnet in this case, because the imagenet class set contains multiple dog and cat classes, it ’ s likely to be beneficial to re"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p169#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 169, "snippet": "146 chapter 5 deep learning for computer vision _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p170#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 170, "snippet": "147using a pretrained convnet extending the model you have ( conv _ base ) by adding dense layers on top, and running the whole thing end to end on th"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p171#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 171, "snippet": "148 chapter 5 deep learning for computer vision train _ features = np. reshape ( train _ features, ( 2000, 4 * 4 * 512 ) ) validation _ features = np."}
{"id": "DGM/Books/deeplearningwithpython.pdf#p172#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 172, "snippet": "149using a pretrained convnet you reach a validation accuracy of about 90 % — much better than you achieved in the previous section with the small mod"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p173#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 173, "snippet": "150 chapter 5 deep learning for computer vision because models behave just like layers, you can add a model ( like conv _ base ) to a sequential model"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p174#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 174, "snippet": "151using a pretrained convnet with this setup, only the weights from the two dense layers that you added will be trained. that ’ s a total of four wei"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p175#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 175, "snippet": "152 chapter 5 deep learning for computer vision 5. 3. 2 fine - tuning another widely used technique for mo del reuse, complementary to feature extract"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p176#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 176, "snippet": "153using a pretrained convnet dense dense flatten maxpooling2d convolution2d convolution2d convolution2d maxpooling2d convolution2d convolution2d conv"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p177#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 177, "snippet": "154 chapter 5 deep learning for computer vision i stated earlier that it ’ s necessary to freeze the convolution base of vgg16 in order to be able to "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p178#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 178, "snippet": "155using a pretrained convnet _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p179#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 179, "snippet": "156 chapter 5 deep learning for computer vision model. compile ( loss = ' binary _ crossentropy ', optimizer = optimizers. rmsprop ( lr = 1e - 5 ), me"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p180#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 180, "snippet": "157using a pretrained convnet def smooth _ curve ( points, factor = 0. 8 ) : smoothed _ points = [ ] for point in points : if smoothed _ points : prev"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p181#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 181, "snippet": "158 chapter 5 deep learning for computer vision the validation accuracy curve look much cl eaner. you ’ re seeing a nice 1 % absolute improvement in a"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p182#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 182, "snippet": "159using a pretrained convnet 5. 3. 3 wrapping up here ’ s what you should take away from the exercises in the past two sections : convnets are the be"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p183#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 183, "snippet": "160 chapter 5 deep learning for computer vision 5. 4 visualizing what convnets learn it ’ s often said that deep - learning models are “ black boxes ”"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p184#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 184, "snippet": "161visualizing what convnets learn _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p185#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 185, "snippet": "162 chapter 5 deep learning for computer vision in order to extract the feature maps you want to look at, you ’ ll create a keras model that takes bat"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p186#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 186, "snippet": "163visualizing what convnets learn activations = activation _ model. predict ( img _ tensor ) for instance, this is the activation of the first convol"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p187#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 187, "snippet": "164 chapter 5 deep learning for computer vision this one looks like a “ bright green dot ” dete ctor, useful to encode cat eyes. at this point, let ’ "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p188#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 188, "snippet": "165visualizing what convnets learn figure 5. 27 every channel of every layer activation on the test cat picture licensed to < null >"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p189#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 189, "snippet": "166 chapter 5 deep learning for computer vision there are a few things to note here : the first layer acts as a coll ection of various edge detectors."}
{"id": "DGM/Books/deeplearningwithpython.pdf#p190#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 190, "snippet": "167visualizing what convnets learn 5. 4. 2 visualizing convnet filters another easy way to inspect the filters learne d by convnets is to display the "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p191#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 191, "snippet": "168 chapter 5 deep learning for computer vision a function that takes a numpy tensor ( as a list of tensors of size 1 ) and returns a list of two nump"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p192#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 192, "snippet": "169visualizing what convnets learn def generate _ pattern ( layer _ name, filter _ index, size = 150 ) : layer _ output = model. get _ layer ( layer _"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p193#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 193, "snippet": "170 chapter 5 deep learning for computer vision layer _ name = ' block1 _ conv1 ' size = 64 margin = 5 results = np. zeros ( ( 8 * size + 7 * margin, "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p194#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 194, "snippet": "171visualizing what convnets learn figure 5. 31 filter patterns for layer block2 _ conv1 figure 5. 32 filter patterns for layer block3 _ conv1 license"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p195#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 195, "snippet": "172 chapter 5 deep learning for computer vision these filter visualizations tell you a lot about how convnet layers see the world : each layer in a co"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p196#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 196, "snippet": "173visualizing what convnets learn respect to the class under consideration. for instance, given an image fed into a dogs - versus - cats convnet, cam"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p197#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 197, "snippet": "174 chapter 5 deep learning for computer vision from keras. preprocessing import image from keras. applications. vgg16 import preprocess _ input, deco"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p198#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 198, "snippet": "175visualizing what convnets learn grads = k. gradients ( african _ elephant _ output, last _ conv _ layer. output ) [ 0 ] pooled _ grads = k. mean ( "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p199#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 199, "snippet": "176 chapter 5 deep learning for computer vision finally, you ’ ll use opencv to generate an image that superimposes the original image on the heatmap "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p200#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 200, "snippet": "177visualizing what convnets learn chapter summary convnets are the best tool for attacking visual - classification problems. convnets work by learnin"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p201#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 201, "snippet": "178 deep learning for text and sequences this chapter explores deep - learning mode ls that can process text ( understood as sequences of word or sequ"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p202#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 202, "snippet": "179 sequence - to - sequence learning, such as decoding an english sentence into french sentiment analysis, such as classifying th e sentiment of twee"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p203#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 203, "snippet": "180 chapter 6 deep learning for text and sequences 6. 1 working with text data text is one of the most widespread forms of sequence data. it can be un"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p204#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 204, "snippet": "181working with text data 6. 1. 1 one - hot encoding of words and characters one - hot encoding is the most common, most basic way to turn a token int"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p205#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 205, "snippet": "182 chapter 6 deep learning for text and sequences import numpy as np samples = [ ' the cat sat on the mat. ', ' the dog ate my homework. ' ] token _ "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p206#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 206, "snippet": "183working with text data from keras. preprocessing. text import tokenizer samples = [ ' the cat sat on the mat. ', ' the dog ate my homework. ' ] tok"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p207#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 207, "snippet": "184 chapter 6 deep learning for text and sequences 6. 1. 2 using word embeddings another popular and powerful way to associate a vector with a word is"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p208#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 208, "snippet": "185working with text data learning word embeddings with the embedding layer the simplest way to associate a dense vector with a word is to choose the "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p209#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 209, "snippet": "186 chapter 6 deep learning for text and sequences it ’ s thus reasonable to learn a new embedding space wi th every new task. fortu - nately, backpro"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p210#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 210, "snippet": "187working with text data sequences ( 2d integer tensor ) into embedded sequences ( 3d float tensor ), flatten the tensor to 2d, and train a single de"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p211#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 211, "snippet": "188 chapter 6 deep learning for text and sequences using pretrained word embeddings sometimes, you have so litt le training data available that you ca"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p212#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 212, "snippet": "189working with text data downloading the imdb data as raw text first, head to http : / / mng. bz / 0tio and download the raw imdb dataset. uncompress"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p213#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 213, "snippet": "190 chapter 6 deep learning for text and sequences word _ index = tokenizer. word _ index print ( ' found % s unique tokens. ' % len ( word _ index ) "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p214#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 214, "snippet": "191working with text data embedding _ dim = 100 embedding _ matrix = np. zeros ( ( max _ words, embedding _ dim ) ) for word, i in word _ index. items"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p215#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 215, "snippet": "192 chapter 6 deep learning for text and sequences training and evaluating the model compile and train the model. model. compile ( optimizer = ' rmspr"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p216#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 216, "snippet": "193working with text data the model quickly starts overfitting, which is unsurprising given the small number of training samples. validation accuracy "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p217#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 217, "snippet": "194 chapter 6 deep learning for text and sequences validation accuracy stalls in the low 50s. so in this case, pretrained word embeddings outperform j"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p218#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 218, "snippet": "195working with text data f. close ( ) if label _ type = = ' neg ' : labels. append ( 0 ) else : labels. append ( 1 ) sequences = tokenizer. texts _ t"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p219#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 219, "snippet": "196 chapter 6 deep learning for text and sequences 6. 2 understanding recurrent neural networks a major characteristic of all neural networks you ’ ve"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p220#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 220, "snippet": "197understanding recurrent neural networks you can even flesh out the function f : the transformation of the input and state into an output will be pa"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p221#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 221, "snippet": "198 chapter 6 deep learning for text and sequences note in this example, the final outp ut is a 2d tensor of shape ( timesteps, output _ features ), w"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p222#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 222, "snippet": "199understanding recurrent neural networks _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p223#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 223, "snippet": "200 chapter 6 deep learning for text and sequences now, let ’ s use such a model on the imdb movie - review - classifica tion problem. first, preproce"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p224#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 224, "snippet": "201understanding recurrent neural networks plt. title ( ' training and validation accuracy ' ) plt. legend ( ) plt. figure ( ) plt. plot ( epochs, los"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p225#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 225, "snippet": "202 chapter 6 deep learning for text and sequences other types of recurrent layers perform much better. let ’ s l ook at some more - advanced layers. "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p226#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 226, "snippet": "203understanding recurrent neural networks let ’ s add to this picture an additional data flow that carries information across time - steps. call its "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p227#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 227, "snippet": "204 chapter 6 deep learning for text and sequences if you want to get philosophical, you can interpret what each of these operations is meant to do. f"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p228#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 228, "snippet": "205understanding recurrent neural networks defaults. keras has good defaults, and things will almost always “ just work ” without you having to spend "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p229#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 229, "snippet": "206 chapter 6 deep learning for text and sequences this time, you achieve up to 89 % validation accuracy. not bad : certainly much better than the sim"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p230#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 230, "snippet": "207advanced use of recurrent neural networks 6. 3 advanced use of recurrent neural networks in this section, we ’ ll revi ew three advanced techniques"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p231#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 231, "snippet": "208 chapter 6 deep learning for text and sequences import os data _ dir = ' / users / fchollet / downloads / jena _ climate ' fname = os. path. join ("}
{"id": "DGM/Books/deeplearningwithpython.pdf#p232#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 232, "snippet": "209advanced use of recurrent neural networks from matplotlib import pyplot as plt temp = float _ data [ :, 1 ] < 1 > temperature ( in degrees celsius "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p233#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 233, "snippet": "210 chapter 6 deep learning for text and sequences on this plot, you can see daily periodicity, especially evident for the last 4 days. also note that"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p234#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 234, "snippet": "211advanced use of recurrent neural networks data — the original array of floating - point data, which you normalized in listing 6. 32. lookback — how"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p235#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 235, "snippet": "212 chapter 6 deep learning for text and sequences train _ gen = generator ( float _ data, lookback = lookback, delay = delay, min _ index = 0, max _ "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p236#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 236, "snippet": "213advanced use of recurrent neural networks here ’ s the evaluation loop. def evaluate _ naive _ method ( ) : batch _ maes = [ ] for step in range ( "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p237#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 237, "snippet": "214 chapter 6 deep learning for text and sequences model. compile ( optimizer = rmsprop ( ), loss = ' mae ' ) history = model. fit _ generator ( train"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p238#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 238, "snippet": "215advanced use of recurrent neural networks solution with a space of complicated models, the simple, well - performing baseline may be unlearnable, e"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p239#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 239, "snippet": "216 chapter 6 deep learning for text and sequences the new validation mae of ~ 0. 265 ( before you start significantly overfitting ) translates to a m"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p240#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 240, "snippet": "217advanced use of recurrent neural networks and recurrent _ dropout, specifying the dropout rate of the recurrent units. let ’ s add dropout and recu"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p241#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 241, "snippet": "218 chapter 6 deep learning for text and sequences you ’ re already taking basic steps to mitiga te overfitting, such as using dropout ). as long as y"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p242#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 242, "snippet": "219advanced use of recurrent neural networks 6. 3. 8 using bidirectional rnns the last technique introduced in this section is called bidirectional rn"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p243#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 243, "snippet": "220 chapter 6 deep learning for text and sequences the reversed - order gru strongly underper forms even the common - sense baseline, indicating that "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p244#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 244, "snippet": "221advanced use of recurrent neural networks history = model. fit ( x _ train, y _ train, epochs = 10, batch _ size = 128, validation _ split = 0. 2 )"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p245#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 245, "snippet": "222 chapter 6 deep learning for text and sequences model. compile ( optimizer = ' rmsprop ', loss = ' binary _ crossentropy ', metrics = [ ' acc ' ] )"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p246#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 246, "snippet": "223advanced use of recurrent neural networks as always, deep learning is more an art than a science. we can provide guidelines that suggest what is li"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p247#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 247, "snippet": "224 chapter 6 deep learning for text and sequences markets and machine learning some readers are bound to want to take the techniques we ’ ve introduc"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p248#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 248, "snippet": "225sequence processing with convnets 6. 4 sequence processing with convnets in chapter 5, you learned about convolutio nal neural networks ( convnets "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p249#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 249, "snippet": "226 chapter 6 deep learning for text and sequences these words in any context in an input sequence. a character - level 1d convnet is thus able to lea"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p250#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 250, "snippet": "227sequence processing with convnets this is the example 1d convnet for the imdb dataset. from keras. models import sequential from keras import layer"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p251#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 251, "snippet": "228 chapter 6 deep learning for text and sequences 6. 4. 4 combining cnns and rnns to process long sequences because 1d convnets process input patches"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p252#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 252, "snippet": "229sequence processing with convnets figure 6. 29 shows the training and validation maes. the validation mae stays in the 0. 40s : you can ’ t even be"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p253#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 253, "snippet": "230 chapter 6 deep learning for text and sequences look at data from longer ago ( by increasing the lookback parameter of the data gen - erator ) or l"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p254#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 254, "snippet": "231sequence processing with convnets history = model. fit _ generator ( train _ gen, steps _ per _ epoch = 500, epochs = 20, validation _ data = val _"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p255#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 255, "snippet": "232 chapter 6 deep learning for text and sequences chapter summary in this chapter, you learned the fo llowing techniques, which are widely applicable"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p256#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 256, "snippet": "233 advanced deep - learning best practices this chapter explores a number of powerful tools that will bring you closer to being able to develop state"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p257#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 257, "snippet": "234 chapter 7 advanced deep - learning best practices 7. 1 going beyond the sequential model : the keras functional api until now, all neural networks"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p258#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 258, "snippet": "235going beyond the sequential model : the keras functional api similarly, some tasks need to predict multiple target attributes of input data. given "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p259#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 259, "snippet": "236 chapter 7 advanced deep - learning best practices these three important use cases — multi - input models, multi - output models, and graph - like "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p260#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 260, "snippet": "237going beyond the sequential model : the keras functional api dense = layers. dense ( 32, activation = ' relu ' ) output _ tensor = dense ( input _ "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p261#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 261, "snippet": "238 chapter 7 advanced deep - learning best practices runtimeerror : graph disconnected : cannot obtain value for tensor ( \" input _ 1 : 0 \", shape = "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p262#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 262, "snippet": "239going beyond the sequential model : the keras functional api following is an example of how you can build such a model with the functional api. you"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p263#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 263, "snippet": "240 chapter 7 advanced deep - learning best practices question = np. random. randint ( 1, question _ vocabulary _ size, size = ( num _ samples, max _ "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p264#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 264, "snippet": "241going beyond the sequential model : the keras functional api importantly, training such a mo del requires the ability to specify different loss fun"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p265#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 265, "snippet": "242 chapter 7 advanced deep - learning best practices model. compile ( optimizer = ' rmsprop ', loss = { ' age ' : ' mse ', ' income ' : ' categorical"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p266#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 266, "snippet": "243going beyond the sequential model : the keras functional api spatial features and channel - wise features, which is more efficient than learning th"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p267#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 267, "snippet": "244 chapter 7 advanced deep - learning best practices from keras import layers branch _ a = layers. conv2d ( 128, 1, activation = ' relu ', strides = "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p268#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 268, "snippet": "245going beyond the sequential model : the keras functional api a residual connection consists of making the output of an earlier layer available as i"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p269#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 269, "snippet": "246 chapter 7 advanced deep - learning best practices 7. 1. 5 layer weight sharing one more important feature of the functional api is the ability to "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p270#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 270, "snippet": "247going beyond the sequential model : the keras functional api processing each input sent ence. rather, you want to process both with a single lstm l"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p271#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 271, "snippet": "248 chapter 7 advanced deep - learning best practices features from the left camera and the ri ght camera before merging the two feeds. such low - lev"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p272#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 272, "snippet": "249inspecting and monitoring deep - learning models using keras callbacks and tensorboard 7. 2 inspecting and monitoring deep - learning models using "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p273#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 273, "snippet": "250 chapter 7 advanced deep - learning best practices keras. callbacks. learningratescheduler keras. callbacks. reducelronplateau keras. callbacks. cs"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p274#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 274, "snippet": "251inspecting and monitoring deep - learning models using keras callbacks and tensorboard callbacks _ list = [ keras. callbacks. reducelronplateau ( m"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p275#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 275, "snippet": "252 chapter 7 advanced deep - learning best practices validation _ sample = self. validation _ data [ 0 ] [ 0 : 1 ] activations = self. activations _ "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p276#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 276, "snippet": "253inspecting and monitoring deep - learning models using keras callbacks and tensorboard visually monitoring metrics during training visualizing your"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p277#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 277, "snippet": "254 chapter 7 advanced deep - learning best practices callbacks = [ keras. callbacks. tensorboard ( log _ dir = ' my _ log _ dir ', histogram _ freq ="}
{"id": "DGM/Books/deeplearningwithpython.pdf#p278#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 278, "snippet": "255inspecting and monitoring deep - learning models using keras callbacks and tensorboard the embeddings tab gives you a way to in spect the embedding"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p279#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 279, "snippet": "256 chapter 7 advanced deep - learning best practices the graphs tab shows an interactive visualization of the graph of low - level tensorflow operati"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p280#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 280, "snippet": "257inspecting and monitoring deep - learning models using keras callbacks and tensorboard note that keras also provides another, cleaner way to plot m"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p281#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 281, "snippet": "258 chapter 7 advanced deep - learning best practices you also have the option of displaying sh ape information in the graph of layers. this example v"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p282#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 282, "snippet": "259inspecting and monitoring deep - learning models using keras callbacks and tensorboard 7. 2. 3 wrapping up keras callbacks provide a simple way to "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p283#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 283, "snippet": "260 chapter 7 advanced deep - learning best practices 7. 3 getting the most out of your models trying out architectures blindly works well enough if y"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p284#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 284, "snippet": "261getting the most out of your models the batchnormalization layer is typically used after a convolutional or densely connected layer : conv _ model."}
{"id": "DGM/Books/deeplearningwithpython.pdf#p285#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 285, "snippet": "262 chapter 7 advanced deep - learning best practices these advantages become especially impo rtant when you ’ re training small models from scratch o"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p286#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 286, "snippet": "263getting the most out of your models convolutions and xception in my paper “ x ception : deep learning with depthwise separable convolutions. ” 8 7."}
{"id": "DGM/Books/deeplearningwithpython.pdf#p287#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 287, "snippet": "264 chapter 7 advanced deep - learning best practices in the right direction. updating hyperpar ameters, on the other hand, is extremely challenging. "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p288#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 288, "snippet": "265getting the most out of your models ensembling relies on the assumption th at different good models trained inde - pendently are likely to be good "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p289#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 289, "snippet": "266 chapter 7 advanced deep - learning best practices that elephants are like snakes, and they would forever stay ignorant of the truth of the elephan"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p290#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 290, "snippet": "267getting the most out of your models time, the process is expensive, and the t ools to do it aren ’ t very good. but the hyperopt and hyperas librar"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p291#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 291, "snippet": "268 chapter 7 advanced deep - learning best practices chapter summary in this chapter, you learned the following : – how to build models as arbitrary "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p292#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 292, "snippet": "269 generative deep learning the potential of artificial intelligence to emulate human thought processes goes beyond passive tasks such as object reco"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p293#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 293, "snippet": "270 chapter 8 generative deep learning granted, the artistic pr oductions we ’ ve seen from ai so far have been fairly low quality. ai isn ’ t anywher"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p294#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 294, "snippet": "271text generation with lstm 8. 1 text generation with lstm in this section, we ’ ll explore how recurrent neural networ ks can be used to generate se"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p295#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 295, "snippet": "272 chapter 8 generative deep learning 8. 1. 2 how do you generate sequence data? the universal way to generate sequence data in deep learning is to t"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p296#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 296, "snippet": "273text generation with lstm 30 % of the time. note that greedy sampling ca n be also cast as sampling from a prob - ability distribution : one where "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p297#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 297, "snippet": "274 chapter 8 generative deep learning higher temperatures result in sampling distributions of higher entropy that will generate more surprising and u"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p298#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 298, "snippet": "275text generation with lstm next, you ’ ll extract partially overlapping sequences of length maxlen, one - hot encode them, and pack them in a 3d num"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p299#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 299, "snippet": "276 chapter 8 generative deep learning because your targets are one - hot encoded, you ’ ll use categorical _ crossentropy as the loss to train the mo"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p300#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 300, "snippet": "277text generation with lstm for i in range ( 400 ) : sampled = np. zeros ( ( 1, maxlen, len ( chars ) ) ) for t, char in enumerate ( generated _ text"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p301#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 301, "snippet": "278 chapter 8 generative deep learning subjection of the subjection of the subjection of the self - concerning the feelings in the superiority in the "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p302#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 302, "snippet": "279text generation with lstm 8. 1. 5 wrapping up you can generate discrete sequence data by training a model to predict the next tokens ( s ), given p"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p303#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 303, "snippet": "280 chapter 8 generative deep learning 8. 2 deepdream deepdream is an artistic image - modification te chnique that uses the representations learned b"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p304#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 304, "snippet": "281deepdream you start not from blank, slightly nois y input, but rather from an existing image — thus the resulting effects latch on to preexisting v"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p305#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 305, "snippet": "282 chapter 8 generative deep learning now, let ’ s define a tensor that contains the loss : the weighted sum of the l2 norm of the activations of the"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p306#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 306, "snippet": "283deepdream for each successive scale, from the smallest to the largest, you run gradient ascent to maximize the loss you previously defined, at that"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p307#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 307, "snippet": "284 chapter 8 generative deep learning original _ shape = img. shape [ 1 : 3 ] successive _ shapes = [ original _ shape ] for i in range ( 1, num _ oc"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p308#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 308, "snippet": "285deepdream img = np. expand _ dims ( img, axis = 0 ) img = inception _ v3. preprocess _ input ( img ) return img def deprocess _ image ( x ) : if k."}
{"id": "DGM/Books/deeplearningwithpython.pdf#p309#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 309, "snippet": "286 chapter 8 generative deep learning random generation of the parameters in the layer _ contributions dictionary to quickly explore many different l"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p310#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 310, "snippet": "287neural style transfer 8. 3 neural style transfer in addition to deepdream, another major development in deep - learning - driven image modification"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p311#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 311, "snippet": "288 chapter 8 generative deep learning here, distance is a norm function such as the l2 norm, content is a function that takes an image and computes a"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p312#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 312, "snippet": "289neural style transfer preserve style by maintaining similar correlations within activations for both low - level layers and high - level layers. fe"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p313#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 313, "snippet": "290 chapter 8 generative deep learning def deprocess _ image ( x ) : x [ :, :, 0 ] + = 103. 939 x [ :, :, 1 ] + = 116. 779 x [ :, :, 2 ] + = 123. 68 x"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p314#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 314, "snippet": "291neural style transfer def style _ loss ( style, combination ) : s = gram _ matrix ( style ) c = gram _ matrix ( combination ) channels = 3 size = i"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p315#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 315, "snippet": "292 chapter 8 generative deep learning loss = k. variable ( 0. ) layer _ features = outputs _ dict [ content _ layer ] target _ image _ features = lay"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p316#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 316, "snippet": "293neural style transfer loss _ value = outs [ 0 ] grad _ values = outs [ 1 ]. flatten ( ). astype ( ' float64 ' ) self. loss _ value = loss _ value s"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p317#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 317, "snippet": "294 chapter 8 generative deep learning figure 8. 8 some example results licensed to < null >"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p318#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 318, "snippet": "295neural style transfer additionally, note that running this style - tr ansfer algorithm is slow. but the transfor - mation operated by the setup is "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p319#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 319, "snippet": "296 chapter 8 generative deep learning 8. 4 generating images with variational autoencoders sampling from a latent space of images to cr eate entirely"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p320#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 320, "snippet": "297generating images with variational autoencoders 8. 4. 2 concept vectors for image editing we already hinted at the idea of a concept vector when we"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p321#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 321, "snippet": "298 chapter 8 generative deep learning 8. 4. 3 variational autoencoders variational autoencoders, simultaneously discovered by kingma and welling in d"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p322#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 322, "snippet": "299generating images with variational autoencoders in practice, such classical au toencoders don ’ t lead to particularly useful or nicely structured "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p323#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 323, "snippet": "300 chapter 8 generative deep learning in technical terms, here ’ s how a vae works : 1 an encoder module turns the input samples input _ img into two"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p324#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 324, "snippet": "301generating images with variational autoencoders x = layers. conv2d ( 32, 3, padding = ' same ', activation = ' relu ' ) ( input _ img ) x = layers."}
{"id": "DGM/Books/deeplearningwithpython.pdf#p325#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 325, "snippet": "302 chapter 8 generative deep learning decoder = model ( decoder _ input, x ) z _ decoded = decoder ( z ) the dual loss of a vae doesn ’ t fit the tra"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p326#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 326, "snippet": "303generating images with variational autoencoders once such a model is trained — on mnist, in this case — you can use the decoder net - work to turn "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p327#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 327, "snippet": "304 chapter 8 generative deep learning 8. 4. 4 wrapping up image generation with deep learning is done by learning latent spaces that cap - ture stati"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p328#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 328, "snippet": "305introduction to generative adversarial networks 8. 5 introduction to generative adversarial networks generative adversarial networks ( gans ), intr"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p329#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 329, "snippet": "306 chapter 8 generative deep learning remarkably, a gan is a system where the optimization minimum isn ’ t fixed, unlike in any other training setup "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p330#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 330, "snippet": "307introduction to generative adversarial networks 8. 5. 1 a schematic gan implementation in this section, we ’ ll explain how to implement a gan in k"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p331#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 331, "snippet": "308 chapter 8 generative deep learning stochasticity is good to induce robustness. because gan training results in a dynamic equilibrium, gans are lik"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p332#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 332, "snippet": "309introduction to generative adversarial networks generator _ input = keras. input ( shape = ( latent _ dim, ) ) x = layers. dense ( 128 * 16 * 16 ) "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p333#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 333, "snippet": "310 chapter 8 generative deep learning 8. 5. 5 the adversarial network finally, you ’ ll set up the gan, which chains the generator and the discrimina"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p334#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 334, "snippet": "311introduction to generative adversarial networks x _ train = x _ train [ y _ train. flatten ( ) = = 6 ] x _ train = x _ train. reshape ( ( x _ train"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p335#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 335, "snippet": "312 chapter 8 generative deep learning when training, you may see the adversarial loss begin to increase considerably, while the discriminative loss t"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p336#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 336, "snippet": "313introduction to generative adversarial networks chapter summary with creative applications of deep learning, deep ne tworks go beyond annotating ex"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p337#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 337, "snippet": "314 conclusions you ’ ve almost reached the end of this book. this last chapter will summarize and review core concepts while also expanding your hori"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p338#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 338, "snippet": "315key concepts in review 9. 1 key concepts in review this section briefly synthesizes the key take aways from this book. if you ever need a quick ref"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p339#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 339, "snippet": "316 chapter 9 conclusions image classification, vastly improved mach ine translation, and more. the hype may ( and likely will ) recede, but the susta"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p340#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 340, "snippet": "317key concepts in review that ’ s the magic of deep learning : turn ing meaning into vectors, into geometric spaces, and then incrementally learning "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p341#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 341, "snippet": "318 chapter 9 conclusions in the future, deep learning will not only be used by specialists — researchers, graduate students, and engineers with an ac"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p342#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 342, "snippet": "319key concepts in review 7 be aware of validation - set overfitting when turning hyperparameters : the fact that your hyperparameters may end up bein"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p343#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 343, "snippet": "320 chapter 9 conclusions remember : to perform binary classification, end your stack of layers with a dense layer with a single unit and a sigmoid ac"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p344#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 344, "snippet": "321key concepts in review convnets convolution layers look at spatially local patterns by applying the same geometric transformation to different spat"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p345#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 345, "snippet": "322 chapter 9 conclusions a point in a geometric space of states ). they should be used preferentially over 1d conv - nets in the case of sequences wh"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p346#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 346, "snippet": "323key concepts in review mapping vector data to vector data – predictive healthcare — mapping patient medical records to predictions of patient outco"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p347#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 347, "snippet": "324 chapter 9 conclusions mapping images and text to text – visual qa — mapping images and natural - la nguage questions about the con - tents of imag"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p348#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 348, "snippet": "325the limitations of deep learning 9. 2 the limitations of deep learning the space of applications that can be im plemented with deep learning is nea"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p349#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 349, "snippet": "326 chapter 9 conclusions in particular, this is highlighted by adversarial examples, which are samples fed to a deep - learning network that are desi"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p350#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 350, "snippet": "327the limitations of deep learning in short, deep - learning models don ’ t have any understanding of their input — at least, not in a human sense. o"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p351#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 351, "snippet": "328 chapter 9 conclusions before — like picturing a horse wearing jeans, for instance, or imagining what we ’ d do if we won the lottery. this ability"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p352#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 352, "snippet": "329the limitations of deep learning extreme generalization, quickly adapting to radically novel situations and planning for long - term future situati"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p353#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 353, "snippet": "330 chapter 9 conclusions 9. 3 the future of deep learning this is a more speculative section aimed at opening horizons for people who want to join a "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p354#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 354, "snippet": "331the future of deep learning they ’ re geometric transformations repeatedly applied inside a for loop. the temporal for loop is itself hardcoded by "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p355#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 355, "snippet": "332 chapter 9 conclusions 9. 3. 2 beyond backpropagation and differentiable layers if machine - learning models become more like programs, then they w"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p356#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 356, "snippet": "333the future of deep learning currently, most of the job of a deep - l earning engineer consists of munging data with python scripts and then tuning "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p357#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 357, "snippet": "334 chapter 9 conclusions a remarkable observation has been made re peatedly in recent years : training the same model to do several loosely connected"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p358#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 358, "snippet": "335the future of deep learning 9. 3. 5 the long - term vision in short, here ’ s my long - term vision for machine learning : models will be more like"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p359#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 359, "snippet": "336 chapter 9 conclusions the system will be able to assemble a new working model appropriate for the task using very little data, thanks to rich prog"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p360#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 360, "snippet": "337staying up to date in a fast - moving field 9. 4 staying up to date in a fast - moving field as final parting words, i want to give you some pointe"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p361#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 361, "snippet": "338 chapter 9 conclusions it ’ s difficult, and becoming increasingly more so, to find the signal in the noise. cur - rently, there isn ’ t a good sol"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p362#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 362, "snippet": "339final words 9. 5 final words this is the end of deep learning with python! i hope you ’ ve learned a thing or two about machine learning, deep lear"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p363#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 363, "snippet": "340 appendix a installing keras and its dependencies on ubuntu the process of setting up a deep - learning workstation is fairly involved and consists"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p364#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 364, "snippet": "341installing the python scientific suite a. 1 installing the python scientific suite if you use a mac, we recommend that you in stall the python scie"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p365#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 365, "snippet": "342 appendix a installing keras and its dependencies on ubuntu a. 2 setting up gpu support using a gpu isn ’ t strictly necessary, but it ’ s strongly"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p366#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 366, "snippet": "343installing keras 4 install tensorflow : a tensorflow with or without gpu support can be installed from pypi using pip. here ’ s the command without"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p367#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 367, "snippet": "344 appendix a installing keras and its dependencies on ubuntu { \" image _ data _ format \" : \" channels _ last \", \" epsilon \" : 1e - 07, \" floatx \" : "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p368#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 368, "snippet": "345 appendix b running jupyter notebooks on an ec2 gpu instance this appendix provides a step - by - step guide to running deep - learning jupyter not"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p369#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 369, "snippet": "346 appendix b running jupyter notebooks on an ec2 gpu instance b. 2 why would you not want to use jupyter on aws for deep learning? aws gpu instances"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p370#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 370, "snippet": "347setting up an aws gpu instance 3 select the p2. xlarge instance ( see figure b. 4 ). this instance type provides access to a single gpu and costs $"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p371#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 371, "snippet": "348 appendix b running jupyter notebooks on an ec2 gpu instance note at the end of the launch process, you ’ ll be asked if you want to create new con"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p372#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 372, "snippet": "349setting up an aws gpu instance 7 create a new ssl certificate using open ssl, and create cert. key and cert. pem files in the current ssl directory"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p373#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 373, "snippet": "350 appendix b running jupyter notebooks on an ec2 gpu instance note in case you aren ’ t accustomed to using vi, remember that you need to press i to"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p374#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 374, "snippet": "351using jupyter from your local browser you should see the safety wa rning shown in figure b. 7. this warning is due to the fact that the ssl certifi"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p375#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 375, "snippet": "licensed to < null >"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p376#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 376, "snippet": "353 symbols * operator 40 + operator 99 numerics 0d tensors. see scalars 1d convolutions 225 – 227 1d pooling, for sequence data 226 1d tensors. see v"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p377#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 377, "snippet": "354 index cern 17 channels axis 123 channels - first convention 36 channels - last convention 36 character - level neural language model 272 ciresan, "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p378#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 378, "snippet": "355index deep learning, limitations of ( continued ) risk of anthropomorphiz - ing machine - learning models 325 – 327 overview 9 – 11, 316 – 317 poss"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p379#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 379, "snippet": "356 index glove ( global vectors for word representation ) downloading word embeddings 190 loading embeddings in models 191 goodfellow, ian 305 gpus ("}
{"id": "DGM/Books/deeplearningwithpython.pdf#p380#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 380, "snippet": "357index keras framework 61 – 64 cntk 62 developing with 62 – 64 running 66 tensorflow 62 theano 62 keras library 27 keras. applications module 145 ke"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p381#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 381, "snippet": "358 index mini - batch 96 mini - batch sgd ( mini - batch stochastic gradient descent ) 49 minsky, marvin 12 mnist dataset 27, 68 model checkpointing "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p382#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 382, "snippet": "359index o object detection 94 objective function 10, 60 occam ’ s razor principle 107 octaves 281 – 282 one - hot encoding of characters 181 – 183 of"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p383#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 383, "snippet": "360 index sampling ( continued ) from latent spaces of images 296 – 297 strategies 272 – 274 sanity preserver, arxiv 338 scalar regression 86, 96 scal"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p384#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 384, "snippet": "361index training, convnets on small datasets ( continued ) data preprocessing 135 – 138 downloading data 131 – 133 relevance for small - data problem"}
{"id": "DGM/Books/deeplearningwithpython.pdf#p385#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 385, "snippet": "for ordering information go to www. manning. com machine learning with tensorflow by nishant shukla isbn : 9781617293870 325 pages, $ 44. 99 december "}
{"id": "DGM/Books/deeplearningwithpython.pdf#p386#c1", "source": "DGM/Books/deeplearningwithpython.pdf", "page": 386, "snippet": "francois chollet m achine learning has made remarkable progress in recent years. we went from near - unusable speech and image recognition, to near - "}
{"id": "DGM/Slides/PixelRNN_PixelCNN.pdf#p1#c1", "source": "DGM/Slides/PixelRNN_PixelCNN.pdf", "page": 1, "snippet": "pixel recurrent neural networks a¨aron van den oord avdnoord @ google. com nal kalchbrenner nalk @ google. com koray kavukcuoglu korayk @ google. com "}
{"id": "DGM/Slides/PixelRNN_PixelCNN.pdf#p2#c1", "source": "DGM/Slides/PixelRNN_PixelCNN.pdf", "page": 2, "snippet": "pixel recurrent neural networks x 1 x i x n x n 2 context x n 2 multi - scale context x 1 x i x n x n 2 r g b r g b r g b mask a mask b context figure"}
{"id": "DGM/Slides/PixelRNN_PixelCNN.pdf#p3#c1", "source": "DGM/Slides/PixelRNN_PixelCNN.pdf", "page": 3, "snippet": "pixel recurrent neural networks the value p ( xi | x1,..., x i−1 ) is the probability of the i - th pixel xi given all the previous pixelsx1,..., x i−"}
{"id": "DGM/Slides/PixelRNN_PixelCNN.pdf#p4#c1", "source": "DGM/Slides/PixelRNN_PixelCNN.pdf", "page": 4, "snippet": "pixel recurrent neural networks pixelcnn row lstm diagonal bilstm figure 4. visualization of the input - to - state and state - to - state mappings fo"}
{"id": "DGM/Slides/PixelRNN_PixelCNN.pdf#p5#c1", "source": "DGM/Slides/PixelRNN_PixelCNN.pdf", "page": 5, "snippet": "pixel recurrent neural networks pixelcnn row lstm diagonal bilstm 7 × 7 conv mask a multiple residual blocks : ( see ﬁg 5 ) conv row lstm diagonal bil"}
{"id": "DGM/Slides/PixelRNN_PixelCNN.pdf#p6#c1", "source": "DGM/Slides/PixelRNN_PixelCNN.pdf", "page": 6, "snippet": "pixel recurrent neural networks in the literature it is currently best practice to add real - valued noise to the pixel values to dequantize the data "}
{"id": "DGM/Slides/PixelRNN_PixelCNN.pdf#p7#c1", "source": "DGM/Slides/PixelRNN_PixelCNN.pdf", "page": 7, "snippet": "pixel recurrent neural networks figure 7. samples from models trained on cifar - 10 ( left ) and imagenet 32x32 ( right ) images. in general we can se"}
{"id": "DGM/Slides/PixelRNN_PixelCNN.pdf#p8#c1", "source": "DGM/Slides/PixelRNN_PixelCNN.pdf", "page": 8, "snippet": "pixel recurrent neural networks figure 8. samples from models trained on imagenet 64x64 images. left : normal model, right : multi - scale model. the "}
{"id": "DGM/Slides/PixelRNN_PixelCNN.pdf#p9#c1", "source": "DGM/Slides/PixelRNN_PixelCNN.pdf", "page": 9, "snippet": "pixel recurrent neural networks conditioning. finally, we also show image completions sampled from the model in figure 9. 6. conclusion in this paper "}
{"id": "DGM/Slides/PixelRNN_PixelCNN.pdf#p10#c1", "source": "DGM/Slides/PixelRNN_PixelCNN.pdf", "page": 10, "snippet": "pixel recurrent neural networks rezende, danilo j, mohamed, shakir, and wierstra, daan. stochastic backpropagation and approximate inference in deep g"}
{"id": "DGM/Slides/PixelRNN_PixelCNN.pdf#p11#c1", "source": "DGM/Slides/PixelRNN_PixelCNN.pdf", "page": 11, "snippet": "pixel recurrent neural networks figure 10. additional samples from a model trained on imagenet 32x32 ( right ) images."}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p1#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 1, "snippet": "1 pd. dr. juanj. durillofundamentals of deep learning"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p2#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 2, "snippet": "the goals of this course • get you up and on your feet quickly • build a foundation to tackle a deep learning project right away • we won ’ t cover th"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p3#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 3, "snippet": "3 www. nvidia. com / dli https : / / courses. nvidia. com / dli - event"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p4#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 4, "snippet": "agenda part 1 : an introduction to deep learning part 2 : how a neural network trains part 3 : convolutional neural networks part 4 : data augmentatio"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p5#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 5, "snippet": "have fun!"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p6#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 6, "snippet": "6 history of ai"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p7#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 7, "snippet": "7 beginning of artificial intelligence computers are made in part to complete human tasks early on, generalized intelligence looked possible turned ou"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p8#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 8, "snippet": "8 early neural networks inspired by biology created in the 1950 ’ s outclassed by von neumann architecture"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p9#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 9, "snippet": "expert systems highly complex programmed by hundreds of engineers rigorous programming of many rules"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p10#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 10, "snippet": "10 expert systems - limitations what are these three images?"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p11#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 11, "snippet": "11 the deep learning revolution"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p12#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 12, "snippet": "12 data - networks need a lot of information to learn from - the digital era and the internet has supplied that data"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p13#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 13, "snippet": "13 computing powerneed a way for our artificial “ brain ” to observe lots of data within a practical amount of time."}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p14#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 14, "snippet": "14 the importance of the gpu a neural network a rendered image"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p15#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 15, "snippet": "15 what is deep learning?"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p16#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 16, "snippet": "16 a ( brief ) introduction to machine learning28. 04. 2021 | pd dr. juan j. durillo"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p17#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 17, "snippet": "17 perceptron – artificial neuron xn x1 x3 x2 finputsw1w2 w3 wn sumactivation function θ =!, \" …, # output most popular activation functions single ar"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p18#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 18, "snippet": "18 neural network x1 x2 input layerintermediate layeroutput!,!!!, \"!!, %! \",!! \", \"! \", %!!,! \" \",! \" %,! \" θ =!,!!,!, \"!,!, %!, \",!!, \", \"!, \", %!,!,"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p19#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 19, "snippet": "( supervised ) learning • data domain ζ : χ×υ χadomain of the input dataυaset of labels ( knowledge ) • data distribution is a probability distributio"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p20#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 20, "snippet": "( supervised ) learning • given θwe can define the expected loss as : =! ~ # ℓθ ; • given d, ℓ, and a model with parameter set θ, we can define learni"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p21#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 21, "snippet": "( supervised ) learning • the dominant algorithms for training neural networks are based on mini - batch stochastic gradient descent ( sgd ) • given a"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p22#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 22, "snippet": "computer vision tasks image segmentation object detection image classification + localizationimage classification predicting the type or class of an o"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p23#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 23, "snippet": "on input representation image language sentence = a ( brief ) introduction to machine learning | pd dr. juan j. durillo | 28. 04. 2021"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p24#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 24, "snippet": "neural networks for image classificationfully connected neural network is a zerois a one is a nine is a five input layer ( a neuron per pixel and colo"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p25#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 25, "snippet": "training neural networks stochastic gradient descent main ideahow the surface looks like in realityθ ) θ1 θ2 a ( brief ) introduction to machine learn"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p26#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 26, "snippet": "neural networks for image classification shift to the left is a zerois a one is a nine is a five a ( brief ) introduction to machine learning | pd dr."}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p27#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 27, "snippet": "no more feature engineering 33 3 3three a ( brief ) introduction to machine learning | pd dr. juan j. durillo | 28. 04. 2021"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p28#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 28, "snippet": "learning features from data : convolutionsreceptive field 1 x ( - 1 ) + 0 x 0 + 1 x 1 + 0 x ( - 2 ) + 1 x 1 + 0 x 2 + 0 x ( - 3 ) + 0 x 0 + 1 x 3 = 4 "}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p29#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 29, "snippet": "filters input image : can we get only vertical linesout of this picture? filter 2 filter 1 filter 3 try the code yourself ( in octave )! i = imread ( "}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p30#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 30, "snippet": "convolutional neural networks ( cnn ) a pooling layer down sample the feature maps produced by a convolution into smaller number of parameters to redu"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p31#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 31, "snippet": "cnn architecture : a common pattern and its influence convolutional layersclassification layer the execution time required during a forward pass throu"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p32#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 32, "snippet": "lenetarchitecture architecturesummary : • 3 convolutional layers filters in all the layers equal to 5x5 ( layer 1 depth = 6, layer 2 depth = 16, layer"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p33#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 33, "snippet": "alexnetand vgg architectures ~ 60, 000, 000 parameters vgg16 alexnet ~ 138, 000, 000 parameters a ( brief ) introduction to machine learning | pd dr. "}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p34#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 34, "snippet": "googlenet ~ 13, 000, 000 parameters • what is the best kernel size for each layer? • concatenating filters instead of stacking them for reducing compu"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p35#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 35, "snippet": ") θ1 θ2 restnet ) θ1 θ2 thanks to the shortcut istransformed into a ( brief ) introduction to machine learning | pd dr. juan j. durillo | 28. 04. 2021"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p36#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 36, "snippet": "increasing complexity a ( brief ) introduction to machine learning | pd dr. juan j. durillo | 28. 04. 2021"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p37#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 37, "snippet": "summarybrief introduction to deep learning with emphasis in deep convolutional neural networks review of basic concepts : from perceptron to the learn"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p38#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 38, "snippet": "38 deep learning flips traditional programming on its head"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p39#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 39, "snippet": "traditional programming define a set of rules for classification 1 program those rules into the computer 2 feed it examples, and the program uses the "}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p40#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 40, "snippet": "machine learning show model the examples with the answer of how to classify 1 model takes guesses, we tell it if it ’ s right or not 2 model learns to"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p41#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 41, "snippet": "this is a fundamental shift"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p42#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 42, "snippet": "42 when to choose deep learning if rules are clear and straightforward, often better to just program it if rules are nuanced, complex, difficult to di"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p43#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 43, "snippet": "deep learning compared to other ai depth and complexity of networks up to billions of parameters ( and growing ) many layers in a model important for "}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p44#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 44, "snippet": "44 how deep learning is transforming the world"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p45#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 45, "snippet": "45 computer vision robotics and manufacturing object detection self driving cars"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p46#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 46, "snippet": "46 natural language processing real time translation voice recognition virtual assistants"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p47#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 47, "snippet": "47 recommender systems content curation targeted advertising shopping recommendations"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p48#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 48, "snippet": "48 reinforcement learning alphago beats world champion in go ai bots beat professional videogamers stock trading robots"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p49#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 49, "snippet": "49 overview of the course"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p50#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 50, "snippet": "50 hands on exercises • get comfortable with the process of deep learning • exposure to different models and datatypes • get a jump - start to tackle "}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p51#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 51, "snippet": "51 structure of the course “ hello world ” of deep learning train a more complicated model new architectures and techniques to improve performance pre"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p52#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 52, "snippet": "platform of the course gpu powered cloud server jupyterlabplatform jupyternotebooks for interactive coding"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p53#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 53, "snippet": "53 software of the course • major deep learning platforms : • tensorflow + keras ( google ) • pytorch ( facebook ) • mxnet ( apache ) • we ’ ll be usi"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p54#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 54, "snippet": "54 first exercise : classify handwritten digits"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p55#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 55, "snippet": "55 hello neural networks • historically important and difficult task for computers train a network to correctly classify handwritten digits • get expo"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p56#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 56, "snippet": "56 let ’ s go!"}
{"id": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf#p57#c1", "source": "DGM/Slides/FoundamentalsOfDeepLearning_lecture1.pdf", "page": 57, "snippet": "57"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p1#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 1, "snippet": "indian institute of technology kharagpur machine learning fundamentals the ability to learn is a core artefact of intelligence course : cs60045 1 pall"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p2#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 2, "snippet": "machine learning"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p3#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 3, "snippet": "reference indian institute of technology kharagpur 3 deep learning ian goodfellow, yoshua bengio, and aaron courville mit press, 2018"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p4#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 4, "snippet": "what is machine learning? • [ mitchell 1997 ] a computer program is said to learn from experience e with respect to some class of tasks t and performa"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p5#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 5, "snippet": "what is machine learning? • [ mitchell 1997 ] a computer program is said to learn from experience e with respect to some class of tasks t and performa"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p6#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 6, "snippet": "what is machine learning? • [ mitchell 1997 ] a computer program is said to learn from experience e with respect to some class of tasks t and performa"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p7#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 7, "snippet": "unsupervised and supervised learning are not formally different • the chain rule of probability states that for a vector ∈, the joint probability dist"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p8#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 8, "snippet": "why would one want machine learning? • static, pre - programmed rules might give poor performance • difficulties in analytically specifying the system"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p9#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 9, "snippet": "linear regression given a training data set, …, where : ( ) = [ … ( ) ] for = …. the output is a linear function of the input which predicts the value"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p10#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 10, "snippet": "linear regression – a first cut approach indian institute of technology kharagpur 10"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p11#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 11, "snippet": "linear regression example indian institute of technology kharagpur 11 mse ( train ) is minimized for w1 = 1. 5 therefore we learn y = w1x = 1. 5x"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p12#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 12, "snippet": "capacity, overfitting and underfitting • the ability of a machine learning algorithm to perform well on previously unobserved inputs is called general"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p13#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 13, "snippet": "example • the representational capacity of a model family is the set of functions that we are allowed to select • the effective capacity of a learning"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p14#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 14, "snippet": "capacity versus generalization error indian institute of technology kharagpur 14"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p15#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 15, "snippet": "the no free lunch theorem how well can a machine learning algorithm generalize from a finite training set of examples? the no free lunch theorem for m"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p16#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 16, "snippet": "effect of training set indian institute of technology kharagpur 16 the optimal capacity plateaus after reaching sufficient complexity to solve the tas"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p17#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 17, "snippet": "hyper - parameters and validation sets • most machine learning algorithms have hyper - parameters or settings that we can tune to control the algorith"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p18#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 18, "snippet": "machine learning process indian institute of technology kharagpur 18 measurements feature extraction machine learning algorithm training and validatio"}
{"id": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf#p19#c1", "source": "DGM/Slides/Lec11 Machine Learning Fundamentals.pdf", "page": 19, "snippet": "how to do machine learning for ( supervised ) classification and regression ( the most common tasks ) : 1. algorithm selection : choose an algorithm 2"}
{"id": "DGM/Slides/introduction _good.pdf#p1#c1", "source": "DGM/Slides/introduction _good.pdf", "page": 1, "snippet": "- - - - introduction to artiﬁcial neural networks • what is an artiﬁcial neural network? - itisac omputational system inspired by the structure proces"}
{"id": "DGM/Slides/introduction _good.pdf#p2#c1", "source": "DGM/Slides/introduction _good.pdf", "page": 2, "snippet": "- - - - - 2 - • wh ya rtiﬁcial neural networks? - m assive parallelism - d istributed representation - l earning ability - g eneralization ablity - fa"}
{"id": "DGM/Slides/introduction _good.pdf#p3#c1", "source": "DGM/Slides/introduction _good.pdf", "page": 3, "snippet": "- - - - - 3 - node input : neti = j σ w ijii node output : o i = f ( neti ) • activation function - a ne xample"}
{"id": "DGM/Slides/introduction _good.pdf#p4#c1", "source": "DGM/Slides/introduction _good.pdf", "page": 4, "snippet": "- - - - - 4 - • topology • learning - l earn the connection weights from a set of training examples - d ifferent network architectures required differ"}
{"id": "DGM/Slides/introduction _good.pdf#p5#c1", "source": "DGM/Slides/introduction _good.pdf", "page": 5, "snippet": "- - - - - 5 -"}
{"id": "DGM/Slides/introduction _good.pdf#p6#c1", "source": "DGM/Slides/introduction _good.pdf", "page": 6, "snippet": "- - - - - 6 - unsupervised learning does not require a correct answer associated with each input pat - tern in the training set explores the underlyin"}
{"id": "DGM/Slides/introduction _good.pdf#p7#c1", "source": "DGM/Slides/introduction _good.pdf", "page": 7, "snippet": "- - - - - 7 - • practical issues - g eneralization vs memorization good fit bad fit ho wt oc hoose the network size ( free parameters ) ho wm anytrain"}
{"id": "DGM/Slides/introduction _good.pdf#p8#c1", "source": "DGM/Slides/introduction _good.pdf", "page": 8, "snippet": "- - - - - 8 -"}
{"id": "DGM/Slides/introduction _good.pdf#p9#c1", "source": "DGM/Slides/introduction _good.pdf", "page": 9, "snippet": "- - - - - 9 - • tw osuccessful applications - z ipcode recognition - texttov oice translation ( nettalk ) - - - -"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p1#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 1, "snippet": "review of deep learning : concepts, cnn architectures, challenges, applications, future directions laith alzubaidi1, 5 *, jinglan zhang1, amjad j. hum"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p2#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 2, "snippet": "page 2 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 introduction recently, machine learning ( ml ) has become very widespread in research and has "}
{"id": "DGM/Slides/2021 paper CNN.pdf#p3#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 3, "snippet": "page 3 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 components of it. furthermore, we have elaborated in detail the most common cnn architectures"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p4#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 4, "snippet": "page 4 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 details the challenges of dl and alternate solutions. “ applications of deep learning ” sec - "}
{"id": "DGM/Slides/2021 paper CNN.pdf#p5#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 5, "snippet": "page 5 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 popular type of ml algorithm in recent years due to the huge growth and evolution of the fiel"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p6#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 6, "snippet": "page 6 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 cancer, us board - certified general pathologists achieved an average accuracy of 61 %, while "}
{"id": "DGM/Slides/2021 paper CNN.pdf#p7#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 7, "snippet": "page 7 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 fig. 2 deep learning family fig. 3 the difference between deep learning and traditional machi"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p8#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 8, "snippet": "page 8 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 x - ray images or other types of images. we end this section by the saying of ai pioneer geoff"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p9#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 9, "snippet": "page 9 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 to the task under consideration. thus, robustness to the usual changes of the input data is a"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p10#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 10, "snippet": "page 10 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 semi - supervised learning. due to difficulty of obtaining a large amount of labeled text doc"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p11#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 11, "snippet": "page 11 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 • it assists you to identify which action produces the highest reward over a longer period. "}
{"id": "DGM/Slides/2021 paper CNN.pdf#p12#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 12, "snippet": "page 12 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 recurrent neural networks rnns are a commonly employed and familiar algorithm in the discipli"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p13#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 13, "snippet": "page 13 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 cnn is considered to be more powerful than rnn. rnn includes less feature com - patibility w"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p14#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 14, "snippet": "page 14 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 however, n must be smaller than m, while q is either equal to or smaller than r. in addi - ti"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p15#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 15, "snippet": "page 15 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 3. large - scale network implementation is much easier with cnn than with other neu - ral ne"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p16#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 16, "snippet": "page 16 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 will increase, and in turn, the size of the output feature map will also increase. core benef"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p17#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 17, "snippet": "page 17 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 small ; hence, this approach is memory - effective. in addition, matrix operation is computa"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p18#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 18, "snippet": "page 18 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 non - linear activation layers are employed after all layers with weights ( so - called learn"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p19#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 19, "snippet": "page 19 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 • parametric linear units : this is mostly the same as leaky relu. the main differ - ence is"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p20#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 20, "snippet": "page 20 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 ( a ) cross - entropy or softmax loss function : this function is commonly employed for measu"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p21#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 21, "snippet": "page 21 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 1. dropout : this is a widely utilized technique for generalization. during each training ep"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p22#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 22, "snippet": "page 22 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 • chances of over - fitting are reduced, since it has a minor influence on regulariza - tion."}
{"id": "DGM/Slides/2021 paper CNN.pdf#p23#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 23, "snippet": "page 23 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 time is required for converging, and it could converge to a local optimum ( for non - convex"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p24#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 24, "snippet": "page 24 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 learning optimization. this is represented by the hessian matrix, which employs a second - or"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p25#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 25, "snippet": "page 25 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 the number of connection between them is ( 10 ∗ 2 = 20 connection, weights ), how the error "}
{"id": "DGM/Slides/2021 paper CNN.pdf#p26#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 26, "snippet": "page 26 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 • expand the dataset with data augmentation or use transfer learning ( explained in lat - ter"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p27#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 27, "snippet": "page 27 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 table 2 brief overview of cnn architectures model main finding depth dataset error rate inpu"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p28#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 28, "snippet": "page 28 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 significance in the recent cnn generations, as well as beginning an innovative research era i"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p29#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 29, "snippet": "page 29 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 perception convolution. these convolutions are executed using a 1×1 filter, which sup - port"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p30#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 30, "snippet": "page 30 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 rearrangement in cnn topology. this rearrangement proposed that the visualization of the feat"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p31#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 31, "snippet": "page 31 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 of different sizes ( 5 × 5, 3× 3, and 1 × 1 ) to capture channel information together with s"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p32#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 32, "snippet": "page 32 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 jam, which substantially decreased the feature space in the following layer, and in turn occa"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p33#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 33, "snippet": "page 33 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 in comparison to the highway network, resnet presented shortcut connections inside layers to"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p34#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 34, "snippet": "page 34 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 convolution very similar to cross - channel correlation. previously, lin et al. utilized the "}
{"id": "DGM/Slides/2021 paper CNN.pdf#p35#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 35, "snippet": "page 35 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 wise - convolutions. thus, the network gains the ability to discriminate clearly between the"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p36#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 36, "snippet": "page 36 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 of resnext was regulated by employing 1 × 1 filters ( low embeddings ) ahead of a 3 × 3 convo"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p37#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 37, "snippet": "page 37 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 resnet, vgg, and alexnet. by contrast, the spatial dimension reduces, since a sub - sam - pl"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p38#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 38, "snippet": "page 38 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 these axes are subsequently used as the 1 × 1 convolutions ( pointwise convolution ) for perf"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p39#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 39, "snippet": "page 39 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 used for varying surroundings. by contrast, stacking multi - attention modules has made ran "}
{"id": "DGM/Slides/2021 paper CNN.pdf#p40#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 40, "snippet": "page 40 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 they can be employed in segmentation. in the final module ( cse ), a similar se - block con -"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p41#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 41, "snippet": "page 41 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 this is highly significant in detection and segmentation operations, since the capsule invol"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p42#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 42, "snippet": "page 42 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 training data dl is extremely data - hungry considering it also involves representation learn"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p43#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 43, "snippet": "page 43 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 common challenge associated with using such models concerns the lack of training data. indee"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p44#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 44, "snippet": "page 44 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 2. a research problem using pre - trained models : training a dl approach requires a massive "}
{"id": "DGM/Slides/2021 paper CNN.pdf#p45#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 45, "snippet": "page 45 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 networks can perform better when these techniques are employed. next, we list some data augm"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p46#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 46, "snippet": "page 46 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 however, highly well - behaved solutions for positional biases available within the training "}
{"id": "DGM/Slides/2021 paper CNN.pdf#p47#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 47, "snippet": "page 47 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 portion of the particular example. within this solution, back - propagation - based tech - n"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p48#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 48, "snippet": "page 48 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 architecture like icarl [ 187, 188 ]. finally, techniques of the third type are founded on du"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p49#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 49, "snippet": "page 49 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 model output. a recently proposed technique penalizes the over - confident outputs for regul"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p50#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 50, "snippet": "page 50 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 exploding gradient problem opposite to the vanishing problem is the one related to gradient. "}
{"id": "DGM/Slides/2021 paper CNN.pdf#p51#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 51, "snippet": "page 51 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 used to locate the object, which is surrounded by a single bounding box. in segmenta - tion "}
{"id": "DGM/Slides/2021 paper CNN.pdf#p52#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 52, "snippet": "page 52 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 classification computer - aided diagnosis ( cadx ) is another title sometimes used for classi"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p53#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 53, "snippet": "page 53 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 adni dataset and using fine - tuned deep supervision techniques. the architectures of vggnet"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p54#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 54, "snippet": "page 54 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 thus, detection is a field of study requiring both accuracy and sensitivity [ 272 – 274 ]. ch"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p55#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 55, "snippet": "page 55 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 segmentation, particularly tumors [ 295 – 300 ]. this issue is highly significant in surgica"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p56#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 56, "snippet": "page 56 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 were techniques introduced by chen et al. [ 308 ]. these authors aimed to enhance the accurac"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p57#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 57, "snippet": "page 57 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 registration approach. in this instance, the network is able of placing each input point clo"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p58#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 58, "snippet": "page 58 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 than 14 million images, along with resnet as a network model, take around 30k to 40k repetiti"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p59#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 59, "snippet": "page 59 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 cpu ‑ based approach the well - behaved performance of the cpu nodes usually assists robust "}
{"id": "DGM/Slides/2021 paper CNN.pdf#p60#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 60, "snippet": "page 60 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 custom floating - point has revealed that lowering the 8 - bit is extremely promising ; more "}
{"id": "DGM/Slides/2021 paper CNN.pdf#p61#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 61, "snippet": "page 61 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 7. false positive rate ( fpr ) : this metric refers to the possibility of a false alarm rati"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p62#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 62, "snippet": "page 62 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 • dl already experiences difficulties in simultaneously modeling multi - complex modalities o"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p63#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 63, "snippet": "page 63 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 case of healthcare data ). to alleviate this issue, tl and data augmentation have been resea"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p64#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 64, "snippet": "page 64 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 la, and maf ; supervision : jz, and yd ; project administration : jz, yd, and js ; funding ac"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p65#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 65, "snippet": "page 65 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 18. lecun y, bengio y, hinton g. deep learning. nature. 2015 ; 521 ( 7553 ) : 436 – 44. 19. "}
{"id": "DGM/Slides/2021 paper CNN.pdf#p66#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 66, "snippet": "page 66 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 55. mnih v, kavukcuoglu k, silver d, rusu aa, veness j, bellemare mg, graves a, riedmiller m,"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p67#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 67, "snippet": "page 67 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 89. lecun y, jackel ld, bottou l, cortes c, denker js, drucker h, guyon i, muller ua, sackin"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p68#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 68, "snippet": "page 68 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 125. chollet f. xception : deep learning with depthwise separable convolutions. in : proceedi"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p69#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 69, "snippet": "page 69 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 160. moreno ‑ barea fj, strazzera f, jerez jm, urda d, franco l. forward noise adjustment sc"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p70#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 70, "snippet": "page 70 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 194. wiedemann s, kirchhoffer h, matlage s, haase p, marban a, marinc t, neumann d, nguyen t,"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p71#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 71, "snippet": "page 71 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 228. zeleznik r, foldyna b, eslami p, weiss j, alexander i, taron j, parmar c, alvi rm, bane"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p72#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 72, "snippet": "page 72 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 258. abraham b, nair ms. computer ‑ aided detection of covid ‑ 19 from x ‑ ray images using m"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p73#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 73, "snippet": "page 73 of 74 alzubaidi et al. j big data ( 2021 ) 8 : 53 291. ciresan dc, giusti a, gambardella lm, schmidhuber j. mitosis detection in breast cancer"}
{"id": "DGM/Slides/2021 paper CNN.pdf#p74#c1", "source": "DGM/Slides/2021 paper CNN.pdf", "page": 74, "snippet": "page 74 of 74alzubaidi et al. j big data ( 2021 ) 8 : 53 324. miao s, wang zj, liao r. a cnn regression approach for real ‑ time 2d / 3d registration."}
{"id": "DGM/Slides/illinopis Rnn.pdf#p1#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 1, "snippet": "introduction to rnns! arun mallya! best viewed with computer modern fonts installed!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p2#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 2, "snippet": "outline! • why recurrent neural networks ( rnns )?! • the vanilla rnn unit! • the rnn forward pass! • backpropagation refresher! • the rnn backward pa"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p3#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 3, "snippet": "motivation! • not all problems can be converted into one with ﬁxed - length inputs and outputs!! • problems such as speech recognition or time - serie"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p4#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 4, "snippet": "recurrent neural networks ( rnns )! • recurrent neural networks take the previous output or hidden states as inputs.! the composite input at time t ha"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p5#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 5, "snippet": "sample feed - forward network! 5 h1! y1! x1! t = 1!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p6#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 6, "snippet": "sample rnn! 6 h1! y1! x1! t = 1! h2! y2! x2! h3! y3! x3! t = 2! t = 3!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p7#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 7, "snippet": "sample rnn! 7 h1! y1! x1! t = 1! h2! y2! x2! h3! y3! x3! t = 2! t = 3! h0!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p8#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 8, "snippet": "the vanilla rnn cell! 8 ht! xt!!! ht - 1!! ht = w!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p9#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 9, "snippet": "the vanilla rnn forward! 9 h1! x1 h0!! c1! y1! h2! x2 h1!! c2! y2! h3! x3 h2!! c3! y3! ht = = f ( ht ) ct = loss ( yt, gtt )"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p10#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 10, "snippet": "the vanilla rnn forward! 10 h1! x1 h0!! c1! y1! h2! x2 h1!! c2! y2! h3! x3 h2!! c3! y3! ht = = f ( ht ) ct = loss ( yt, gtt ) indicates shared weights"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p11#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 11, "snippet": "recurrent neural networks ( rnns )! • note that the weights are shared over time! • essentially, copies of the rnn cell are made over time ( unrolling"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p12#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 12, "snippet": "sentiment classiﬁcation! • classify a! restaurant review from yelp! or! movie review from imdb or! …! as positive or negative!! • inputs : multiple wo"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p13#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 13, "snippet": "rnn! the! h1! sentiment classiﬁcation!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p14#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 14, "snippet": "rnn! the! rnn! food! h1! h2! sentiment classiﬁcation!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p15#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 15, "snippet": "rnn! the! rnn! food! h1! h2! rnn! good! hn - 1! hn! sentiment classiﬁcation!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p16#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 16, "snippet": "rnn! the! rnn! food! h1! h2! rnn! good! hn - 1! hn! linear classiﬁer! sentiment classiﬁcation!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p17#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 17, "snippet": "rnn! the! rnn! food! h1! h2! rnn! good! hn - 1! hn! linear classiﬁer! sentiment classiﬁcation! ignore! ignore! h1! h2!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p18#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 18, "snippet": "rnn! the! rnn! food! h1! h2! rnn! good! hn - 1! h = sum ( … )! h1! h2! hn! sentiment classiﬁcation! http : / / deeplearning. net / tutorial / lstm. ht"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p19#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 19, "snippet": "rnn! the! rnn! food! h1! h2! rnn! good! hn - 1! h = sum ( … )! h1! h2! hn! linear classiﬁer! sentiment classiﬁcation! http : / / deeplearning. net / t"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p20#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 20, "snippet": "image captioning! • given an image, produce a sentence describing its contents!! • inputs : image feature ( from a cnn )! • outputs : multiple words ("}
{"id": "DGM/Slides/illinopis Rnn.pdf#p21#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 21, "snippet": "rnn! image captioning! cnn!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p22#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 22, "snippet": "rnn! image captioning! cnn! rnn! h2! h1! the! h2! linear classiﬁer!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p23#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 23, "snippet": "rnn! image captioning! cnn! rnn! rnn! h2! h3! h1! the! dog! h2! h3! linear classiﬁer! linear classiﬁer!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p24#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 24, "snippet": "rnn outputs : image captions! show and tell : a neural image caption generator, cvpr 15!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p25#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 25, "snippet": "rnn outputs : language modeling! http : / / karpathy. github. io / 2015 / 05 / 21 / rnn - /! viola :! why, salisbury must ﬁnd his ﬂesh and thought! th"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p26#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 26, "snippet": "input – output scenarios! single - single! single - multiple! multiple - single! multiple - multiple! feed - forward network! image captioning! sentim"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p27#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 27, "snippet": "input – output scenarios! note : we might deliberately choose to frame our problem as a! particular input - output scenario for ease of training or! b"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p28#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 28, "snippet": "the vanilla rnn forward! 28 h1! x1 h0!! c1! y1! h2! x2 h1!! c2! y2! h3! x3 h2!! c3! y3! ht = = f ( ht ) ct = loss ( yt, gtt ) “ unfold ” network throu"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p29#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 29, "snippet": "backpropagation refresher! f ( x ; w )! x! y! c! sgd updatew←w−η∂c∂w∂c∂w = = f ( x ; w ) c = loss ( y, ygt )"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p30#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 30, "snippet": "multiple layers! f1 ( x ; w1 )! x! y1! c! sgd updatew2←w2−η∂c∂w2w1←w1−η∂c∂w1f2 ( y1 ; w2 )! y2! y1 = f1 ( x ; w1 ) y2 = f2 ( y1 ; w2 ) c = loss ( y2, "}
{"id": "DGM/Slides/illinopis Rnn.pdf#p31#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 31, "snippet": "chain rule for gradient computation! f1 ( x ; w1 )! x! y1! c! ∂c∂w1 = ( y1 ; w2 )! y2! find ∂c∂w1, ∂c∂w2∂c∂w2 = application of the chain rule! y1 = f1"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p32#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 32, "snippet": "chain rule for gradient computation! does output change due to −how does output change due to = = f ( x ; w )! x! y! we are interested in computing :!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p33#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 33, "snippet": "chain rule for gradient computation! does output change due to −how does output change due to = = we are interested in computing :!, to the layer are "}
{"id": "DGM/Slides/illinopis Rnn.pdf#p34#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 34, "snippet": "extension to computational graphs! f ( x ; w )! f1 ( y ; w1 )! f2 ( y ; w2 )! f ( x ; w )! x! y! x! y! y! y2! y1!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p35#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 35, "snippet": "extension to computational graphs! f ( x ; w )! ( y ; w1 )! ( y ; w2 )! ( x ; w )!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p36#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 36, "snippet": "extension to computational graphs! f ( x ; w )! ( y ; w1 )! ( y ; w2 )! ( x ; w )! accumulation! σ"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p37#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 37, "snippet": "backpropagation through time ( bptt )! • one of the methods used to train rnns! • the unfolded network ( used during forward pass ) is treated as one "}
{"id": "DGM/Slides/illinopis Rnn.pdf#p38#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 38, "snippet": "the unfolded vanilla rnn! 38 h1! x1!! c1! y1! h2! c2! y2! h3! c3! y3! h0!! h1!! h2!! x2!! x3!! • treat the unfolded network as one big feed - forward "}
{"id": "DGM/Slides/illinopis Rnn.pdf#p39#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 39, "snippet": "the unfolded vanilla rnn forward! 39 h1! x1 h0!! c1! y1! h2! x2 h1!! c2! y2! h3! x3 h2!! c3! y3!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p40#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 40, "snippet": "40 h1! x1 h0!! c1! y1! h2! x2 h1!! c2! y2! h3! x3 h2!! c3! y3! the unfolded vanilla rnn backward!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p41#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 41, "snippet": "the vanilla rnn backward! 41 h1! x1 h0!! c1! y1! h2! x2 h1!! c2! y2! h3! x3 h2!! c3! y3! ht = = f ( ht ) ct = loss ( yt, gtt ) ∂ct∂h1 = =!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p42#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 42, "snippet": "issues with the vanilla rnns! • in the same way a product of k real numbers can shrink to zero or explode to inﬁnity, so can a product of matrices! • "}
{"id": "DGM/Slides/illinopis Rnn.pdf#p43#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 43, "snippet": "the identity relationship! • recall! ht = ht−1 + f ( xt ) ∂ct∂h1 = =! • suppose that instead of a matrix multiplication, we had an identity relationsh"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p44#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 44, "snippet": "the identity relationship! • recall! ht = ht−1 + f ( xt ) ∂ct∂h1 = =! • suppose that instead of a matrix multiplication, we had an identity relationsh"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p45#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 45, "snippet": "disclaimer! • the explanations in the previous few slides are handwavy! • for rigorous proofs and derivations, please refer to! on the of training rec"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p46#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 46, "snippet": "long short - term memory ( lstm ) 1! 46 • the lstm uses this idea of “ constant error flow ” for rnns to create a “ constant error carousel ” ( cec ) "}
{"id": "DGM/Slides/illinopis Rnn.pdf#p47#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 47, "snippet": "the lstm idea! cell! ht! 47 xt!!! ht - 1!! ct = ct−1 + ct! ht = tanhct w! * dashed line indicates time - lag!!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p48#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 48, "snippet": "the original lstm cell! it! ot! input gate! output gate! cell! ht! 48 xt ht - 1!! xt ht - 1!! ct = ct−1 + ct! ht = ot⊗tanhctit = + for ot! xt!!! ht - "}
{"id": "DGM/Slides/illinopis Rnn.pdf#p49#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 49, "snippet": "the popular lstm cell! it! ot! ft! input gate! output gate! forget gate! ht! 49 xt ht - 1!! cell! ct! ct = ft⊗ct−1 + xt ht - 1!! xt ht - 1!! xt!!! ht "}
{"id": "DGM/Slides/illinopis Rnn.pdf#p50#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 50, "snippet": "lstm – forward / backward! 50 go to : illustrated lstm forward and backward pass!"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p51#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 51, "snippet": "summary! 51 • rnns allow for processing of variable length inputs and outputs by maintaining state information across time steps! • various input - ou"}
{"id": "DGM/Slides/illinopis Rnn.pdf#p52#c1", "source": "DGM/Slides/illinopis Rnn.pdf", "page": 52, "snippet": "other useful resources / references! 52 • http : / / cs231n. stanford. edu / slides / winter1516 _ lecture10. pdf! • http : / / www. cs. toronto. edu "}
{"id": "DGM/Slides/Autoencoders_1.pptx#p1#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 1, "snippet": "autoencoders supervised learning uses explicit labels / correct output in order to train a network. e. g., classification of images. unsupervised lear"}
{"id": "DGM/Slides/Autoencoders_1.pptx#p2#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 2, "snippet": "autoencoders autoencoders are designed to reproduce their input, especially for images. key point is to reproduce the input from a learned encoding. h"}
{"id": "DGM/Slides/Autoencoders_1.pptx#p3#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 3, "snippet": "autoencoders compare pca / svd pca takes a collection of vectors ( images ) and produces a usually smaller set of vectors that can be used to approxim"}
{"id": "DGM/Slides/Autoencoders_1.pptx#p4#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 4, "snippet": "autoencoders : structure encoder : compress input into a latent - space of usually smaller dimension. h = f ( x ) decoder : reconstruct input from the"}
{"id": "DGM/Slides/Autoencoders_1.pptx#p5#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 5, "snippet": "autoencoders : applications denoising : input clean image + noise and train to reproduce the clean image. https : / / www. edureka. co / blog / autoen"}
{"id": "DGM/Slides/Autoencoders_1.pptx#p6#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 6, "snippet": "autoencoders : applications image colorization : input black and white and train to produce color images https : / / www. edureka. co / blog / autoenc"}
{"id": "DGM/Slides/Autoencoders_1.pptx#p7#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 7, "snippet": "autoencoders : applications watermark removal https : / / www. edureka. co / blog / autoencoders - tutorial /"}
{"id": "DGM/Slides/Autoencoders_1.pptx#p8#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 8, "snippet": "properties of autoencoders data - specific : autoencoders are only able to compress data similar to what they have been trained on. lossy : the decomp"}
{"id": "DGM/Slides/Autoencoders_1.pptx#p9#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 9, "snippet": "capacity as with other nns, overfitting is a problem when capacity is too large for the data. autoencoders address this through some combination of : "}
{"id": "DGM/Slides/Autoencoders_1.pptx#p10#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 10, "snippet": "bottleneck layer ( undercomplete ) suppose input images are nxn and the latent space is m < nxn. then the latent space is not sufficient to reproduce "}
{"id": "DGM/Slides/Autoencoders_1.pptx#p11#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 11, "snippet": "simple bottleneck layer in keras input _ img = input ( shape = ( 784, ) ) encoding _ dim = 32 encoded = dense ( encoding _ dim, activation = ' relu ' "}
{"id": "DGM/Slides/Autoencoders_1.pptx#p12#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 12, "snippet": "denoising autoencoders basic autoencoder trains to minimize the loss between x and the reconstruction g ( f ( x ) ). denoising autoencoders train to m"}
{"id": "DGM/Slides/Autoencoders_1.pptx#p13#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 13, "snippet": "denoising autoencoders denoising autoencoders can ’ t simply memorize the input output relationship. intuitively, a denoising autoencoder learns a pro"}
{"id": "DGM/Slides/Autoencoders_1.pptx#p14#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 14, "snippet": "sparse autoencoders construct a loss function to penalize activations within a layer. usually regularize the weights of a network, not the activations"}
{"id": "DGM/Slides/Autoencoders_1.pptx#p15#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 15, "snippet": "sparse autoencoders construct a loss function to penalize activations the network. l1 regularization : penalize the absolute value of the vector of ac"}
{"id": "DGM/Slides/Autoencoders_1.pptx#p16#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 16, "snippet": "contractive autoencoders arrange for similar inputs to have similar activations. i. e., the derivative of the hidden layer activations are small with "}
{"id": "DGM/Slides/Autoencoders_1.pptx#p17#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 17, "snippet": "contractive autoencoders contractive autoencoders make the feature extraction function ( ie. encoder ) resist infinitesimal perturbations of the input"}
{"id": "DGM/Slides/Autoencoders_1.pptx#p18#c1", "source": "DGM/Slides/Autoencoders_1.pptx", "page": 18, "snippet": "autoencoders both the denoising and contractive autoencoder can perform well advantage of denoising autoencoder : simpler to implement - requires addi"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p1#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 1, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 20161 lecture"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p2#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 2, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 20162 adminis"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p3#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 3, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 20163 mini - "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p4#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 4, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 20164 image c"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p5#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 5, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 20165 forces "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p6#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 6, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 20166 convolu"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p7#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 7, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 20167 a bit o"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p8#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 8, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 20168 hierarc"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p9#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 9, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 20169 convolu"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p10#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 10, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201610 32 32 "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p11#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 11, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201611 32 32 "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p12#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 12, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201612 32 32 "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p13#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 13, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201613 32 32 "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p14#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 14, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201614 32 32 "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p15#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 15, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201615 32 32 "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p16#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 16, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201616 32 32 "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p17#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 17, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201617 previe"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p18#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 18, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201618 previe"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p19#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 19, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201619 previe"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p20#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 20, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201620 previe"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p21#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 21, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201621 exampl"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p22#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 22, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201622 previe"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p23#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 23, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201623 a clos"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p24#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 24, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201624 7x7 in"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p25#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 25, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201625 7x7 in"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p26#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 26, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201626 7x7 in"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p27#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 27, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201627 7x7 in"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p28#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 28, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201628 7x7 in"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p29#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 29, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201629 7x7 in"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p30#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 30, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201630 7x7 in"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p31#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 31, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201631 7x7 in"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p32#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 32, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201632 7x7 in"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p33#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 33, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201633 7x7 in"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p34#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 34, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201634 n n f "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p35#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 35, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201635 in pra"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p36#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 36, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201636 in pra"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p37#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 37, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201637 in pra"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p38#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 38, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201638 rememb"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p39#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 39, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201639 exampl"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p40#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 40, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201640 exampl"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p41#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 41, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201641 exampl"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p42#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 42, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201642 exampl"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p43#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 43, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201643"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p44#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 44, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201644 common"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p45#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 45, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201645 ( btw,"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p46#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 46, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201646 exampl"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p47#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 47, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201647 exampl"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p48#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 48, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201648 exampl"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p49#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 49, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201649 the br"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p50#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 50, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201650 the br"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p51#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 51, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201651 the br"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p52#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 52, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201652 the br"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p53#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 53, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201653 two mo"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p54#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 54, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201654 poolin"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p55#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 55, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201655 1 1 2 "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p56#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 56, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201656"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p57#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 57, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201657 common"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p58#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 58, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201658 fully "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p59#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 59, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201659 http :"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p60#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 60, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201660 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p61#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 61, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201661 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p62#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 62, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201662 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p63#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 63, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201663 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p64#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 64, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201664 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p65#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 65, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201665 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p66#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 66, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201666 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p67#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 67, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201667 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p68#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 68, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201668 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p69#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 69, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201669 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p70#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 70, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201670 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p71#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 71, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201671 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p72#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 72, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201672 input "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p73#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 73, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201673 input "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p74#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 74, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201674 input "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p75#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 75, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 2016 75 case "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p76#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 76, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201676 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p77#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 77, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201677 slide "}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p78#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 78, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201678 ( slid"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p79#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 79, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201679"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p80#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 80, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201680 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p81#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 81, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201681 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p82#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 82, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201682 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p83#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 83, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201683 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p84#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 84, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201684 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p85#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 85, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201685 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p86#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 86, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201686 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p87#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 87, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201687 case s"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p88#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 88, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201688 policy"}
{"id": "DGM/Slides/CNN_3 Stanford.pdf#p89#c1", "source": "DGM/Slides/CNN_3 Stanford.pdf", "page": 89, "snippet": "lecture 7 - 27 jan 2016fei - fei li & andrej karpathy & justin johnsonfei - fei li & andrej karpathy & justin johnson lecture 7 - 27 jan 201689 summar"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p1#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 1, "snippet": "csc321 lecture 9 recurrent neural nets roger grosse and nitish srivastava february 3, 2015 roger grosse and nitish srivastava csc321 lecture 9 recurre"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p2#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 2, "snippet": "overview you just implemented a neural probabilistic language model for assignment 1 : roger grosse and nitish srivastava csc321 lecture 9 recurrent n"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p3#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 3, "snippet": "overview recall that we made a markov assumption : p ( wi | w1,..., wi −1 ) = p ( wi | wi −3, wi −2, wi −1 ). this means the model is memoryless, i. e"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p4#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 4, "snippet": "overview recall that we made a markov assumption : p ( wi | w1,..., wi −1 ) = p ( wi | wi −3, wi −2, wi −1 ). this means the model is memoryless, i. e"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p5#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 5, "snippet": "recurrent neural nets rnns are a kind of neural net model which use hidden units to remember things over time. when we compute with them, we unroll th"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p6#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 6, "snippet": "recurrent neural nets one way to use rnns to model text : as with our language model, each word is represented as an indicator vector, the model predi"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p7#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 7, "snippet": "recurrent neural nets when we generate from the model ( i. e. compute samples from its distribution over sentences ), the outputs feed back in to the "}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p8#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 8, "snippet": "recurrent neural nets another approach is to model text one character at a time! this solves the problem of what to do about previously unseen words. "}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p9#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 9, "snippet": "recurrent neural nets from ’ s lecture video ( optional lecture h ), an example of a paragraph generated by an rnn language model one character at a t"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p10#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 10, "snippet": "question 1 : rnn examples now let ’ s look at some simple examples of rnns. this one sums its inputs : 2 2 2 w = 1 w = 1 - 0. 5 1. 5 1. 5 w = 1 w = 1 "}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p11#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 11, "snippet": "question 1 : rnn examples what do these rnns do? input unit logistic hidden unit 1 linear output unit w = 1 w = 5 bias = - 3 logistic hidden unit 2 w "}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p12#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 12, "snippet": "question 1 : rnn examples this one determines if the total values of the ﬁrst or second input are larger : input unit1 linear hidden unit logisticoutp"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p13#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 13, "snippet": "finite state machines a major motivation for rnns is that they can perform computations. finite state machines ( fsms ) are a simple model of computat"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p14#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 14, "snippet": "finite state machines a major motivation for rnns is that they can perform computations. finite state machines ( fsms ) are a simple model of computat"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p15#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 15, "snippet": "finite state machines what does this fsm do? roger grosse and nitish srivastava csc321 lecture 9 recurrent neural nets february 3, 2015 13 / 20"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p16#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 16, "snippet": "question 2 : parity assume we have a sequence of binary inputs. we ’ ll consider how to determine the parity, i. e. whether the number of 1 ’ s is eve"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p17#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 17, "snippet": "question 2 : parity assume we have a sequence of binary inputs. we ’ ll consider how to determine the parity, i. e. whether the number of 1 ’ s is eve"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p18#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 18, "snippet": "question 2 : parity this fsm determines the parity : roger grosse and nitish srivastava csc321 lecture 9 recurrent neural nets february 3, 2015 16 / 2"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p19#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 19, "snippet": "question 2 : parity we can ’ t get by with just one hidden unit, since we need to solve the xor problem at each time step. recall that a feed - forwar"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p20#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 20, "snippet": "question 2 : parity parity is a classic example of a problem that ’ s to solve with a standard feed - forward net, but easy to solve with an rnn. said"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p21#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 21, "snippet": "what can rnns compute? in 2014, google researchers built an rnn that learns to execute simple python programs, one character at a time! learning to ex"}
{"id": "DGM/Slides/lec9_toronto univ.pdf#p22#c1", "source": "DGM/Slides/lec9_toronto univ.pdf", "page": 22, "snippet": "what can rnns compute? some example results : under review as a conference paper at iclr 2015 supplementary material input : length, nesting stack = e"}
{"id": "DGM/Slides/DGM syllabus.pdf#p1#c1", "source": "DGM/Slides/DGM syllabus.pdf", "page": 1, "snippet": "0 + voraanaih kara mit world peace university | pasa midd. mit - wpu course structure course code csd7pm05a course category programme major course tit"}
{"id": "DGM/Slides/DGM syllabus.pdf#p2#c1", "source": "DGM/Slides/DGM syllabus.pdf", "page": 2, "snippet": "| or viemaanam karat j | mit world peace mit - wpu | university | r er study : lenet, alexnet6 unit 3 recurrent neural networks : sequences of unequal"}
{"id": "DGM/Slides/DGM syllabus.pdf#p3#c1", "source": "DGM/Slides/DGM syllabus.pdf", "page": 3, "snippet": "| of vemmanamn karas mit world peace university 2 mit - wpu 4. https : / / web. stanford. edu / class / cs231a / lectures / intro _ cnn. pdf 5. https "}
{"id": "DGM/Slides/GANs.pdf#p1#c1", "source": "DGM/Slides/GANs.pdf", "page": 1, "snippet": "deep learning for computer vision generative models mina rezaei, goncalo mordido"}
{"id": "DGM/Slides/GANs.pdf#p2#c1", "source": "DGM/Slides/GANs.pdf", "page": 2, "snippet": "deep learning for computer vision slide # 2 content 1. why unsupervised learning, and why generative models? ( selected slides from stanford universit"}
{"id": "DGM/Slides/GANs.pdf#p3#c1", "source": "DGM/Slides/GANs.pdf", "page": 3, "snippet": "deep learning for computer vision slide # 3 supervised learning supervised learning data : ( x, y ) where x is data, y is label goal : learn a functio"}
{"id": "DGM/Slides/GANs.pdf#p4#c1", "source": "DGM/Slides/GANs.pdf", "page": 4, "snippet": "deep learning for computer vision slide # 4 supervised learning supervised learning data : ( x, y ) where x is data, y is label goal : learn a functio"}
{"id": "DGM/Slides/GANs.pdf#p5#c1", "source": "DGM/Slides/GANs.pdf", "page": 5, "snippet": "deep learning for computer vision slide # 5 supervised learning supervised learning data : ( x, y ) where x is data, y is label goal : learn a functio"}
{"id": "DGM/Slides/GANs.pdf#p6#c1", "source": "DGM/Slides/GANs.pdf", "page": 6, "snippet": "deep learning for computer vision slide # 6 unsupervised learning unspervised learning data : x, no labels!! goal : learn some underlying hidden struc"}
{"id": "DGM/Slides/GANs.pdf#p7#c1", "source": "DGM/Slides/GANs.pdf", "page": 7, "snippet": "deep learning for computer vision slide # 7 unsupervised learning unspervised learning data : x, no labels!! goal : learn some underlying hidden struc"}
{"id": "DGM/Slides/GANs.pdf#p8#c1", "source": "DGM/Slides/GANs.pdf", "page": 8, "snippet": "deep learning for computer vision slide # 8 unsupervised learning unspervised learning data : x, no labels!! goal : learn some underlying hidden struc"}
{"id": "DGM/Slides/GANs.pdf#p9#c1", "source": "DGM/Slides/GANs.pdf", "page": 9, "snippet": "deep learning for computer vision slide # 9 supervised vs unsupervised learning unsupervised learning data : x just data, no labels! goal : learn some"}
{"id": "DGM/Slides/GANs.pdf#p10#c1", "source": "DGM/Slides/GANs.pdf", "page": 10, "snippet": "deep learning for computer vision lecture 13 - generative models given training data, generate new samples from same distribution generative models tr"}
{"id": "DGM/Slides/GANs.pdf#p11#c1", "source": "DGM/Slides/GANs.pdf", "page": 11, "snippet": "deep learning for computer vision generative models given training data, generate new samples from same distribution generative models training data ~"}
{"id": "DGM/Slides/GANs.pdf#p12#c1", "source": "DGM/Slides/GANs.pdf", "page": 12, "snippet": "deep learning for computer vision why generative model? generative models slide # 12"}
{"id": "DGM/Slides/GANs.pdf#p13#c1", "source": "DGM/Slides/GANs.pdf", "page": 13, "snippet": "deep learning for computer vision why generative model? • increasing dataset, realistic samples for artwork, super - resolution, colorization, etc. • "}
{"id": "DGM/Slides/GANs.pdf#p14#c1", "source": "DGM/Slides/GANs.pdf", "page": 14, "snippet": "deep learning for computer vision slide # 14 taxonomy of generative models generative models figure copyright and adapted from ian goodfellow, tutoria"}
{"id": "DGM/Slides/GANs.pdf#p15#c1", "source": "DGM/Slides/GANs.pdf", "page": 15, "snippet": "deep learning for computer vision fully visible belief network explicit density model use chain rule to decompose likelihood of an image x into produc"}
{"id": "DGM/Slides/GANs.pdf#p16#c1", "source": "DGM/Slides/GANs.pdf", "page": 16, "snippet": "deep learning for computer vision fully visible belief network explicit density model use chain rule to decompose likelihood of an image x into produc"}
{"id": "DGM/Slides/GANs.pdf#p17#c1", "source": "DGM/Slides/GANs.pdf", "page": 17, "snippet": "deep learning for computer vision pixelrnn [ van der oord et al. 2016 ] dependency on previous pixels modeled using an rnn ( lstm ) generate image pix"}
{"id": "DGM/Slides/GANs.pdf#p18#c1", "source": "DGM/Slides/GANs.pdf", "page": 18, "snippet": "deep learning for computer vision pixelrnn [ van der oord et al. 2016 ] dependency on previous pixels modeled using an rnn ( lstm ) generate image pix"}
{"id": "DGM/Slides/GANs.pdf#p19#c1", "source": "DGM/Slides/GANs.pdf", "page": 19, "snippet": "deep learning for computer vision pixelrnn [ van der oord et al. 2016 ] dependency on previous pixels modeled using an rnn ( lstm ) generate image pix"}
{"id": "DGM/Slides/GANs.pdf#p20#c1", "source": "DGM/Slides/GANs.pdf", "page": 20, "snippet": "deep learning for computer vision pixelrnn [ van der oord et al. 2016 ] dependency on previous pixels modeled using an rnn ( lstm ) generate image pix"}
{"id": "DGM/Slides/GANs.pdf#p21#c1", "source": "DGM/Slides/GANs.pdf", "page": 21, "snippet": "deep learning for computer vision pixelcnn generative models slide # 21 still generate image pixels starting from corner dependency on previous pixels"}
{"id": "DGM/Slides/GANs.pdf#p22#c1", "source": "DGM/Slides/GANs.pdf", "page": 22, "snippet": "deep learning for computer vision pixelcnn vs pixelrnn generative models slide # 22 improving pixelcnn performance • gated convolutional layers • shor"}
{"id": "DGM/Slides/GANs.pdf#p23#c1", "source": "DGM/Slides/GANs.pdf", "page": 23, "snippet": "deep learning for computer vision slide # 23 taxonomy of generative models generative models figure copyright and adapted from ian goodfellow, tutoria"}
{"id": "DGM/Slides/GANs.pdf#p24#c1", "source": "DGM/Slides/GANs.pdf", "page": 24, "snippet": "deep learning for computer vision autoencoders generative models slide # 24"}
{"id": "DGM/Slides/GANs.pdf#p25#c1", "source": "DGM/Slides/GANs.pdf", "page": 25, "snippet": "deep learning for computer vision autoencoders generative models slide # 25 encoder decoderlatent"}
{"id": "DGM/Slides/GANs.pdf#p26#c1", "source": "DGM/Slides/GANs.pdf", "page": 26, "snippet": "deep learning for computer vision denoised autoencoder generative models slide # 26"}
{"id": "DGM/Slides/GANs.pdf#p27#c1", "source": "DGM/Slides/GANs.pdf", "page": 27, "snippet": "deep learning for computer vision autoencoder application generative models slide # 27 neural inpainting semantic segmentation"}
{"id": "DGM/Slides/GANs.pdf#p28#c1", "source": "DGM/Slides/GANs.pdf", "page": 28, "snippet": "deep learning for computer vision variational autoencoders ( vae ) generative models slide # 28 reconstruction loss stay close to normal ( 0, 1 )"}
{"id": "DGM/Slides/GANs.pdf#p29#c1", "source": "DGM/Slides/GANs.pdf", "page": 29, "snippet": "deep learning for computer vision variational autoencoders ( vae ) generative models slide # 29 z = µ + σθε where ε ~ normal ( 0, 1 )"}
{"id": "DGM/Slides/GANs.pdf#p30#c1", "source": "DGM/Slides/GANs.pdf", "page": 30, "snippet": "deep learning for computer vision • model : latent - variable model p ( x | z, theta ) usually specified by a neural network • inference : recognition"}
{"id": "DGM/Slides/GANs.pdf#p31#c1", "source": "DGM/Slides/GANs.pdf", "page": 31, "snippet": "deep learning for computer vision pros • flexible generative model • end - to - end gradient training • measurable objective ( and lower bound - model"}
{"id": "DGM/Slides/GANs.pdf#p32#c1", "source": "DGM/Slides/GANs.pdf", "page": 32, "snippet": "deep learning for computer vision slide # 32 taxonomy of generative models generative models figure copyright and adapted from ian goodfellow, tutoria"}
{"id": "DGM/Slides/GANs.pdf#p33#c1", "source": "DGM/Slides/GANs.pdf", "page": 33, "snippet": "deep learning for computer vision generative adversarial networks ( goodfellow et al., 2014 ) generative models slide # 35 ▪ gans or gan for short ▪ a"}
{"id": "DGM/Slides/GANs.pdf#p34#c1", "source": "DGM/Slides/GANs.pdf", "page": 34, "snippet": "deep learning for computer vision generative adversarial networks ( goodfellow et al., 2014 ) ▪ generator ( g ) that learns the real data distribution"}
{"id": "DGM/Slides/GANs.pdf#p35#c1", "source": "DGM/Slides/GANs.pdf", "page": 35, "snippet": "deep learning for computer vision generative adversarial networks ( goodfellow et al., 2014 ) ▪ both models are trained together ( minimax game ) : ▪ "}
{"id": "DGM/Slides/GANs.pdf#p36#c1", "source": "DGM/Slides/GANs.pdf", "page": 36, "snippet": "deep learning for computer vision generative adversarial networks ( goodfellow et al., 2014 ) generative models slide # 38"}
{"id": "DGM/Slides/GANs.pdf#p37#c1", "source": "DGM/Slides/GANs.pdf", "page": 37, "snippet": "deep learning for computer vision conditional gans ( cgan ) ( mirza et al., 2014 ) ▪ g and d can be conditioned by additional information y ▪ adding y"}
{"id": "DGM/Slides/GANs.pdf#p38#c1", "source": "DGM/Slides/GANs.pdf", "page": 38, "snippet": "deep learning for computer vision conditional gans ( cgan ) ( mirza et al., 2014 ) gauthier, 2015 y = senior y = mouth open generative models slide # "}
{"id": "DGM/Slides/GANs.pdf#p39#c1", "source": "DGM/Slides/GANs.pdf", "page": 39, "snippet": "deep learning for computer vision conditional gans ( cgan ) ( mirza et al., 2014 ) gauthier, 2015 y = senior y = mouth open generative models slide # "}
{"id": "DGM/Slides/GANs.pdf#p40#c1", "source": "DGM/Slides/GANs.pdf", "page": 40, "snippet": "deep learning for computer vision conditional gans ( cgan ) ( mirza et al., 2014 ) gauthier, 2015 y = senior y = mouth open generative models slide # "}
{"id": "DGM/Slides/GANs.pdf#p41#c1", "source": "DGM/Slides/GANs.pdf", "page": 41, "snippet": "deep learning for computer vision limitations of gans chart 41 1. training instability good sample generation requires reaching nash equilibrium in th"}
{"id": "DGM/Slides/GANs.pdf#p42#c1", "source": "DGM/Slides/GANs.pdf", "page": 42, "snippet": "deep learning for computer vision evaluation metrics ▪ what makes a good generative model? ■ each generated sample is indistinguishable from a real sa"}
{"id": "DGM/Slides/GANs.pdf#p43#c1", "source": "DGM/Slides/GANs.pdf", "page": 43, "snippet": "deep learning for computer vision evaluation metrics ▪ how to evaluate the generated samples? ■ cannot rely on the models ’ loss : - ( ■ human evaluat"}
{"id": "DGM/Slides/GANs.pdf#p44#c1", "source": "DGM/Slides/GANs.pdf", "page": 44, "snippet": "deep learning for computer vision evaluation metrics ▪ inception score ( is ) [ salimans et al., 2016 ] ■ inception model ( szegedy et al., 2015 ) tra"}
{"id": "DGM/Slides/GANs.pdf#p45#c1", "source": "DGM/Slides/GANs.pdf", "page": 45, "snippet": "deep learning for computer vision evaluation metrics ▪ frechet inception distance ( fid ) [ heusel et al., 2017 ] ■ calculates the distance between re"}
{"id": "DGM/Slides/GANs.pdf#p46#c1", "source": "DGM/Slides/GANs.pdf", "page": 46, "snippet": "deep learning for computer vision evaluation metrics ▪ is vs fid fid considers the real dataset fid requires less sampling ( faster ) ( ~ 10k instead "}
{"id": "DGM/Slides/GANs.pdf#p47#c1", "source": "DGM/Slides/GANs.pdf", "page": 47, "snippet": "deep learning for computer vision ▪ mnist ( handwritten dataset ) ▪ condition the number generation per row https : / / github. com / gftm / class _ g"}
{"id": "DGM/Slides/GANs.pdf#p48#c1", "source": "DGM/Slides/GANs.pdf", "page": 48, "snippet": "deep learning for computer vision practical scenario generative models slide # 50 ▪ task 1 - add label as input to both models ( plus the combined mod"}
{"id": "DGM/Slides/GANs.pdf#p49#c1", "source": "DGM/Slides/GANs.pdf", "page": 49, "snippet": "deep learning for computer vision practical scenario generative models slide # 51 ▪ task 1 - add label as input to both models ( plus the combined mod"}
{"id": "DGM/Slides/GANs.pdf#p50#c1", "source": "DGM/Slides/GANs.pdf", "page": 50, "snippet": "deep learning for computer vision practical scenario generative models slide # 52 ▪ task 1 - add label as input to both models ( plus the combined mod"}
{"id": "DGM/Slides/GANs.pdf#p51#c1", "source": "DGM/Slides/GANs.pdf", "page": 51, "snippet": "deep learning for computer vision practical scenario generative models slide # 53 ▪ task 2 - get labels ( y ) from dataset ■ def train ( self, epochs,"}
{"id": "DGM/Slides/GANs.pdf#p52#c1", "source": "DGM/Slides/GANs.pdf", "page": 52, "snippet": "deep learning for computer vision practical scenario generative models slide # 54 ▪ task 3 - add labels to the models ’ losses ■ def train ( self, epo"}
{"id": "DGM/Slides/GANs.pdf#p53#c1", "source": "DGM/Slides/GANs.pdf", "page": 53, "snippet": "deep learning for computer vision practical scenario generative models slide # 55 ▪ task 4 - generate specific numbers for each row ■ def sample _ ima"}
{"id": "DGM/Slides/GANs.pdf#p54#c1", "source": "DGM/Slides/GANs.pdf", "page": 54, "snippet": "deep learning for computer vision references ■ a. radford, l. mety, and s. chintala. 2016. unsupervised representation learning with deep convolutiona"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p1#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 1, "snippet": "6. 874, 6. 802, 20. 390, 20. 490, hst. 506 computational systems biology deep learning in the life sciences lecture 3 : convolutional neural networks "}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p2#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 2, "snippet": "today : convolutional neural networks ( cnns ) 1. scene understanding and object recognition for machines ( and humans ) – scene / object recognition "}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p3#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 3, "snippet": "1a. what do you see, and how? can we teach machines to see?"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p4#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 4, "snippet": "what do you see?"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p5#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 5, "snippet": "how do you see? how can we help computers see?"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p6#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 6, "snippet": "what computers ‘ see ’ : images as numbers what the computer \" sees \" levin image processing & computer vision an image is just a matrix of numbers [ "}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p7#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 7, "snippet": "1b. classical machine vision roots in study of human / animal brains"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p8#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 8, "snippet": "inspiration : human / animal visual cortex • layers of neurons : pixels, edges, shapes, primitives, scenes • e. g. layer 4 responds to bands w / given"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p9#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 9, "snippet": "primitives : neurons & action potentials • chemical accumulation across dendritic connections • pre - synaptic axon post - synaptic dendrite neuronal "}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p10#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 10, "snippet": "abstraction layers : edges, bars, dir., shapes, objects, scenes lgn : small dots v1 : orientation, disparity, some color v4 : color, basic shapes, 2d "}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p11#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 11, "snippet": "• massive recent expanse of human brain has re - used a relatively simple but general learning architecture general “ learning machine ”, reused widel"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p12#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 12, "snippet": "today : convolutional neural networks ( cnns ) 1. scene understanding and object recognition for machines ( and humans ) – scene / object recognition "}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p13#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 13, "snippet": "2a. spatial structure for image recognition"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p14#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 14, "snippet": "using spatial structure idea : connect patches of input to neurons in hidden layer. neuron connected to region of input. only “ sees ” these values. i"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p15#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 15, "snippet": "using spatial structure connect patch in input layer to a single neuron in subsequent layer. use a sliding window to define connections. how can we we"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p16#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 16, "snippet": "feature extraction with convolution - filter of size 4x4 : 16 differentweights - apply this same filter to 4x4 patches in input - shift by 2 pixels fo"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p17#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 17, "snippet": "fully connected neural network fullyconnected : • each neuron in hidden layer connected to all neurons in input layer • no spatial information • many,"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p18#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 18, "snippet": "high level feature detection let ’ s identify key features in each image category wheels, licenseplate, headlights door, windows, stepsnose, eyes, mou"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p19#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 19, "snippet": "fully connected neural network"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p20#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 20, "snippet": "2b. convolutions and filters"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p21#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 21, "snippet": "convolution operation is element wise multiply and add filter / kernel"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p22#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 22, "snippet": "producing feature maps original sharpen edge detect “ strong ” edge detect"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p23#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 23, "snippet": "a simple pattern : edges how can we detect edges with a kernel? input - 1 - 1 filter output ( goodfellow 2016 )"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p24#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 24, "snippet": "simple kernels / filters"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p25#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 25, "snippet": "x or x? image is represented as matrix of pixel values … and computers are literal! we want to be able to classify an x as an x even if it ’ s shifted"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p26#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 26, "snippet": "there are three approaches to edge cases in convolution"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p27#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 27, "snippet": "( goodfellow 2016 ) zero padding controls output size • full convolution : zero pad input so output is produced whenever an output value contains at l"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p29#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 29, "snippet": "today : convolutional neural networks ( cnns ) 1. scene understanding and object recognition for machines ( and humans ) – scene / object recognition "}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p30#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 30, "snippet": "3a. learning visual features de novo"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p31#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 31, "snippet": "key idea : learn hierarchy of features directly from the data ( rather than hand - engineering them ) low level features mid level features high level"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p32#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 32, "snippet": "key idea : re - use parameters convolution shares parameters example 3x3 convolution on a 5x5 image"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p33#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 33, "snippet": "feature extraction with convolution 1 ) apply a set of weights – a filter – to extract local features 2 ) use multiple filters to extract differentfea"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p34#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 34, "snippet": "lenet - 5 • gradient based learning applied to document recognition - y. lecun, l. bottou, y. bengio, p. haffner ; 1998 • helped establish how we use "}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p35#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 35, "snippet": "lenet - 5 32×32×1 28×28×6 14×14×6 10×10×16 5×5×16 120 84 5 × 5 s = 1 f = 2 s = 2 avg pool 5 × 5 s = 1 avg pool f = 2 s = 2...... reminder : output siz"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p36#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 36, "snippet": "lenet - 5 • only 60k parameters • as we go deeper in the network : ↓,, ↑ • general structure : conv - > pool - > conv - > pool - > fc - > fc - > outpu"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p37#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 37, "snippet": "backpropagation of convolution slide taken from forward and backpropagation in convolutional neural network. - medium"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p38#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 38, "snippet": "3b. convolutional neural networks ( cnns )"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p39#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 39, "snippet": "an image classification cnn"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p40#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 40, "snippet": "representation learning in deep cnns mid levelfeatures low levelfeatures high levelfeatures edges, darkspots conv layer1 lee + icml 2009 eyes, ears, n"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p41#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 41, "snippet": "cnns for classification 1. convolution : apply filters to generate featuremaps. 2. non - linearity : often relu. 3. pooling : downsampling operation o"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p42#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 42, "snippet": "example – six convolutional layers"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p43#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 43, "snippet": "convolutional layers : local connectivity for a neuron in hidden layer : - take inputs from patch - compute weighted sum - apply bias tf. keras. layer"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p44#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 44, "snippet": "convolutional layers : local connectivity for a neuron in hidden layer : • take inputs from patch • compute weighted sum • apply bias 4x4 filter : mat"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p45#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 45, "snippet": "cnns : spatial arrangement of output volume depth width height layerdimensions : w d where h and w are spatial dimensionsd ( depth ) = number of filte"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p46#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 46, "snippet": "introducing non - linearity rectified linear unit ( relu ) - apply after every convolution operation ( i. e., afterconvolutionallayers ) - relu : pixe"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p47#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 47, "snippet": "pooling max pooling, average pooling 1 ) reduced dimensionality 2 ) spatial invariance tf. keras. layers. max pool2d ( pool _ size = ( 2, 2 ), strides"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p48#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 48, "snippet": "the rectified linear unit ( relu ) is a common non - linear detector stage after convolution x = tf. nn. conv2d ( x, w, strides = [ 1, strides, stride"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p49#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 49, "snippet": "pooling reduces dimensionality by giving up spatial location • max pooling reports the maximum output within a defined neighborhood • padding can be s"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p50#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 50, "snippet": "dilated convolution"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p51#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 51, "snippet": "91 cnns for classification : feature learning 1. learn features in input image through convolution 2. introduce non - linearity through activation fun"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p52#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 52, "snippet": "cnns for classification : class probabilities - conv and pool layers output high - level features of input - fully connected layer uses these features"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p53#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 53, "snippet": "putting it all together import tensorflow as tf def generate _ model ( ) : model = tf. keras. sequential ( [ # first convolutional layer tf. keras. la"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p54#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 54, "snippet": "today : convolutional neural networks ( cnns ) 1. scene understanding and object recognition for machines ( and humans ) – scene / object recognition "}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p55#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 55, "snippet": "4a. real - world feature invariance is hard"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p56#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 56, "snippet": "how can computers recognize objects?"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p57#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 57, "snippet": "how can computers recognize objects? challenge : • objects can be anywhere in the scene, in any orientation, rotation, color hue, etc. • how can we ov"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p58#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 58, "snippet": "detect features to classify li / johnson / yeung c231n feature invariance to perturbation is hard"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p59#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 59, "snippet": "next - generation models explode # of parameters"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p60#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 60, "snippet": "lenet - 5 • gradient based learning applied to document recognition - y. lecun, l. bottou, y. bengio, p. haffner ; 1998 • helped establish how we use "}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p61#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 61, "snippet": "imagenet large scale visual recognition challenge ( ilsvrc ) winners slide taken from fei - fei & justin johnson & serena yeung. lecture 9."}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p62#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 62, "snippet": "alexnet • imagenet classification with deep convolutional neural networks - alex krizhevsky, ilya sutskever, geoffrey e. hinton ; 2012 • facilitated b"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p63#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 63, "snippet": "imagenet large scale visual recognition challenge ( ilsvrc ) winners • the annual “ olympics ” of computer vision. • teams from across the world compe"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p64#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 64, "snippet": "imagenet large scale visual recognition challenge ( ilsvrc ) winners slide taken from fei - fei & justin johnson & serena yeung. lecture 9."}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p65#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 65, "snippet": "alexnet [ krizhevsky et al., 2012 ] architecture conv1 max pool1 norm1 conv2 max pool2 norm2 conv3 conv4 conv5 max pool3 fc6 fc7 fc8 • input : 227x227"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p66#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 66, "snippet": "alexnet [ krizhevsky et al., 2012 ]"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p67#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 67, "snippet": "alexnet [ krizhevsky et al., 2012 ] • input : 227x227x3 images ( 224x224 before padding ) • after conv1 : 55x55x96 • second layer : 3x3 filters applie"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p68#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 68, "snippet": "alexnet... 227×227 ×3 55×55 × 96 27×27 ×96 27×27 ×256 13×13 ×256 13×13 ×384 13×13 ×384 13×13 ×256 6×6 ×256 11 × 11 s = 4 p = 0 3 × 3 s = 2 max pool 5 "}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p69#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 69, "snippet": "alexnet... 4096 4096 softmax 1000 [ krizhevsky et al., 2012 ] fc fc this slide is taken from andrew ng"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p70#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 70, "snippet": "alexnet [ krizhevsky et al., 2012 ] details / retrospectives : • first use of relu • used norm layers ( not common anymore ) • heavy data augmentation"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p71#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 71, "snippet": "alexnet [ krizhevsky et al., 2012 ] • trained on gtx 580 gpu with only 3 gb of memory. • network spread across 2 gpus, half the neurons ( feature maps"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p72#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 72, "snippet": "alexnet alexnet was the coming out party for cnns in the computer vision community. this was the first time a model performed so well on a historicall"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p73#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 73, "snippet": "imagenet large scale visual recognition challenge ( ilsvrc ) winners slide taken from fei - fei & justin johnson & serena yeung. lecture 9."}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p74#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 74, "snippet": "imagenet large scale visual recognition challenge ( ilsvrc ) winners slide taken from fei - fei & justin johnson & serena yeung. lecture 9."}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p75#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 75, "snippet": "vggnet • very deep convolutional networks for large scale image recognition - karen simonyan and andrew zisserman ; 2015 • the runner - up at the ilsv"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p76#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 76, "snippet": "vggnet • smaller filters only 3x3 conv filters, stride 1, pad 1 and 2x2 max pool, stride 2 • deeper network alexnet : 8 layers vggnet : 16 - 19 layers"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p77#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 77, "snippet": "vggnet [ simonyan and zisserman, 2014 ] • why use smaller filters? ( 3x3 conv ) stack of three 3x3 conv ( stride 1 ) layers has the same effective rec"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p78#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 78, "snippet": "vggnet [ simonyan and zisserman, 2014 ] vgg16 : total memory : 24m * 4 bytes ~ = 96mb / image total params : 138m parameters slide taken from fei - fe"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p79#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 79, "snippet": "[ simonyan and zisserman, 2014 ] slide taken from fei - fei & justin johnson & serena yeung. lecture 9. input memory : 224 * 224 * 3 = 150k params : 0"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p80#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 80, "snippet": "vggnet [ simonyan and zisserman, 2014 ] details / retrospectives : • ilsvrc ’ 14 2nd in classification, 1st in localization • similar training procedu"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p81#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 81, "snippet": "vggnet vgg net reinforced the notion that convolutional neural networks have to have a deep network of layers in order for this hierarchical represent"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p82#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 82, "snippet": "imagenet large scale visual recognition challenge ( ilsvrc ) winners slide taken from fei - fei & justin johnson & serena yeung. lecture 9."}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p83#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 83, "snippet": "googlenet • going deeper with convolutions - christian szegedy et al. ; 2015 • ilsvrc 2014 competition winner • also significantly deeper than alexnet"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p84#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 84, "snippet": "googlenet • 22 layers • efficient “ inception ” module - strayed from the general approach of simply stacking conv and pooling layers on top of each o"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p85#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 85, "snippet": "googlenet “ inception module ” : design a good local network topology ( network within a network ) and then stack these modules on top of each other s"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p86#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 86, "snippet": "googlenet details / retrospectives : • deeper networks, with computational efficiency • 22 layers • efficient “ inception ” module • no fc layers • 12"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p87#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 87, "snippet": "googlenet introduced the idea that cnn layers didn ’ t always have to be stacked up sequentially. coming up with the inception module, the authors sho"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p88#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 88, "snippet": "imagenet large scale visual recognition challenge ( ilsvrc ) winners slide taken from fei - fei & justin johnson & serena yeung. lecture 9."}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p89#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 89, "snippet": "resnet • deep residual learning for image recognition - kaiming he, xiangyu zhang, shaoqing ren, jian sun ; 2015 • extremely deep network – 152 layers"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p90#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 90, "snippet": "resnet • ilsvrc ’ 15 classification winner ( 3. 57 % top 5 error, humans generally hover around a 5 - 10 % error rate ) swept all classification and d"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p91#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 91, "snippet": "resnet • what happens when we continue stacking deeper layers on a convolutional neural network? • 56 - layer model performs worse on both training an"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p92#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 92, "snippet": "resnet • hypothesis : the problem is an optimization problem. very deep networks are harder to optimize. • solution : use network layers to fit residu"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p93#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 93, "snippet": "resnet residual block input x goes through conv - relu - conv series and gives us f ( x ). that result is then added to the original input x. let ’ s "}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p94#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 94, "snippet": "resnet short cut / skip connection [ ] [ + 2 ] [ + ] = [ + ] [ ] + [ + ] [ + ] = ( [ + ] ) [ + ] = [ + ] [ + ] + [ + ] [ + ] = ( [ + ] ) [ + 1 ] a [ l"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p95#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 95, "snippet": "resnet full resnet architecture : • stack residual blocks • every residual block has two 3x3 conv layers • periodically, double # of filters and downs"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p96#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 96, "snippet": "resnet • total depths of 34, 50, 101, or 152 layers for imagenet • for deeper networks ( resnet - 50 + ), use “ bottleneck ” layer to improve efficien"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p97#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 97, "snippet": "resnet experimental results : • able to train very deep networks without degrading • deeper networks now achieve lower training errors as expected [ h"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p98#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 98, "snippet": "resnet the best cnn architecture that we currently have and is a great innovation for the idea of residual learning. even better than human performanc"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p99#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 99, "snippet": "accuracy comparison the best cnn architecture that we currently have and is a great innovation for the idea of residual learning. slide taken from fei"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p100#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 100, "snippet": "forward pass time and power consumption the best cnn architecture that we currently have and is a great innovation for the idea of residual learning. "}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p101#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 101, "snippet": "imagenet large scale visual recognition challenge ( ilsvrc ) winners slide taken from fei - fei & justin johnson & serena yeung. lecture 9."}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p102#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 102, "snippet": "today : convolutional neural networks ( cnns ) 1. scene understanding and object recognition for machines ( and humans ) – scene / object recognition "}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p103#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 103, "snippet": "countless applications"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p104#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 104, "snippet": "an architecture for many applications detection semantic segmentation end - to - end robotic control"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p105#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 105, "snippet": "semantic segmentation : fully convolutional networks fcn : fully convolutionalnetwork. network designed with all convolutional layers, with downsampli"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p106#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 106, "snippet": "facial detection & recognition"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p107#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 107, "snippet": "self - driving cars amini + icra 2019."}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p108#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 108, "snippet": "self - driving cars : navigation from visual perception raw perception i ( ex. camera ) coarse maps m ( ex. gps ) possible controlcommands amini + icr"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p109#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 109, "snippet": "end - to - end framework for autonomous navigation entire model trained end - to - end without any human labelling or annotations amini + icra 2019"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p110#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 110, "snippet": "automatic colorization of black and white images"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p111#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 111, "snippet": "optimizing images post processing feature optimization ( illumination ) post processing feature optimization ( color curves and details ) post process"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p112#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 112, "snippet": "up - scaling low - resolution images"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p113#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 113, "snippet": "medicine, biology, healthcare gulshan + jama 2016."}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p114#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 114, "snippet": "breast cancer screening 6. breast cancer case missed by radiologist but detected byai ai md readers ai md readers cnn - based system outperformed expe"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p115#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 115, "snippet": "semantic segmentation : biomedical image analysis brain t umors dong + miua 2017. malaria infection soleimany + arxiv 2019. dong + miua 2017 ; soleima"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p116#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 116, "snippet": "deepbind [ alipanahi et al., 2015 ]"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p117#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 117, "snippet": "predicting disease mutations [ alipanahi et al., 2015 ]"}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p118#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 118, "snippet": "today : convolutional neural networks ( cnns ) 1. scene understanding and object recognition for machines ( and humans ) – scene / object recognition "}
{"id": "DGM/Slides/CNN_good slides_stanford.pdf#p119#c1", "source": "DGM/Slides/CNN_good slides_stanford.pdf", "page": 119, "snippet": "deep learning for computer vision : summary foundations • why computer vision? • representingimages • convolutions forfeature extraction cnns • cnn ar"}
{"id": "DGM/Slides/CNN_2.pdf#p1#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 1, "snippet": "convolutional neural networks intelligent systems for pattern recognition ( ispr ) davide bacciu dipartimento di informatica universita di pisa"}
{"id": "DGM/Slides/CNN_2.pdf#p2#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 2, "snippet": "lecture outline • introduction and historical perspective • dissecting the components of a cnn • convolution, stride, pooling • cnn architectures for "}
{"id": "DGM/Slides/CNN_2.pdf#p3#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 3, "snippet": "introduction introduction convolutional nn advanced topics outline history convolution fundamentals convolutional neural networks"}
{"id": "DGM/Slides/CNN_2.pdf#p4#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 4, "snippet": "introduction introduction convolutional nn advanced topics outline history convolution fundamentals destroying machine vision research since 2012 conv"}
{"id": "DGM/Slides/CNN_2.pdf#p5#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 5, "snippet": "neocognitron introduction convolutional nn advanced topics outline history convolution fundamentals • hubel - wiesel ( ‘ 59 ) model of brain visual pr"}
{"id": "DGM/Slides/CNN_2.pdf#p6#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 6, "snippet": "cnn for sequences • apply a bank of 16 convolution kernels to sequences ( windows of 15 elements ) • trained by backpropagation with parameter sharing"}
{"id": "DGM/Slides/CNN_2.pdf#p7#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 7, "snippet": "cnn for images first convolutional neural network for images dates back to 1989 ( lecun ) introduction convolutional nn advanced topics outline histor"}
{"id": "DGM/Slides/CNN_2.pdf#p8#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 8, "snippet": "a revised bio - inspired model ( hmax ) learning hierarchical representation of objects with the hubel - wiesel model ( riesenhuber & poggio, 1999 ) i"}
{"id": "DGM/Slides/CNN_2.pdf#p9#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 9, "snippet": "dense vector multiplication processing images : the dense way introduction convolutional nn advanced topics outline history convolution fundamentals 3"}
{"id": "DGM/Slides/CNN_2.pdf#p10#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 10, "snippet": "convolution ( refresher ) introduction convolutional nn advanced topics outline history convolution fundamentals 32x32 matrix input preserving spatial"}
{"id": "DGM/Slides/CNN_2.pdf#p11#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 11, "snippet": "adaptive convolution introduction convolutional nn advanced topics outline history convolution fundamentals 1 0 1 2 3 4 1 0 1 1 0 1 0 2 0 1 0 1, 2 = +"}
{"id": "DGM/Slides/CNN_2.pdf#p12#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 12, "snippet": "convolutional features introduction convolutional nn advanced topics outline history convolution fundamentals 32x32 slide the filter on the image comp"}
{"id": "DGM/Slides/CNN_2.pdf#p13#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 13, "snippet": "multi - channel convolution introduction convolutional nn advanced topics outline history convolution fundamentals 32x32x3 5x5x3 convolution filter ha"}
{"id": "DGM/Slides/CNN_2.pdf#p14#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 14, "snippet": "multi - channel convolution introduction convolutional nn advanced topics outline history convolution fundamentals 28x28 all channels are typically co"}
{"id": "DGM/Slides/CNN_2.pdf#p15#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 15, "snippet": "stride • basic convolution slides the filter on the image one pixel at a time • stride = 1 introduction convolutional nn advanced topics outline histo"}
{"id": "DGM/Slides/CNN_2.pdf#p16#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 16, "snippet": "stride • basic convolution slides the filter on the image one pixel at a time • stride = 1 introduction convolutional nn advanced topics outline histo"}
{"id": "DGM/Slides/CNN_2.pdf#p17#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 17, "snippet": "stride • basic convolution slides the filter on the image one pixel at a time • stride = 1 introduction convolutional nn advanced topics outline histo"}
{"id": "DGM/Slides/CNN_2.pdf#p18#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 18, "snippet": "stride • basic convolution slides the filter on the image one pixel at a time • stride = 1 introduction convolutional nn advanced topics outline histo"}
{"id": "DGM/Slides/CNN_2.pdf#p19#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 19, "snippet": "stride • basic convolution slides the filter on the image one pixel at a time • stride = 1 • can define a different stride • hyperparameter introducti"}
{"id": "DGM/Slides/CNN_2.pdf#p20#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 20, "snippet": "stride • basic convolution slides the filter on the image one pixel at a time • stride = 1 • can define a different stride • hyperparameter introducti"}
{"id": "DGM/Slides/CNN_2.pdf#p21#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 21, "snippet": "stride • basic convolution slides the filter on the image one pixel at a time • stride = 1 • can define a different stride • hyperparameter introducti"}
{"id": "DGM/Slides/CNN_2.pdf#p22#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 22, "snippet": "stride • basic convolution slides the filter on the image one pixel at a time • stride = 1 • can define a different stride • hyperparameter introducti"}
{"id": "DGM/Slides/CNN_2.pdf#p23#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 23, "snippet": "stride • basic convolution slides the filter on the image one pixel at a time • stride = 1 • can define a different stride • hyperparameter introducti"}
{"id": "DGM/Slides/CNN_2.pdf#p24#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 24, "snippet": "stride • basic convolution slides the filter on the image one pixel at a time • stride = 1 • can define a different stride • hyperparameter • stride r"}
{"id": "DGM/Slides/CNN_2.pdf#p25#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 25, "snippet": "stride • basic convolution slides the filter on the image one pixel at a time • stride = 1 • can define a different stride • hyperparameter • stride r"}
{"id": "DGM/Slides/CNN_2.pdf#p26#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 26, "snippet": "stride • basic convolution slides the filter on the image one pixel at a time • stride = 1 • can define a different stride • hyperparameter • stride r"}
{"id": "DGM/Slides/CNN_2.pdf#p27#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 27, "snippet": "activation map size introduction convolutional nn advanced topics outline history convolution fundamentals what is the size of the image after applica"}
{"id": "DGM/Slides/CNN_2.pdf#p28#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 28, "snippet": "activation map size introduction convolutional nn advanced topics outline history convolution fundamentals what is the size of the image after applica"}
{"id": "DGM/Slides/CNN_2.pdf#p29#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 29, "snippet": "activation map size introduction convolutional nn advanced topics outline history convolution fundamentals what is the size of the image after applica"}
{"id": "DGM/Slides/CNN_2.pdf#p30#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 30, "snippet": "activation map size introduction convolutional nn advanced topics outline history convolution fundamentals what is the size of the image after applica"}
{"id": "DGM/Slides/CNN_2.pdf#p31#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 31, "snippet": "zero padding introduction convolutional nn advanced topics outline history convolution fundamentals 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 add columns and "}
{"id": "DGM/Slides/CNN_2.pdf#p32#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 32, "snippet": "zero padding introduction convolutional nn advanced topics outline history convolution fundamentals 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 add columns and "}
{"id": "DGM/Slides/CNN_2.pdf#p33#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 33, "snippet": "zero padding introduction convolutional nn advanced topics outline history convolution fundamentals 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 add columns and "}
{"id": "DGM/Slides/CNN_2.pdf#p34#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 34, "snippet": "feature map transformation • convolution is a linear operator • apply an element - wise nonlinearity to obtain a transformed feature map introduction "}
{"id": "DGM/Slides/CNN_2.pdf#p35#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 35, "snippet": "pooling • operates on the feature map to make the representation • smaller ( subsampling ) • robust to ( some ) transformations introduction convoluti"}
{"id": "DGM/Slides/CNN_2.pdf#p36#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 36, "snippet": "pooling facts • max pooling is the one used more frequently, but other forms are possible • average pooling • l2 - norm pooling • random pooling • it "}
{"id": "DGM/Slides/CNN_2.pdf#p37#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 37, "snippet": "the convolutional architecture introduction convolutional nn advanced topics model notable architectures visualizing convolutions convolutional filter"}
{"id": "DGM/Slides/CNN_2.pdf#p38#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 38, "snippet": "a bigger picture introduction convolutional nn advanced topics model notable architectures visualizing convolutions input cl 1 cl 2 cl 3 cl 4 fcl 1 fc"}
{"id": "DGM/Slides/CNN_2.pdf#p39#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 39, "snippet": "convolutional filter banks introduction convolutional nn advanced topics model notable architectures visualizing convolutions × × × × × convolutional "}
{"id": "DGM/Slides/CNN_2.pdf#p40#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 40, "snippet": "specifying cnn in code ( keras ) introduction convolutional nn advanced topics model notable architectures visualizing convolutions model = sequential"}
{"id": "DGM/Slides/CNN_2.pdf#p41#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 41, "snippet": "a note on convolution • we know that discrete convolution between and image i and a filter / kernel k is introduction convolutional nn advanced topics"}
{"id": "DGM/Slides/CNN_2.pdf#p42#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 42, "snippet": "cnn as a sparse neural network let us take a 1 - d input ( sequence ) to ease graphics introduction convolutional nn advanced topics model notable arc"}
{"id": "DGM/Slides/CNN_2.pdf#p43#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 43, "snippet": "dense network the dense counterpart would look like this introduction convolutional nn advanced topics model notable architectures visualizing convolu"}
{"id": "DGM/Slides/CNN_2.pdf#p44#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 44, "snippet": "strided convolution introduction convolutional nn advanced topics model notable architectures visualizing convolutions make connectivity sparser"}
{"id": "DGM/Slides/CNN_2.pdf#p45#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 45, "snippet": "max - pooling and spatial invariance introduction convolutional nn advanced topics model notable architectures visualizing convolutions pooling featur"}
{"id": "DGM/Slides/CNN_2.pdf#p46#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 46, "snippet": "cross channel pooling and spatial invariance introduction convolutional nn advanced topics model notable architectures visualizing convolutions input "}
{"id": "DGM/Slides/CNN_2.pdf#p47#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 47, "snippet": "hierarchical feature organization introduction convolutional nn advanced topics model notable architectures visualizing convolutions the deeper the la"}
{"id": "DGM/Slides/CNN_2.pdf#p48#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 48, "snippet": "zero - padding effect introduction convolutional nn advanced topics model notable architectures visualizing convolutions assuming no pooling"}
{"id": "DGM/Slides/CNN_2.pdf#p49#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 49, "snippet": "cnn training introduction convolutional nn advanced topics model notable architectures visualizing convolutions variants of the standard backpropagati"}
{"id": "DGM/Slides/CNN_2.pdf#p50#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 50, "snippet": "backpropagating on convolution introduction convolutional nn advanced topics model notable architectures visualizing convolutions k = 3, s = 1 convolu"}
{"id": "DGM/Slides/CNN_2.pdf#p51#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 51, "snippet": "deconvolution ( transposed convolution ) introduction convolutional nn advanced topics model notable architectures visualizing convolutions k = 3, s ="}
{"id": "DGM/Slides/CNN_2.pdf#p52#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 52, "snippet": "deconvolution ( transposed convolution ) introduction convolutional nn advanced topics model notable architectures visualizing convolutions k = 3, s ="}
{"id": "DGM/Slides/CNN_2.pdf#p53#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 53, "snippet": "lenet - 5 ( 1989 ) • grayscale images • filters are 5x5 with stride 1 ( sigmoid nonlinearity ) • pooling is 2x2 with stride 2 • no zero padding introd"}
{"id": "DGM/Slides/CNN_2.pdf#p54#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 54, "snippet": "alexnet ( 2012 ) - architecture introduction convolutional nn advanced topics model notable architectures visualizing convolutions • rgb images 227x22"}
{"id": "DGM/Slides/CNN_2.pdf#p55#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 55, "snippet": "alexnet - innovations introduction convolutional nn advanced topics model notable architectures visualizing convolutions • use heavy data augmentation"}
{"id": "DGM/Slides/CNN_2.pdf#p56#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 56, "snippet": "relu nonlinearity introduction convolutional nn advanced topics model notable architectures visualizing convolutions • relu help counteract gradient v"}
{"id": "DGM/Slides/CNN_2.pdf#p57#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 57, "snippet": "alexnet - parameters introduction convolutional nn advanced topics model notable architectures visualizing convolutions • 62. 3 millions of parameters"}
{"id": "DGM/Slides/CNN_2.pdf#p58#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 58, "snippet": "vggnet – vgg16 ( 2014 ) introduction convolutional nn advanced topics model notable architectures visualizing convolutions • standardized convolutiona"}
{"id": "DGM/Slides/CNN_2.pdf#p59#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 59, "snippet": "googlenet ( 2015 ) introduction convolutional nn advanced topics model notable architectures visualizing convolutions inception module • kernels of di"}
{"id": "DGM/Slides/CNN_2.pdf#p60#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 60, "snippet": "1x1 convolutions are helpful introduction convolutional nn advanced topics model notable architectures visualizing convolutions 56x56x64 take 5 kernel"}
{"id": "DGM/Slides/CNN_2.pdf#p61#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 61, "snippet": "back on googlenet introduction convolutional nn advanced topics model notable architectures visualizing convolutions • only 5 millions of parameters •"}
{"id": "DGM/Slides/CNN_2.pdf#p62#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 62, "snippet": "batch normalization • very deep neural network are subject to internal covariate shift • distribution of inputs to a layer n might vary ( shift ) with"}
{"id": "DGM/Slides/CNN_2.pdf#p63#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 63, "snippet": "resnet ( 2015 ) introduction convolutional nn advanced topics model notable architectures visualizing convolutions begin of the ultra - deep network e"}
{"id": "DGM/Slides/CNN_2.pdf#p64#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 64, "snippet": "resnet trick introduction convolutional nn advanced topics model notable architectures visualizing convolutions 3x3 convolution 3x3 convolution relu x"}
{"id": "DGM/Slides/CNN_2.pdf#p65#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 65, "snippet": "cnn architecture evolution introduction convolutional nn advanced topics model notable architectures visualizing convolutions"}
{"id": "DGM/Slides/CNN_2.pdf#p66#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 66, "snippet": "understanding cnn embedding tsne projection of alexnet last hidden dense layer introduction convolutional nn advanced topics model notable architectur"}
{"id": "DGM/Slides/CNN_2.pdf#p67#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 67, "snippet": "interpreting intermediate levels • what about the information captured in convolutional layers? • visualize kernel weights ( filters ) • naive approac"}
{"id": "DGM/Slides/CNN_2.pdf#p68#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 68, "snippet": "deconvolutional network ( deconvnet ) introduction convolutional nn advanced topics model notable architectures visualizing convolutions • attach a de"}
{"id": "DGM/Slides/CNN_2.pdf#p69#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 69, "snippet": "filters & patches – layer 1 introduction convolutional nn advanced topics model notable architectures visualizing convolutions zeiler & fergus, visual"}
{"id": "DGM/Slides/CNN_2.pdf#p70#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 70, "snippet": "filters & patches – layer 2 introduction convolutional nn advanced topics model notable architectures visualizing convolutions zeiler & fergus, visual"}
{"id": "DGM/Slides/CNN_2.pdf#p71#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 71, "snippet": "filters & patches – layer 3 introduction convolutional nn advanced topics model notable architectures visualizing convolutions zeiler & fergus, visual"}
{"id": "DGM/Slides/CNN_2.pdf#p72#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 72, "snippet": "filters & patches – layer 4 introduction convolutional nn advanced topics model notable architectures visualizing convolutions zeiler & fergus, visual"}
{"id": "DGM/Slides/CNN_2.pdf#p73#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 73, "snippet": "filters & patches – layer 5 introduction convolutional nn advanced topics model notable architectures visualizing convolutions zeiler & fergus, visual"}
{"id": "DGM/Slides/CNN_2.pdf#p74#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 74, "snippet": "occlusions • measure what happens to feature maps and object classification if we occlude part of the image • slide a grey mask on the image and proje"}
{"id": "DGM/Slides/CNN_2.pdf#p75#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 75, "snippet": "occlusions introduction convolutional nn advanced topics model notable architectures visualizing convolutions zeiler & fergus, visualizing and underst"}
{"id": "DGM/Slides/CNN_2.pdf#p76#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 76, "snippet": "dense cnn introduction convolutional nn advanced topics advanced convolutional models applications conclusions batch normalization + relu + 3x3 conv •"}
{"id": "DGM/Slides/CNN_2.pdf#p77#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 77, "snippet": "causal convolutions introduction convolutional nn advanced topics advanced convolutional models applications conclusions preventing a convolution from"}
{"id": "DGM/Slides/CNN_2.pdf#p78#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 78, "snippet": "causal & dilated convolutions introduction convolutional nn advanced topics advanced convolutional models applications conclusions ( ∗ ) (, ) = −, − ("}
{"id": "DGM/Slides/CNN_2.pdf#p79#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 79, "snippet": "convolutional autoencoder introduction convolutional nn advanced topics advanced convolutional models applications conclusions best of two worlds trai"}
{"id": "DGM/Slides/CNN_2.pdf#p80#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 80, "snippet": "semantic segmentation introduction convolutional nn advanced topics advanced convolutional models applications conclusions traditional cnn cannot be u"}
{"id": "DGM/Slides/CNN_2.pdf#p81#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 81, "snippet": "fully convolutional networks ( fcn ) introduction convolutional nn advanced topics advanced convolutional models applications conclusions convolutiona"}
{"id": "DGM/Slides/CNN_2.pdf#p82#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 82, "snippet": "deconvolution architecture introduction convolutional nn advanced topics advanced convolutional models applications conclusions badrinarayanan et al, "}
{"id": "DGM/Slides/CNN_2.pdf#p83#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 83, "snippet": "segnet segmentation introduction convolutional nn advanced topics advanced convolutional models applications conclusions demo here : http : / / mi. en"}
{"id": "DGM/Slides/CNN_2.pdf#p84#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 84, "snippet": "use dilated convolutions introduction convolutional nn advanced topics advanced convolutional models applications conclusions always perform 3x3 convo"}
{"id": "DGM/Slides/CNN_2.pdf#p85#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 85, "snippet": "segmentation by dilated cnn introduction convolutional nn advanced topics advanced convolutional models applications conclusions yu et al, multi - sca"}
{"id": "DGM/Slides/CNN_2.pdf#p86#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 86, "snippet": "cnn & genomic sequences introduction convolutional nn advanced topics advanced convolutional models applications conclusions t a g a c a t c t … … 1d "}
{"id": "DGM/Slides/CNN_2.pdf#p87#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 87, "snippet": "deepbind introduction convolutional nn advanced topics advanced convolutional models applications conclusions alipanahi et al. \" predicting the sequen"}
{"id": "DGM/Slides/CNN_2.pdf#p88#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 88, "snippet": "deepsea introduction convolutional nn advanced topics advanced convolutional models applications conclusions t a g a c a t c t 919 outputs predicting "}
{"id": "DGM/Slides/CNN_2.pdf#p89#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 89, "snippet": "software • cnn are supported by any deep learning framework ( tf, torch, pytorch, ms cognitive tk, … ) • caffe was one of the initiators and basically"}
{"id": "DGM/Slides/CNN_2.pdf#p90#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 90, "snippet": "caffe protobuffer introduction convolutional nn advanced topics advanced convolutional models applications conclusions name : \" lenet \" layer { name :"}
{"id": "DGM/Slides/CNN_2.pdf#p91#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 91, "snippet": "software • cnn are supported by any deep learning framework ( tf, torch, pytorch, ms cognitive tk, … ) • caffe was one of the initiators and basically"}
{"id": "DGM/Slides/CNN_2.pdf#p92#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 92, "snippet": "other software • matlab distributes its neural network toolbox which allows importing pretrained models from caffe and keras - tf • matconvnet is an u"}
{"id": "DGM/Slides/CNN_2.pdf#p93#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 93, "snippet": "guis major hardware producers have gui wrapping caffe to play with cnns introduction convolutional nn advanced topics advanced convolutional models ap"}
{"id": "DGM/Slides/CNN_2.pdf#p94#c1", "source": "DGM/Slides/CNN_2.pdf", "page": 94, "snippet": "take home messages • key things • convolutions in place of dense multiplications allow sparse connectivity and weight sharing • pooling enforces invar"}
{"id": "DGM/Slides/applications of ML and DL.pdf#p1#c1", "source": "DGM/Slides/applications of ML and DL.pdf", "page": 1, "snippet": "fundamentals machine learning and deep learning christian janiesch1 & patrick zschech2 & kai heinrich 3 received : 7 october 2020 / accepted : 19 marc"}
{"id": "DGM/Slides/applications of ML and DL.pdf#p2#c1", "source": "DGM/Slides/applications of ML and DL.pdf", "page": 2, "snippet": "knowledge into a machine - accessible form and allows to de - velop intelligent systems more efficiently. during the last decades, the field of ml has"}
{"id": "DGM/Slides/applications of ML and DL.pdf#p3#c1", "source": "DGM/Slides/applications of ML and DL.pdf", "page": 3, "snippet": "depending on the learning task, the field offers various classes of ml algorithms, each of them coming in multiple specifications and variants, includ"}
{"id": "DGM/Slides/applications of ML and DL.pdf#p4#c1", "source": "DGM/Slides/applications of ML and DL.pdf", "page": 4, "snippet": "needed for the corresponding learning task. this is the net - works ’ core capability, which is commonly known as deep learning. simple anns ( e. g., "}
{"id": "DGM/Slides/applications of ML and DL.pdf#p5#c1", "source": "DGM/Slides/applications of ML and DL.pdf", "page": 5, "snippet": "vectors ( salton and buckley 1988 ), part - of - speech ( pos ) tag - ging, and word shape features ( wu et al. 2018 ). manual fea - ture design is a "}
{"id": "DGM/Slides/applications of ML and DL.pdf#p6#c1", "source": "DGM/Slides/applications of ML and DL.pdf", "page": 6, "snippet": "whereas classification models are assessed by calculating dif - ferent ratios of correctly and incorrectly predicted instances, such as accuracy, reca"}
{"id": "DGM/Slides/applications of ML and DL.pdf#p7#c1", "source": "DGM/Slides/applications of ML and DL.pdf", "page": 7, "snippet": "properties such as the type of learning mechanisms ( e. g., shal - low ml vs. dl ), the number and type of manually generated or self - extracted feat"}
{"id": "DGM/Slides/applications of ML and DL.pdf#p8#c1", "source": "DGM/Slides/applications of ML and DL.pdf", "page": 8, "snippet": "over time ” ( gama et al. 2014 ). that is, ml models for intel - ligent systems may not produce satisfactory results, when his - torical data does not"}
{"id": "DGM/Slides/applications of ML and DL.pdf#p9#c1", "source": "DGM/Slides/applications of ML and DL.pdf", "page": 9, "snippet": "learning from already establi shed models to other applica - tions. that is, they allow customers with limited ai develop - ment resources to purchase"}
{"id": "DGM/Slides/applications of ML and DL.pdf#p10#c1", "source": "DGM/Slides/applications of ML and DL.pdf", "page": 10, "snippet": "fuchs, d. j. ( 2018 ). the dangers of human - like bias in machine - learning algorithms. missouri s & t ’ s peer to peer, 2 ( 1 ), 15. gama, j., zlio"}
{"id": "DGM/Slides/applications of ML and DL.pdf#p11#c1", "source": "DGM/Slides/applications of ML and DL.pdf", "page": 11, "snippet": "play. science, 362 ( 6419 ), 1140 – 1144. https : / / doi. org / 10. 1126 / science. aar6404. spooner, t., fearnley, j., savani, r., & koukorinis, a. "}
{"id": "DGM/Slides/Autoencoders_types.pdf#p1#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 1, "snippet": "deep learning srihari autoencoderssargur sriharisrihari @ buffalo. edu 1"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p2#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 2, "snippet": "deep learning srihari topics in autoencoders • what is an autoencoder? 1. undercompleteautoencoders2. regularized autoencoders3. representational powe"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p3#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 3, "snippet": "deep learning srihariwhat is an autoencoder? • a neural network trained using unsupervised learning • trained to copy its input to its output • learns"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p4#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 4, "snippet": "deep learning srihariembedding is a point on a manifold • an embedding is a low - dimensional vector • with fewer dimensions than than the ambient spa"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p5#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 5, "snippet": "deep learning srihari a manifold in ambient space 5 age progression / regression by conditional adversarial autoencoder ( caagithub : https : / / gith"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p6#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 6, "snippet": "deep learning sriharigeneral structure of an autoencoder • maps an input xto an output r ( called reconstruction ) through an internal representation "}
{"id": "DGM/Slides/Autoencoders_types.pdf#p7#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 7, "snippet": "deep learning srihari autoencoders differ from general data compression • autoencoders are data - specific • i. e., only able to compress data similar"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p8#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 8, "snippet": "deep learning srihari what does an autoencoder learn? • learning g ( f ( x ) ) = xeverywhere is not useful • autoencoders are designed to be unable to"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p9#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 9, "snippet": "deep learning srihari autoencoder history • part of neural network landscape for decades • used for dimensionality reduction and feature learning • th"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p10#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 10, "snippet": "deep learning srihari an autoencoder architecture 10 weights ware learnt using : 1. training samples, and 2. a loss functionas discussed nextencoderf "}
{"id": "DGM/Slides/Autoencoders_types.pdf#p11#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 11, "snippet": "deep learning srihari two autoencoder training methods1. autoencoder is a feed - forward non - recurrent neural net • with an input layer, an output l"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p12#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 12, "snippet": "deep learning srihari 1. undercomplete autoencoder • copying input to output sounds useless • but we have no interest in decoder output • we hope htak"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p13#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 13, "snippet": "deep learning srihari autoencoder with linear decoder + mse is pca • learning process is that of minimizing a loss functionl ( x, g ( f ( x ) ) ) • wh"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p14#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 14, "snippet": "deep learning srihari autoencoder training using a loss function • encoderfand decoder g • one hidden layer • non - linear encoder • takes input xεrd "}
{"id": "DGM/Slides/Autoencoders_types.pdf#p15#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 15, "snippet": "deep learning srihari encoder / decoder capacity • if encoder fand decoder gare allowed too much capacity • autoencoder can learn to perform the copyi"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p16#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 16, "snippet": "deep learning srihari cases when autoencoder learning fails • where autoencoders fail to learn anything useful : 1. capacity of encoder / decoder f / "}
{"id": "DGM/Slides/Autoencoders_types.pdf#p17#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 17, "snippet": "deep learning srihari right autoencoder design : use regularization • ideally, choose code size ( dimension ofh ) small and capacity of encoder fand d"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p18#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 18, "snippet": "deep learning srihari 2. regularized autoencoder properties • regularized aes have properties beyond copying input to output : • sparsity of represent"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p19#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 19, "snippet": "deep learning srihari generative models viewed as autoencoders • beyond regularized autoencoders • generative models with latent variables and an infe"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p20#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 20, "snippet": "deep learning srihari latent variables treated as distributions 20 source : https : / / www. jeremyjordan. me / variational - autoencoders /"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p21#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 21, "snippet": "deep learning srihari • vae is a generative model • able to generate samples that look like samples from training data • with mnist, these fake sample"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p22#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 22, "snippet": "deep learning srihari sparse autoencoder 22 only a few nodes are encouraged to activate when a single sample is fed into the network fewer nodes activ"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p23#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 23, "snippet": "deep learning srihari sparse autoencoder loss function • a sparse autoencoder is an autoencoder whose • training criterion includes a sparsity penalty"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p24#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 24, "snippet": "deep learning srihari sparse encoder doesn ’ t have bayesian interpretation • penalty termω ( h ) is a regularizer term added to a feedforward network"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p25#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 25, "snippet": "deep learning srihari generative model view of sparse autoencoder • rather than thinking of sparsity penalty as a regularizer for copying task, think "}
{"id": "DGM/Slides/Autoencoders_types.pdf#p26#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 26, "snippet": "deep learning srihari sparsity - inducing priors • the log pmodel ( h ) term can be sparsity - inducing. for example the laplace prior • corresponds t"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p27#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 27, "snippet": "deep learning srihari denoising autoencoders ( dae ) • rather than adding a penalty ωto the cost function, we can obtain an autoencoder that learns so"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p28#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 28, "snippet": "deep learning srihari regularizing by penalizing derivatives • another strategy for regularizing an autoencoder • use penalty as in sparse autoencoder"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p29#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 29, "snippet": "deep learning srihari 3. representational power, layer size and depth • autoencoders are often trained with with single layer • however using deep enc"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p30#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 30, "snippet": "deep learning srihari 4. stochastic encoders and decoders • general strategy for designing the output units and loss function of a feedforward network"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p31#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 31, "snippet": "deep learning srihari loss function for stochastic decoder • given a hidden code h, we may think of the decoder as providing a conditional distributio"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p32#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 32, "snippet": "deep learning srihari stochastic encoder • we can also generalize the notion of an encoding function f ( x ) to an encoding distribution pencoder ( h "}
{"id": "DGM/Slides/Autoencoders_types.pdf#p33#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 33, "snippet": "deep learning srihari structure of stochastic autoencoder • both the encoder and decoder are not simple functions but involve a distribution • the out"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p34#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 34, "snippet": "deep learning srihari relationship to joint distribution • any latent variable model pmodel ( h | x ) defines a stochastic encoder pencoder ( h | x ) "}
{"id": "DGM/Slides/Autoencoders_types.pdf#p35#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 35, "snippet": "deep learning srihari sampling pmodel ( h | x ) 35 x pencoder ( h | x ) pdecoder ( x | h )"}
{"id": "DGM/Slides/Autoencoders_types.pdf#p36#c1", "source": "DGM/Slides/Autoencoders_types.pdf", "page": 36, "snippet": "deep learning srihari ex : sampling p ( x | h ) : deepstyle • boil down to a representation which relates to style • by iterating neural network throu"}
{"id": "DGM/Slides/DeepEnergyModels_Paper.pdf#p1#c1", "source": "DGM/Slides/DeepEnergyModels_Paper.pdf", "page": 1, "snippet": "learning deep energy models jiquan ngiam jngiam @ cs. stanford. edu zhenghao chen zhenghao @ cs. stanford. edu pang wei koh pangwei @ cs. stanford. ed"}
{"id": "DGM/Slides/DeepEnergyModels_Paper.pdf#p2#c1", "source": "DGM/Slides/DeepEnergyModels_Paper.pdf", "page": 2, "snippet": "learning deep energy models chines ( dbms ). in their seminal work, hinton et al. ( 2006a ) demonstrated how to train deep belief net - works with mul"}
{"id": "DGM/Slides/DeepEnergyModels_Paper.pdf#p3#c1", "source": "DGM/Slides/DeepEnergyModels_Paper.pdf", "page": 3, "snippet": "learning deep energy models features ( e. g. to transformations of the input ) might make the decoding ambiguous and could therefore be harder to achi"}
{"id": "DGM/Slides/DeepEnergyModels_Paper.pdf#p4#c1", "source": "DGM/Slides/DeepEnergyModels_Paper.pdf", "page": 4, "snippet": "learning deep energy models product of student - t ( pot ) distributions and covari - ance rbms ( crbms ) can be viewed as special cases of dems and b"}
{"id": "DGM/Slides/DeepEnergyModels_Paper.pdf#p5#c1", "source": "DGM/Slides/DeepEnergyModels_Paper.pdf", "page": 5, "snippet": "learning deep energy models table 1. generative performance of models on natural im - age patches. joint training of the models signiﬁcantly im - prov"}
{"id": "DGM/Slides/DeepEnergyModels_Paper.pdf#p6#c1", "source": "DGM/Slides/DeepEnergyModels_Paper.pdf", "page": 6, "snippet": "learning deep energy models ( a ) spot m1 ( b ) data samples ( c ) spot m1 - m2 ( d ) spot m1 - m2 - m12 ( e ) spot m1 - m2 - m12 - m3 ( f ) spot m1 -"}
{"id": "DGM/Slides/DeepEnergyModels_Paper.pdf#p7#c1", "source": "DGM/Slides/DeepEnergyModels_Paper.pdf", "page": 7, "snippet": "learning deep energy models figure 5. samples from spot m12 model trained on the norb dataset. beyond the dataset. we also performed a control ex - pe"}
{"id": "DGM/Slides/DeepEnergyModels_Paper.pdf#p8#c1", "source": "DGM/Slides/DeepEnergyModels_Paper.pdf", "page": 8, "snippet": "learning deep energy models figure 6. left - layer 1 ﬁlters from a single layer sigmoid - dem model ( equivalent to an rbm ). right - same ﬁlters afte"}
{"id": "DGM/Slides/AllSlides.pdf#p1#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 1, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny an introduction to neural networks neural networks and deep learning, springer, "}
{"id": "DGM/Slides/AllSlides.pdf#p2#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 2, "snippet": "neural networks • neural networks have seen an explosion in popularity in recent years. – victories in eye - catching competitions like theimagenet co"}
{"id": "DGM/Slides/AllSlides.pdf#p3#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 3, "snippet": "overview of the presented material • the videos are based on the book : c. aggarwal. neural networks and deep learning, springer, 2018. – videos not m"}
{"id": "DGM/Slides/AllSlides.pdf#p4#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 4, "snippet": "overview of book • the book covers both the old and the new in neural networks. – the core learning methods like backpropagation, tradi - tional archi"}
{"id": "DGM/Slides/AllSlides.pdf#p5#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 5, "snippet": "neural networks : two views • a way to simulate biological learning by simulating the ner - vous system. • a way to increase the power of known models"}
{"id": "DGM/Slides/AllSlides.pdf#p6#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 6, "snippet": "historical origins • the ﬁrst model of a computational unit was theperceptron ( 1958 ). – was roughly inspired by the biological model of a neuron. – "}
{"id": "DGM/Slides/AllSlides.pdf#p7#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 7, "snippet": "the perceptron [ image courtesy : smithsonian institute ]"}
{"id": "DGM/Slides/AllSlides.pdf#p8#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 8, "snippet": "the first neural winter : minsky and papert ’ s book • minsky and papert ’ s book ” perceptrons ” ( 1969 ) showed that the perceptron only had limited"}
{"id": "DGM/Slides/AllSlides.pdf#p9#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 9, "snippet": "did we really not know how to train multiple units? • it depends on who you ask. – ai researchers didn ’ t know ( and didn ’ t believe it possible ). "}
{"id": "DGM/Slides/AllSlides.pdf#p10#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 10, "snippet": "general view of artiﬁcial intelligence ( seventies / eighties ) • it was the era or work on logic and reasoning ( discrete math - ematics ). – viewed "}
{"id": "DGM/Slides/AllSlides.pdf#p11#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 11, "snippet": "backpropagation : the second coming • rumelhart, hinton, and williams wrote two papers on back - propagation in 1986 ( independent from prior work ). "}
{"id": "DGM/Slides/AllSlides.pdf#p12#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 12, "snippet": "the nineties • acceptance of backpropagation encouraged more research in multilayer networks. • by the year 2000, most of the modern architectures had"}
{"id": "DGM/Slides/AllSlides.pdf#p13#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 13, "snippet": "what changed? • modern neural architectures are similar to those available in the year 2000 ( with some optimization tweaks ). • main : lots of data a"}
{"id": "DGM/Slides/AllSlides.pdf#p14#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 14, "snippet": "a cautionary note • deep learning has now assumed the mantle of the ai panacea. – we have heard this story before. – why should it be this time? – exc"}
{"id": "DGM/Slides/AllSlides.pdf#p15#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 15, "snippet": "how it all started : the biological inspiration • neural networks were originally designed to simulate the learning process in biological organisms. •"}
{"id": "DGM/Slides/AllSlides.pdf#p16#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 16, "snippet": "neural networks : the biological inspiration neuron w1 w2 w3 w4 axon dendrites with synaptic weights w5 ( a ) biological neural network ( b ) artiﬁcia"}
{"id": "DGM/Slides/AllSlides.pdf#p17#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 17, "snippet": "learning in biological vs artiﬁcial networks • in living organisms, synaptic weights change in response to external stimuli. – an unpleasant experienc"}
{"id": "DGM/Slides/AllSlides.pdf#p18#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 18, "snippet": "comments on the biological paradigm • the biological paradigm is often criticized as a very inexact caricature. – the functions computed in a neural n"}
{"id": "DGM/Slides/AllSlides.pdf#p19#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 19, "snippet": "an alternative view : the computational graph extension of traditional machine learning • the elementary units compute similar functions to traditiona"}
{"id": "DGM/Slides/AllSlides.pdf#p20#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 20, "snippet": "machine learning versus deep learning accuracy amount of data deep learning conventional machine learning • for smaller data sets, traditional machine"}
{"id": "DGM/Slides/AllSlides.pdf#p21#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 21, "snippet": "reasons for recent popularity • the recent success of neural networks has been caused by an increase in data and computational power. – increased comp"}
{"id": "DGM/Slides/AllSlides.pdf#p22#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 22, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny single layer networks : the perceptron neural networks and deep learning, spring"}
{"id": "DGM/Slides/AllSlides.pdf#p23#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 23, "snippet": "binary classiﬁcation and linear regression problems • in the binary classiﬁcation problem, each training pair ( x, y ) contains feature variables x = "}
{"id": "DGM/Slides/AllSlides.pdf#p24#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 24, "snippet": "the perceptron : earliest historical architecture input nodes output node y w1 w2 w3 w4 x4 x3 x2 x1 x5 w5 • thednodes in the input layer only transmit"}
{"id": "DGM/Slides/AllSlides.pdf#p25#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 25, "snippet": "what is the perceptron doing? • tries to ﬁnd alinear separator w · x = 0 between the two classes. • ideally, all positive instances ( y = 1 ) should b"}
{"id": "DGM/Slides/AllSlides.pdf#p26#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 26, "snippet": "bias neurons input nodes output node w1 w2 w3 w4 w5 b + 1 bias neuron y x4 x3 x2 x1 x5 • in many settings ( e. g., skewed class distribution ) we need"}
{"id": "DGM/Slides/AllSlides.pdf#p27#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 27, "snippet": "training a perceptron • go through the input - output pairs ( x, y ) one by one and make updates, if predicted value from observed valuey⇒biological r"}
{"id": "DGM/Slides/AllSlides.pdf#p28#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 28, "snippet": "what objective function is the perceptron optimizing? • at the time, the perceptron was proposed, the notion of loss function was not popular⇒updates "}
{"id": "DGM/Slides/AllSlides.pdf#p29#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 29, "snippet": "perceptron vs linear svms • perceptron criterion is a shifted version of hinge - loss in svm : lsvm i = m a x { 1−yi ( w · xi ), 0 } – the pre - condi"}
{"id": "DGM/Slides/AllSlides.pdf#p30#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 30, "snippet": "perceptron vs linear svms optimal solution found by perceptron optimal solution found by svm marginally correct prediction loss function discourages m"}
{"id": "DGM/Slides/AllSlides.pdf#p31#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 31, "snippet": "where does the perceptron fail? linearly separablenot linearly separable w x = 0 • the perceptron fails at similar problems as a linear svm – classica"}
{"id": "DGM/Slides/AllSlides.pdf#p32#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 32, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny activation and loss functions neural networks and deep learning, springer, 2018 "}
{"id": "DGM/Slides/AllSlides.pdf#p33#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 33, "snippet": "why do we need activation functions? • an activation function φ ( v ) in the output layer can control the nature of the output ( e. g., probability va"}
{"id": "DGM/Slides/AllSlides.pdf#p34#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 34, "snippet": "perceptron : activation functions continuous score output y loss = max ( 0, - y [ w x ] ) linear activation perceptron criterion input nodes w x discr"}
{"id": "DGM/Slides/AllSlides.pdf#p35#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 35, "snippet": "why do we need loss functions? • the loss function is typically paired with the activation func - tion to quantify how far we are from a desired resul"}
{"id": "DGM/Slides/AllSlides.pdf#p36#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 36, "snippet": "identity activation • identity activation φ ( v ) = vis often used in the output layer, when the outputs are real values. • for a single - layer netwo"}
{"id": "DGM/Slides/AllSlides.pdf#p37#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 37, "snippet": "sigmoid activation • sigmoid activation is deﬁned as φ ( v ) = 1 / ( 1 + exp ( −v ) ). • for a training pair ( x, y ), one obtains the following predi"}
{"id": "DGM/Slides/AllSlides.pdf#p38#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 38, "snippet": "tanh activation −10−50510 −1 −0. 8 −0. 6 −0. 4 −0. 2 0 0. 2 0. 4 0. 6 0. 8 1 −6−4−20246 −1 −0. 8 −0. 6 −0. 4 −0. 2 0 0. 2 0. 4 0. 6 0. 8 1 ( a ) sigmo"}
{"id": "DGM/Slides/AllSlides.pdf#p39#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 39, "snippet": "piecewise linear activation functions −2−1. 5−1−0. 500. 511. 52 −1 −0. 8 −0. 6 −0. 4 −0. 2 0 0. 2 0. 4 0. 6 0. 8 1 −2−1. 5−1−0. 500. 511. 52 −1 −0. 8 "}
{"id": "DGM/Slides/AllSlides.pdf#p40#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 40, "snippet": "softmax activation function • all activation functions discussed so far map scalars to scalars. • the softmax activation function maps vectors to vect"}
{"id": "DGM/Slides/AllSlides.pdf#p41#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 41, "snippet": "derivatives of activation functions • neural network learning requires gradient descent of the loss. • loss is often a function of the outputo, which "}
{"id": "DGM/Slides/AllSlides.pdf#p42#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 42, "snippet": "useful derivatives • sigmoid : ∂o ∂v = o ( 1−o ) • tanh : ∂o ∂v = 1−o2 • relu : derivative is 1 for positive values ofvand 0 otherwise. • hard tanh : "}
{"id": "DGM/Slides/AllSlides.pdf#p43#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 43, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny multilayer neural networks neural networks and deep learning, springer, 2018 cha"}
{"id": "DGM/Slides/AllSlides.pdf#p44#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 44, "snippet": "multilayer neural networks input layer hidden layer output layer y x4 x3 x2 x1 x5 • in multilayer networks, the output of a node can feed into otherhi"}
{"id": "DGM/Slides/AllSlides.pdf#p45#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 45, "snippet": "multilayer neural networks input layer hidden layer output layer y x4 x3 x2 x1 x5 • the layers between the input and output are referred to as hiddenb"}
{"id": "DGM/Slides/AllSlides.pdf#p46#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 46, "snippet": "scalar versus vector diagrams y x4 x3 x2 x1 x5 h11 h12 h13h23 h22 h21 h1h2 xscalar weights on connections weight matrices on connections y xh1h2 x 5 x"}
{"id": "DGM/Slides/AllSlides.pdf#p47#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 47, "snippet": "using activation functions • the nature of the activation in output layers is often con - trolled by the nature of output – identity activation for re"}
{"id": "DGM/Slides/AllSlides.pdf#p48#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 48, "snippet": "why are hidden layers nonlinear? • a multi - layer network that uses only the identity activation function in all its layers reduces to a single - lay"}
{"id": "DGM/Slides/AllSlides.pdf#p49#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 49, "snippet": "role of hidden layers • nonlinear hidden layers perform the role of hierarchical fea - ture engineering. – early layers learn primitive features and l"}
{"id": "DGM/Slides/AllSlides.pdf#p50#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 50, "snippet": "example of classifying inseparable data not linearly separable a b c x2 x1 ( - 1, 1 ) ( 0, 1 ) ( 1, 1 ) first layer transform h2 h1 a b c ( 0, 1 ) ( 0"}
{"id": "DGM/Slides/AllSlides.pdf#p51#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 51, "snippet": "the feature engineering view of hidden layers input layer w x = 0 hidden layers learn features that are friendly to machine learning algorithms like c"}
{"id": "DGM/Slides/AllSlides.pdf#p52#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 52, "snippet": "multilayer networks as computational graphs • consider the case in which each layer computes the vector - to - vector functionfi. • the overallcomposi"}
{"id": "DGM/Slides/AllSlides.pdf#p53#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 53, "snippet": "how do we train? • we want to compute the derivatives with respect to the parameters in all layers to perform gradient descent. • the complex nature o"}
{"id": "DGM/Slides/AllSlides.pdf#p54#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 54, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny connecting machine learning with shallow neural networks neural networks and dee"}
{"id": "DGM/Slides/AllSlides.pdf#p55#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 55, "snippet": "neural networks and machine learning • neural networks are optimization - based learning models. • many classical machine learning models use continuo"}
{"id": "DGM/Slides/AllSlides.pdf#p56#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 56, "snippet": "the continuum between machine learning and deep learning accuracy amount of data deep learning conventional machine learning • classical machine learn"}
{"id": "DGM/Slides/AllSlides.pdf#p57#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 57, "snippet": "the deep learning advantage • exploring the neural models for traditional machine learning is useful because it exposes the cases in which deep learni"}
{"id": "DGM/Slides/AllSlides.pdf#p58#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 58, "snippet": "recap : perceptron versus linear support vector machine output node y loss = max ( 0, - y [ w x ] ) linear activation perceptron criterion ( smooth su"}
{"id": "DGM/Slides/AllSlides.pdf#p59#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 59, "snippet": "perceptron criterion versus hinge loss loss perceptron criterionhinge loss 1 0 value of w x for positive class instance • loss for positive class trai"}
{"id": "DGM/Slides/AllSlides.pdf#p60#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 60, "snippet": "what about the kernel svm? input layer hidden layer ( rbf activation ) output layer y x3 x2 x1 + 1 bias neuron ( hidden layer ) • rbf network for unsu"}
{"id": "DGM/Slides/AllSlides.pdf#p61#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 61, "snippet": "much of machine learning is a shallow neural model • by minor changes to the architecture of perceptron we can get : – linear regression, fisher discr"}
{"id": "DGM/Slides/AllSlides.pdf#p62#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 62, "snippet": "why do we care about connections? • connections tell us about the cases that it makes sense to use conventional machine learning : – if you have less "}
{"id": "DGM/Slides/AllSlides.pdf#p63#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 63, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny neural models for linear regression, classiﬁcation, and the fisher discriminant "}
{"id": "DGM/Slides/AllSlides.pdf#p64#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 64, "snippet": "widrow - rule : the neural avatar of linear regression • the perceptron ( 1958 ) was historically followed by widrow - learning ( 1960 ). • identical "}
{"id": "DGM/Slides/AllSlides.pdf#p65#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 65, "snippet": "linear regression : an introduction • in linear regression, we have training pairs ( xi, yi ) f o ri∈ { 1... n }, s ot h a t xicontainsd - dimensional"}
{"id": "DGM/Slides/AllSlides.pdf#p66#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 66, "snippet": "linear regression with numerical targets : neural model output node y linear activation squared loss loss = ( y - [ w x ] ) 2 x input nodes w • predic"}
{"id": "DGM/Slides/AllSlides.pdf#p67#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 67, "snippet": "widrow - : linear regression with binary targets • foryi∈ { −1, + 1 }, we use same loss of ( ) 2, and update of w + α ( ) delta xi. – when applied to "}
{"id": "DGM/Slides/AllSlides.pdf#p68#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 68, "snippet": "comparison of widrow - with perceptron and svm • convert the binary loss functions and updates to a form more easily comparable to perceptron usingy2 "}
{"id": "DGM/Slides/AllSlides.pdf#p69#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 69, "snippet": "some interesting historical facts • hinton proposed the svml2 - loss three years before cortes and vapnik ’ s paper on svms. – g. hinton. connectionis"}
{"id": "DGM/Slides/AllSlides.pdf#p70#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 70, "snippet": "connections with fisher discriminant • consider a binary classiﬁcation problem with training in - stances ( xi, yi ) a n dyi∈ { −1, + 1 }. – mean - ce"}
{"id": "DGM/Slides/AllSlides.pdf#p71#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 71, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny neural models for logistic regression neural networks and deep learning, springe"}
{"id": "DGM/Slides/AllSlides.pdf#p72#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 72, "snippet": "logistic regression : a probabilistic model • consider the training pair ( xi, yi ) w i t hd - dimensional feature variables in xiand class variableyi"}
{"id": "DGM/Slides/AllSlides.pdf#p73#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 73, "snippet": "maximum - likelihood objective functions • why did we use the negative logarithms? • logistic regression is an example of a maximum - likelihood objec"}
{"id": "DGM/Slides/AllSlides.pdf#p74#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 74, "snippet": "logistic regression : neural model loss = - log ( | y / 2 - 0. 5 + y | ) sigmoid activation log likelihood y = probability of + 1 y = observed value ("}
{"id": "DGM/Slides/AllSlides.pdf#p75#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 75, "snippet": "interpreting the logistic update • an important multiplicative factor in the update increment is 1 / ( 1 + exp [ yi ( w · xi ) ] ). • this factor is p"}
{"id": "DGM/Slides/AllSlides.pdf#p76#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 76, "snippet": "comparing updates of models • the unregularized updates of the perceptron, svm, widrow -, and logistic regression can all be written in the following "}
{"id": "DGM/Slides/AllSlides.pdf#p77#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 77, "snippet": "comparing loss functions of models −3−2−10123 −1 −0. 5 0 0. 5 1 1. 5 2 2. 5 3 3. 5 4 prediction = w. x for x in positive class penalty perceptron ( su"}
{"id": "DGM/Slides/AllSlides.pdf#p78#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 78, "snippet": "other comments on logistic regression • many classical neural models use repeated computational units with logistic and tanh activation functions in h"}
{"id": "DGM/Slides/AllSlides.pdf#p79#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 79, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny the softmax activation function and multinomial logistic regression neural netwo"}
{"id": "DGM/Slides/AllSlides.pdf#p80#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 80, "snippet": "binary classes versus multiple classes • all the models discussed so far discuss only the binary class setting in which the class label is drawn from "}
{"id": "DGM/Slides/AllSlides.pdf#p81#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 81, "snippet": "generalizing logistic regression • logistic regression produces probabilities of the two out - comes of a binary class. • multinomiallogistic regressi"}
{"id": "DGM/Slides/AllSlides.pdf#p82#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 82, "snippet": "the softmax activation function • the softmax activation function is a natural vector - centric generalization of the scalar - to - scalar sigmoid act"}
{"id": "DGM/Slides/AllSlides.pdf#p83#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 83, "snippet": "loss functions for softmax • recall that we use the negative logarithm of the probability of observed class in binary logistic regression. – natural g"}
{"id": "DGM/Slides/AllSlides.pdf#p84#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 84, "snippet": "cross - entropy loss of softmax • like the binary logistic case, the losslis a negative log probability. softmax probability vector⇒ [,,... ] [... ] ="}
{"id": "DGM/Slides/AllSlides.pdf#p85#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 85, "snippet": "loss derivative of softmax • since softmax is almost always paired with cross - entropy loss l, we can directly estimate∂l ∂vrfor each pre - activatio"}
{"id": "DGM/Slides/AllSlides.pdf#p86#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 86, "snippet": "multinomial logistic regression loss = - log ( - y2 ) x vi = v1 v2 v3 w3 w2 w1 wi x true class y2 = exp ( v2 ) / [ ( vi ) ] y1 = exp ( v1 ) / [ ( vi )"}
{"id": "DGM/Slides/AllSlides.pdf#p87#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 87, "snippet": "computing the derivative of the loss • the cross - entropy loss for theith training instance isli = −log ( ( i ) ). • for gradient - descent, we need "}
{"id": "DGM/Slides/AllSlides.pdf#p88#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 88, "snippet": "gradient descent update • each separator wris updated using the gradient : wr−α∂li ∂ wr • substituting the gradient from the previous slide, we obtain"}
{"id": "DGM/Slides/AllSlides.pdf#p89#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 89, "snippet": "summary • the book also contains details of the multiclass perceptron and weston - watkins svm. • multinomial logistic regression is a direct generali"}
{"id": "DGM/Slides/AllSlides.pdf#p90#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 90, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny the autoencoder for unsupervised representation learning neural networks and dee"}
{"id": "DGM/Slides/AllSlides.pdf#p91#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 91, "snippet": "unsupervised learning • the models we have discussed so far use training pairs of the form ( x, y ) in which the feature variables xand target yare cl"}
{"id": "DGM/Slides/AllSlides.pdf#p92#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 92, "snippet": "example • consider a 2 - dimensional data set in which all points are distributed on the circumference of an origin - centered circle. • all points in"}
{"id": "DGM/Slides/AllSlides.pdf#p93#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 93, "snippet": "unsupervised models and compression • unsupervised models are closely related to compression be - cause compression captures a model of regularities i"}
{"id": "DGM/Slides/AllSlides.pdf#p94#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 94, "snippet": "deﬁning the input and output of an autoencoder input layer hidden layer output layer xi 4 xi 3 xi 2 xi 1 xi 5 output of this layer provides reduced re"}
{"id": "DGM/Slides/AllSlides.pdf#p95#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 95, "snippet": "encoder and decoder originaldata reconstructeddata code encoder ( multilayerneural network ) functionf (. ) decoder ( multilayerneural network ) funct"}
{"id": "DGM/Slides/AllSlides.pdf#p96#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 96, "snippet": "basic structure of autoencoder • it is common ( but not necessary ) for anm - layer autoen - coder to have a symmetric architecture between the input "}
{"id": "DGM/Slides/AllSlides.pdf#p97#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 97, "snippet": "undercomplete autoencoders and dimensionality reduction • the number of units in each middle layer is typically fewer than that in the input ( or outp"}
{"id": "DGM/Slides/AllSlides.pdf#p98#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 98, "snippet": "overcomplete autoencoders and representation learning • what happens if the number of units in hidden layer is equal to or larger than input / output "}
{"id": "DGM/Slides/AllSlides.pdf#p99#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 99, "snippet": "applications • dimensionality reduction⇒use activations of constricted hidden layer • sparse feature learning⇒use activations of con - strained / regu"}
{"id": "DGM/Slides/AllSlides.pdf#p100#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 100, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny singular value decomposition with autoencoders neural networks and deep learning"}
{"id": "DGM/Slides/AllSlides.pdf#p101#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 101, "snippet": "singular value decomposition • truncated svd is theapproximatedecomposition of ann×d matrixdintod≈qσpt, w h e r eq, σ, a n dparen×k, k×k, andd×kmatric"}
{"id": "DGM/Slides/AllSlides.pdf#p102#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 102, "snippet": "relaxed and unnormalized deﬁnition of svd • two - way decomposition : find andn×kmatrixu, a n d d×kmatrixvso that | | d−uvt | | 2is minimized. – prope"}
{"id": "DGM/Slides/AllSlides.pdf#p103#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 103, "snippet": "dimensionality reduction and matrix factorization • singular value decomposition is a dimensionality reduction method ( like any matrix factorization "}
{"id": "DGM/Slides/AllSlides.pdf#p104#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 104, "snippet": "the autoencoder architecture for svd input layer output of this layer provides reduced representation x4 x3 x2 x1 x5 wt output layer xi 4 xi 3 xi 2 xi"}
{"id": "DGM/Slides/AllSlides.pdf#p105#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 105, "snippet": "why is this svd? • if we use the mean - squared error as the loss function, we are optimizing | | d−uvt | | 2over the entire training data. – this is "}
{"id": "DGM/Slides/AllSlides.pdf#p106#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 106, "snippet": "some interesting facts • the optimal encoder weight matrixwwill be the pseudo - inverse of the decoder weight matrixvif the training data spans the fu"}
{"id": "DGM/Slides/AllSlides.pdf#p107#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 107, "snippet": "deep autoencoders −1 −0. 5 0 0. 5 1 −1 −0. 5 0 0. 5 1 1. 5 −0. 2 0 0. 2 0. 4 0. 6 0. 8 1 1. 2 point a point c point b −5 0 5 −0. 6 −0. 4 −0. 2 0 0. 2 "}
{"id": "DGM/Slides/AllSlides.pdf#p108#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 108, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny row - index to row - value autoencoders : incomplete matrix factorization for re"}
{"id": "DGM/Slides/AllSlides.pdf#p109#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 109, "snippet": "recommender systems • recap of svd : factorizesd≈uvtso that the sum - of - squares of residuals | | d−uvt | | 2is minimized. – helpful to watch previo"}
{"id": "DGM/Slides/AllSlides.pdf#p110#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 110, "snippet": "with autoencoder • if some of the inputs are missing, then using an autoencoder architecture will implicitly assume default values for some inputs ( l"}
{"id": "DGM/Slides/AllSlides.pdf#p111#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 111, "snippet": "row - index - to - row - value autoencoder • autoencoders map row values to row values. – discuss an autoencoder architecture to map the one - hot enc"}
{"id": "DGM/Slides/AllSlides.pdf#p112#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 112, "snippet": "row - index - to - row - value autoencoder for rs 0 1 0 0 5 missing 4 alice bob sayani john one - hot encoded input shrek e. t. nixon gandhi nero miss"}
{"id": "DGM/Slides/AllSlides.pdf#p113#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 113, "snippet": "how to handle incompletely speciﬁed entries? 0 1 0 0 5 4 alice bob sayani john shrek e. t. observed ratings ( sayani ) : e. t., shrek 0 0 1 0 5 alice "}
{"id": "DGM/Slides/AllSlides.pdf#p114#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 114, "snippet": "equivalence to classical matrix factorization for rs • since the two weight matrices areuandvt, the one - hot input encoding will pull out the relevan"}
{"id": "DGM/Slides/AllSlides.pdf#p115#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 115, "snippet": "training equivalence • forkhidden nodes, there arekpaths between each user and each item identiﬁer. • backpropagation updates weights along allkpaths "}
{"id": "DGM/Slides/AllSlides.pdf#p116#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 116, "snippet": "advantage of neural view over classical mf view • the neural view provides natural ways to add power to the architecture with nonlinearity and depth. "}
{"id": "DGM/Slides/AllSlides.pdf#p117#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 117, "snippet": "importance of row - index - to - row - value autoencoders • several mf methods in machine learning can be expressed as row - index - to - row - value "}
{"id": "DGM/Slides/AllSlides.pdf#p118#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 118, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny word2vec : the skipgram model neural networks and deep learning, springer, 2018 "}
{"id": "DGM/Slides/AllSlides.pdf#p119#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 119, "snippet": "word2vec : an overview • word2veccomputes embeddings of words using sequential proximity in sentences. – ifparisis closely related tofrance, t h e npa"}
{"id": "DGM/Slides/AllSlides.pdf#p120#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 120, "snippet": "words and context • a window of sizeton either side is predicted using a word. • this model tries to predict the contextwi−twi−t + 1... wi−1 wi + 1..."}
{"id": "DGM/Slides/AllSlides.pdf#p121#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 121, "snippet": "where have we seen this setup before? • similar to recommender systems withimplicit feedback. • instead of user - item matrices, we have square word -"}
{"id": "DGM/Slides/AllSlides.pdf#p122#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 122, "snippet": "word2vec : skipgram model x1 x2 x3 xd h1 h2 hp y11 y12 y13 y1d yj1 yj2 yj3 yjd ym1 ym2 ym3 ymd u = [ ujq ] v = [ vqj ] v = [ vqj ] v = [ vqj ] d x p m"}
{"id": "DGM/Slides/AllSlides.pdf#p123#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 123, "snippet": "word2vec : skipgram model x1 x2 x3 xd h1 h2 hp yj1 yj2 yj3 yjd u = [ ujq ] v = [ vqj ] d x p matrixp x d matrix minibatch the m d - dimensional output"}
{"id": "DGM/Slides/AllSlides.pdf#p124#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 124, "snippet": "word2vec : skipgram model with negative sampling x1 x2 x3 xd h1 h2 hp yj1 yj2 yj3 yjd u = [ ujq ] v = [ vqj ] d x p matrixp x d matrix minibatch the m"}
{"id": "DGM/Slides/AllSlides.pdf#p125#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 125, "snippet": "can you see the similarity? x1 x2 x3 xd h1 h2 hp yj1 yj2 yj3 yjd u = [ ujq ] v = [ vqj ] d x p matrixp x d matrixthe vast majority of zero outputs are"}
{"id": "DGM/Slides/AllSlides.pdf#p126#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 126, "snippet": "word2vec is nonlinear matrix factorization • levy and goldberg showed anindirectrelationship between word2vecsgns and ppmi matrix factorization. • we "}
{"id": "DGM/Slides/AllSlides.pdf#p127#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 127, "snippet": "other extensions • we can apply a row - index - to - value autoencoder to any type of matrix to learn embeddings of either rows or columns. • applying"}
{"id": "DGM/Slides/AllSlides.pdf#p128#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 128, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny backpropagation i : computing derivatives in computational graphs [ without back"}
{"id": "DGM/Slides/AllSlides.pdf#p129#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 129, "snippet": "why do we need backpropagation? • to perform any kind of learning, we need to compute the partial derivative of the loss function with respect to each"}
{"id": "DGM/Slides/AllSlides.pdf#p130#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 130, "snippet": "the complexity of computational graphs • a computational graph is a directed acyclic graph in which each node computes a function of its incoming node"}
{"id": "DGM/Slides/AllSlides.pdf#p131#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 131, "snippet": "recursive nesting is ugly! • consider a computational graph containing two nodes in a path and inputw. • the ﬁrst node computesy = g ( w ) and the sec"}
{"id": "DGM/Slides/AllSlides.pdf#p132#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 132, "snippet": "backpropagation along single path ( univariate chain rule ) w g ( w ) = w2f ( y ) = cos ( y ) o = f ( g ( w ) ) = cos ( w2 ) input weight output y = g"}
{"id": "DGM/Slides/AllSlides.pdf#p133#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 133, "snippet": "backpropagation along multiple paths ( multivariate chain rule ) • neural networks contain multiple nodes in each layer. • consider the functionf ( g1"}
{"id": "DGM/Slides/AllSlides.pdf#p134#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 134, "snippet": "example of multivariable chain rule w f ( w ) = w2 g ( y ) = cos ( y ) h ( z ) = sin ( z ) k ( p, q ) = p + q o = [ cos ( w2 ) ] + [ sin ( w2 ) ] o in"}
{"id": "DGM/Slides/AllSlides.pdf#p135#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 135, "snippet": "pathwise aggregation lemma • let a non - null setpof paths exist from a variablewin the computational graph to outputo. – local gradient of node with "}
{"id": "DGM/Slides/AllSlides.pdf#p136#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 136, "snippet": "an exponential time algorithm for computing partial derivatives • the path aggregation lemma provides a simple way to com - pute the derivative with r"}
{"id": "DGM/Slides/AllSlides.pdf#p137#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 137, "snippet": "example : deep computational graph with product nodes o w input weightoutput w w2 w4 w2 w4 w w8 w8 w16 w16 o = w32 each node computes the product of i"}
{"id": "DGM/Slides/AllSlides.pdf#p138#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 138, "snippet": "example of increasing complexity with depth o w input weightoutput w w2 w4 w2 w4 w w8 w8 w16 w16 o = w32 each node computes the product of its inputs "}
{"id": "DGM/Slides/AllSlides.pdf#p139#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 139, "snippet": "observations on exponential time algorithm • not very practical approach⇒million paths for a network with 100 nodes in each layer and three layers. • "}
{"id": "DGM/Slides/AllSlides.pdf#p140#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 140, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny backpropagation ii : using dynamic programming [ backpropagation ] to compute de"}
{"id": "DGM/Slides/AllSlides.pdf#p141#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 141, "snippet": "composition functions • neural networks compute composition functions with a lot of repetitivenesscaused by a node appearing in multiple paths. • the "}
{"id": "DGM/Slides/AllSlides.pdf#p142#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 142, "snippet": "11 w input weightoutput 1 4 5 3 6 2 7 8 9 10 each node i contains y ( i ) and each edge between i and j contains z ( i, j ) example : z ( 4, 6 ) = par"}
{"id": "DGM/Slides/AllSlides.pdf#p143#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 143, "snippet": "dynamic programming and directed acyclic graphs • dynamic programming used extensively in directed acyclic graphs. – t y p i c a l : exponentially agg"}
{"id": "DGM/Slides/AllSlides.pdf#p144#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 144, "snippet": "dynamic programming update • leta ( i ) be the set of nodes at the ends of outgoing edges from nodei. • lets ( i, o ) b et h eintermediatevariable ind"}
{"id": "DGM/Slides/AllSlides.pdf#p145#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 145, "snippet": "how does it apply to neural networks? break up h = ( w x ). ah post - activation value pre - activation value = w x. { x w h = ( ah ) { x w • a neural"}
{"id": "DGM/Slides/AllSlides.pdf#p146#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 146, "snippet": "pre - activation variables to create computational graph • compute derivativeδ ( i, o ) o fl o s slatowith respect to pre - activation variable at nod"}
{"id": "DGM/Slides/AllSlides.pdf#p147#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 147, "snippet": "post - activation variables to create computation graph • the variables in the computation graph are hidden values afteractivation function applicatio"}
{"id": "DGM/Slides/AllSlides.pdf#p148#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 148, "snippet": "variables for both pre - activation and post - activation values • nice way of decoupling the linear multiplication and activa - tion operations. • si"}
{"id": "DGM/Slides/AllSlides.pdf#p149#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 149, "snippet": "losses at arbitrary nodes • we assume that the loss is incurred at a single output node. • in case of multiple output nodes, one only has to add up th"}
{"id": "DGM/Slides/AllSlides.pdf#p150#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 150, "snippet": "handling shared weights • you saw an example in autoencoders where encoder and de - coder weights are shared. • also happens in specialized architectu"}
{"id": "DGM/Slides/AllSlides.pdf#p151#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 151, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny backpropagation iii : a decoupled view of vector - centric backpropagation neura"}
{"id": "DGM/Slides/AllSlides.pdf#p152#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 152, "snippet": "multiple computational graphs from same neural network • we can create a computational graph in multiple ways from the variables in a neural network. "}
{"id": "DGM/Slides/AllSlides.pdf#p153#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 153, "snippet": "scalar versus vector computational graphs • the backpropagation discussion so far uses scalar operations. • neural networks are constructed in layer -"}
{"id": "DGM/Slides/AllSlides.pdf#p154#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 154, "snippet": "vector - centric and decoupled view of single layer multiply with wt multiply with ’ linear transformactivation ( elementwise ) ( elementwise ) apply "}
{"id": "DGM/Slides/AllSlides.pdf#p155#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 155, "snippet": "converting scalar updates to vector form • recap : when the partial derivative of nodeqwith respect to nodepisz ( p, q ), the dynamic programming upda"}
{"id": "DGM/Slides/AllSlides.pdf#p156#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 156, "snippet": "the jacobian • consider layeriand layer - ( i + 1 ) with activations ziand zi + 1. – thekth activation in layer - ( i + 1 ) is obtained by applying an"}
{"id": "DGM/Slides/AllSlides.pdf#p157#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 157, "snippet": "on linear layer and activation functions multiply with wt multiply with ’ linear transformactivation ( elementwise ) ( elementwise ) apply multiply de"}
{"id": "DGM/Slides/AllSlides.pdf#p158#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 158, "snippet": "table of forward propagation and backward propagation function forward backward linear zi + 1 = wt zi gi = w gi + 1 sigmoid zi + 1 = sigmoid ( zi ) gi"}
{"id": "DGM/Slides/AllSlides.pdf#p159#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 159, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny neural network training [ initialization, preprocessing, mini - batching, tuning"}
{"id": "DGM/Slides/AllSlides.pdf#p160#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 160, "snippet": "how to check correctness of backpropagation • consider a particular weightwof a randomly selected edge in the network. • letl ( w ) be the current val"}
{"id": "DGM/Slides/AllSlides.pdf#p161#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 161, "snippet": "what does “ closely enough ” mean? • algorithm - determined derivative isgeand the approximate derivative isga. ρ = | ge−ga | | ge + ga | ( 13 ) • the"}
{"id": "DGM/Slides/AllSlides.pdf#p162#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 162, "snippet": "stochastic gradient descent • we have always worked withpoint - wiseloss functions so far. – corresponds to stochastic gradient descent. – in practice"}
{"id": "DGM/Slides/AllSlides.pdf#p163#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 163, "snippet": "mini - batch stochastic gradient descent • one can improve accuracy of gradient computation by using a batch of instances. – instead of holding a vect"}
{"id": "DGM/Slides/AllSlides.pdf#p164#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 164, "snippet": "why does mini - batching work? • at early learning stages, the weight vectors are very poor. – training data is highly redundant in terms of important"}
{"id": "DGM/Slides/AllSlides.pdf#p165#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 165, "snippet": "feature normalization • standardization : normalize to zero mean and unit variance. • whitening : transform the data to a de - correlated axis system "}
{"id": "DGM/Slides/AllSlides.pdf#p166#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 166, "snippet": "weight initialization • initializations are surprisingly important. – poor initializations can lead to bad convergence behavior. – instability across "}
{"id": "DGM/Slides/AllSlides.pdf#p167#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 167, "snippet": "symmetry breaking • bad idea to initialize weights to the same value. – results in weights being updated in lockstep. – creates redundant features. • "}
{"id": "DGM/Slides/AllSlides.pdf#p168#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 168, "snippet": "sensitivity to number of inputs • more inputs increase output sensitivity to the average weight. – additive of multiple inputs : variance linearly in "}
{"id": "DGM/Slides/AllSlides.pdf#p169#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 169, "snippet": "tuning hyperparameters • hyperparameters represent the parameters like number of layers, nodes per layer, learning rate, and regularization pa - ramet"}
{"id": "DGM/Slides/AllSlides.pdf#p170#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 170, "snippet": "grid search • perform grid search over parameter space. – select set of values for each parameter in some “ reason - able ” range. – test over all com"}
{"id": "DGM/Slides/AllSlides.pdf#p171#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 171, "snippet": "how to select values for each parameter • natural approach is to select uniformly distributed values of parameters. – not the best approach in many ca"}
{"id": "DGM/Slides/AllSlides.pdf#p172#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 172, "snippet": "sampling versus grid search • with a large number of parameters, grid search is still ex - pensive. • with 10 parameters, choosing just 3 values for e"}
{"id": "DGM/Slides/AllSlides.pdf#p173#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 173, "snippet": "large - scale settings • multiple threads are often run with sampled parameter set - tings. • accuracy tracked on a separate out - of - sample validat"}
{"id": "DGM/Slides/AllSlides.pdf#p174#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 174, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny gradient ratios, vanishing and exploding gradient problems neural networks and d"}
{"id": "DGM/Slides/AllSlides.pdf#p175#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 175, "snippet": "of varying slopes in gradient descent • neural network learning is amultivariableoptimization prob - lem. • weights have magnitudes of partial deriva "}
{"id": "DGM/Slides/AllSlides.pdf#p176#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 176, "snippet": "example value of x value of y −40−30−20−10010203040 −40 −30 −20 −10 0 10 20 30 40 value of x value of y −40−30−20−10010203040 −40 −30 −20 −10 0 10 20 "}
{"id": "DGM/Slides/AllSlides.pdf#p177#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 177, "snippet": "revisiting feature normalization • in the previous lecture, we discussed feature normalization. • when features have very magnitudes, gradient ra - ti"}
{"id": "DGM/Slides/AllSlides.pdf#p178#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 178, "snippet": "the vanishing and exploding gradient problems • an extreme manifestation of varying sensitivity occurs in deep networks. • the weights / activation de"}
{"id": "DGM/Slides/AllSlides.pdf#p179#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 179, "snippet": "example xw1 w2wm - 1 h1h2hm - 1 • neural network with one node per layer. • forward propagation multiplicatively depends on each weight and activation"}
{"id": "DGM/Slides/AllSlides.pdf#p180#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 180, "snippet": "activation function propensity to vanishing gradients • partial derivative of sigmoid with outputo⇒o ( 1−o ). – maximum value ato = 0. 5o f0. 25. – fo"}
{"id": "DGM/Slides/AllSlides.pdf#p181#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 181, "snippet": "exploding gradients • initializing weights to very large values to compensate for the activation functions can cause exploding gradients. • exploding "}
{"id": "DGM/Slides/AllSlides.pdf#p182#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 182, "snippet": "0510152025300 5 10 15 20 25 30 0 0. 2 0. 4 0. 6 0. 8 1 1. 2 1. 4 y x loss gentle gradient before cliff overshoots • often occurs with the exploding gr"}
{"id": "DGM/Slides/AllSlides.pdf#p183#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 183, "snippet": "a partial fix to vanishing gradients • the relu has linear activation for nonnegative values and otherwise sets outputs to 0. • the relu has a partial"}
{"id": "DGM/Slides/AllSlides.pdf#p184#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 184, "snippet": "leaky relu • for negative inputs, the leaky relu can still propagate some gradient backwards. – at the reduced rate ofα < 1 times the learning case fo"}
{"id": "DGM/Slides/AllSlides.pdf#p185#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 185, "snippet": "maxout • the activation used is max { w1 · x, w2 · x } with two vectors. • one can view the maxout as a generalization of the relu. – the relu is obta"}
{"id": "DGM/Slides/AllSlides.pdf#p186#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 186, "snippet": "gradient clipping for exploding gradients • try to make the components of the partial deriva - tives more even. – value - based clipping : all partial"}
{"id": "DGM/Slides/AllSlides.pdf#p187#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 187, "snippet": "other comments on vanishing and exploding gradients • the methods discussed above are only partial ﬁxes. • other ﬁxes discussed in later lectures : – "}
{"id": "DGM/Slides/AllSlides.pdf#p188#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 188, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny first - order gradient descent methods neural networks and deep learning, spring"}
{"id": "DGM/Slides/AllSlides.pdf#p189#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 189, "snippet": "first - order descent • first - order methods work with steepest - descent directions. • modiﬁcations to basic form of steepest - descent : – need to "}
{"id": "DGM/Slides/AllSlides.pdf#p190#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 190, "snippet": "learning rate decay • initial learning rates should be high but reduce over time. • the two most common decay functions areexponential decay andinvers"}
{"id": "DGM/Slides/AllSlides.pdf#p191#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 191, "snippet": "momentum methods : marble rolling down hill loss value of neural network parameter gd slows down in flat region gd gets trapped in local optimum • use"}
{"id": "DGM/Slides/AllSlides.pdf#p192#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 192, "snippet": "avoiding zig - zagging with momentum starting point without momentum with momentum starting point optimum ( b ) without momentum starting point optimu"}
{"id": "DGM/Slides/AllSlides.pdf#p193#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 193, "snippet": "nesterov momentum • modiﬁcation of the traditional momentum method in which the gradients are computed at a point that would be reached after executin"}
{"id": "DGM/Slides/AllSlides.pdf#p194#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 194, "snippet": "adagrad • aggregatesquared magnitude ofith partial derivative inai. • the square - root ofaiis proportional to the root - mean - square slope. – the a"}
{"id": "DGM/Slides/AllSlides.pdf#p195#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 195, "snippet": "adagrad intuition • scaling the derivative inversely with√ aiencourages faster relativemovements along gently sloping directions. – absolute movements"}
{"id": "DGM/Slides/AllSlides.pdf#p196#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 196, "snippet": "rmsprop • the rmsprop algorithm usesexponential smoothingwith parameterρ∈ ( 0, 1 ) in the relative estimations of the gradi - ents. – absolute magnitu"}
{"id": "DGM/Slides/AllSlides.pdf#p197#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 197, "snippet": "rmsprop with nesterov momentum • possible to combine rmsprop with nesterov momentum √ ai ( ∂l ( w + β v ) ∂wi ) ; + • maintenance ofaiis done with shi"}
{"id": "DGM/Slides/AllSlides.pdf#p198#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 198, "snippet": "adadelta and adam • both methods derive intuition from rmsprop – adadelta track of an exponentially smoothed value of the incremental changesof weight"}
{"id": "DGM/Slides/AllSlides.pdf#p199#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 199, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny second - order gradient descent methods neural networks and deep learning, sprin"}
{"id": "DGM/Slides/AllSlides.pdf#p200#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 200, "snippet": "why second - order methods? 0510152025300 5 10 15 20 25 30 0 0. 2 0. 4 0. 6 0. 8 1 1. 2 1. 4 y x loss gentle gradient before cliff overshoots • first "}
{"id": "DGM/Slides/AllSlides.pdf#p201#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 201, "snippet": "revisiting the bowl value of x value of y −40−30−20−10010203040 −40 −30 −20 −10 0 10 20 30 40 value of x value of y −40−30−20−10010203040 −40 −30 −20 "}
{"id": "DGM/Slides/AllSlides.pdf#p202#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 202, "snippet": "a valley −2 −1 0 1 2 −1 −0. 5 0 0. 5 1 −1 0 1 2 3 4 5 x y f ( x, y ) least curvature direction • gently sloping directions are better with less curvat"}
{"id": "DGM/Slides/AllSlides.pdf#p203#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 203, "snippet": "the hessian • the second - order derivatives of the loss functionl ( w ) a r e of the following form : hij = ∂2l ( w ) ∂wi∂wj • the partial derivative"}
{"id": "DGM/Slides/AllSlides.pdf#p204#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 204, "snippet": "quadratic approximation of loss function • one can write a quadratic approximation of the loss function with taylor expansion about w0 : l ( w ) ≈l ( "}
{"id": "DGM/Slides/AllSlides.pdf#p205#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 205, "snippet": "newton ’ s update • can solve quadratic approximation in one step from initial point w0. ∇l ( w ) = 0 [ gradient of loss function ] ∇l ( w0 ) + h ( w−"}
{"id": "DGM/Slides/AllSlides.pdf#p206#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 206, "snippet": "why second - order methods? • pre - multiplying with the inverse hessian ﬁnds a trade - be - tween speed of descent and curvature."}
{"id": "DGM/Slides/AllSlides.pdf#p207#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 207, "snippet": "basic second - order algorithm and approximations • keep making newton ’ s updates to convergence ( single step needed for quadratic function ) – even"}
{"id": "DGM/Slides/AllSlides.pdf#p208#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 208, "snippet": "conjugate gradient method • get to optimal indsteps ( instead of single newton step ) wheredis number of parameters. • use optimal step - sizes to get"}
{"id": "DGM/Slides/AllSlides.pdf#p209#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 209, "snippet": "conjugate gradients on 2 - dimensional quadratic • two conjugate directions are required to reach optimality"}
{"id": "DGM/Slides/AllSlides.pdf#p210#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 210, "snippet": "conjugate gradient algorithm • for quadratic functions only. – update wt + wt + αt qt. here, the step sizeαtis com - puted using line search. – set qt"}
{"id": "DGM/Slides/AllSlides.pdf#p211#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 211, "snippet": "computing projection of hessian • the update requires computation of theprojectionof the hessian rather than inversion of hessian. qt + 1 = −∇l ( wt +"}
{"id": "DGM/Slides/AllSlides.pdf#p212#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 212, "snippet": "other second - order methods • quasi - newton method : a sequence of increasingly accurate approximations of the inverse hessian matrix are used in va"}
{"id": "DGM/Slides/AllSlides.pdf#p213#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 213, "snippet": "problems with second - order methods −1−0. 8−0. 6−0. 4−0. 200. 20. 40. 60. 81 −1 −0. 8 −0. 6 −0. 4 −0. 2 0 0. 2 0. 4 0. 6 0. 8 1 x f ( x ) −1 −0. 5 0 "}
{"id": "DGM/Slides/AllSlides.pdf#p214#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 214, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny batch normalization neural networks and deep learning, springer, 2018 chapter 3,"}
{"id": "DGM/Slides/AllSlides.pdf#p215#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 215, "snippet": "revisiting the vanishing and exploding gradient problems xw1 w2wm - 1 h1h2hm - 1 • neural network with one node per layer. • forward propagation multi"}
{"id": "DGM/Slides/AllSlides.pdf#p216#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 216, "snippet": "revisiting the bowl value of x value of y −40−30−20−10010203040 −40 −30 −20 −10 0 10 20 30 40 value of x value of y −40−30−20−10010203040 −40 −30 −20 "}
{"id": "DGM/Slides/AllSlides.pdf#p217#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 217, "snippet": "input shift • one can view the input to each layer as a shifting data set of hidden activations during training. • a shifting input causes problems du"}
{"id": "DGM/Slides/AllSlides.pdf#p218#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 218, "snippet": "solution : batch normalization add batch normalization bn break up ai bn vi ( a ) post - activation normalization ( b ) pre - activation normalization"}
{"id": "DGM/Slides/AllSlides.pdf#p219#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 219, "snippet": "batch normalization node • theith unit contains two parametersβiandγithat need to be learned. • normalize overbatchofminstances forith unit. μi = r = "}
{"id": "DGM/Slides/AllSlides.pdf#p220#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 220, "snippet": "changes to backpropagation • we need to backpropagate through the newly added layer of normalization nodes. – the bn node can be treated like any othe"}
{"id": "DGM/Slides/AllSlides.pdf#p221#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 221, "snippet": "issues in inference • the transformation parametersμiandσidepend on the batch. • how should one compute them during testing when asingle test instance"}
{"id": "DGM/Slides/AllSlides.pdf#p222#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 222, "snippet": "batch normalization as regularizer • batch normalization also acts as a regularizer. • same data point can cause somewhat updates de - pending on whic"}
{"id": "DGM/Slides/AllSlides.pdf#p223#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 223, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny model generalization and the bias - variance trade - neural networks and deep le"}
{"id": "DGM/Slides/AllSlides.pdf#p224#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 224, "snippet": "what is model generalization? • in a machine learning problem, we try to generalize the known dependent variable on seen instances to unseen instances"}
{"id": "DGM/Slides/AllSlides.pdf#p225#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 225, "snippet": "memorization vs generalization • why is the accuracy on seen data higher? – trained model remembers some of the irrelevant nuances. • when is the gap "}
{"id": "DGM/Slides/AllSlides.pdf#p226#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 226, "snippet": "example : predictyfromx linear simplification true model x = 2 y x • first impression : polynomial model such asy = w0 + w1x + w2x2 + w3x3 + w4x4is “ "}
{"id": "DGM/Slides/AllSlides.pdf#p227#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 227, "snippet": "training data sets with five points linear simplification true model x = 2x = 2 x = 2polynomialprediction at x = 2 linear prediction at x = 2 x = 2 • "}
{"id": "DGM/Slides/AllSlides.pdf#p228#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 228, "snippet": "observations • the higher - order model is more complex than the linear model and has lessbias. – but it has more parameters. – for a small training d"}
{"id": "DGM/Slides/AllSlides.pdf#p229#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 229, "snippet": "noise component • unlike bias and variance, noise is a property of thedatarather than the model. • noise refers to unexplained data from true modelyi "}
{"id": "DGM/Slides/AllSlides.pdf#p230#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 230, "snippet": "bias - variance trade - : setup • imagine you are given the true distributionbof training data ( including labels ). • you have a principled way of sa"}
{"id": "DGM/Slides/AllSlides.pdf#p231#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 231, "snippet": "informal deﬁnition of bias • compute averaged prediction of each test instancexover training modelsg ( x, d ). • averaged prediction of test instance "}
{"id": "DGM/Slides/AllSlides.pdf#p232#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 232, "snippet": "informal deﬁnition of variance • the valueg ( x, d ) will vary withdfor ﬁxedx. – the prediction of the same test instance will be over trained models."}
{"id": "DGM/Slides/AllSlides.pdf#p233#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 233, "snippet": "bias - variance equation • lete [ mse ] be the expected mean - squared error of the ﬁxed set of test instances over samples of training data sets. e ["}
{"id": "DGM/Slides/AllSlides.pdf#p234#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 234, "snippet": "the bias - variance trade - squared error model complexity overall error optimal complexity • optimal point of model complexity is somewhere in middle"}
{"id": "DGM/Slides/AllSlides.pdf#p235#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 235, "snippet": "key takeaway of bias - variance trade - • a model with greater complexity might betheoreticallymore accurate ( i. e., low bias ). – but you have less "}
{"id": "DGM/Slides/AllSlides.pdf#p236#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 236, "snippet": "model generalization in neural networks • the recent success of neural networks is made possible by increased data. – large data sets help in generali"}
{"id": "DGM/Slides/AllSlides.pdf#p237#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 237, "snippet": "how to detect overﬁtting • the error on test data might be caused by several reasons. – other reasons might be bias ( underﬁtting ), noise, and poor c"}
{"id": "DGM/Slides/AllSlides.pdf#p238#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 238, "snippet": "improving generalization in neural networks • key techniques to improve generalization : – penalty - based regularization. – constraints like shared p"}
{"id": "DGM/Slides/AllSlides.pdf#p239#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 239, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny penalty - based regularization neural networks and deep learning, springer, 2018"}
{"id": "DGM/Slides/AllSlides.pdf#p240#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 240, "snippet": "revisiting example : predictyfromx linear simplification true model x = 2 y x • first impression : polynomial model such asy = w0 + w1x + w2x2 + w3x3 "}
{"id": "DGM/Slides/AllSlides.pdf#p241#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 241, "snippet": "economy in parameters • a lower - order model has economy in parameters. – a linear model uses two parameters, whereas an order - 4 model uses ﬁve par"}
{"id": "DGM/Slides/AllSlides.pdf#p242#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 242, "snippet": "soft economy vs hard economy • fixing the architecture up front is an inﬂexible solution. • a softer solution uses a larger model but imposes a ( tuna"}
{"id": "DGM/Slides/AllSlides.pdf#p243#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 243, "snippet": "on updates • for learning rateα, on update is to multiply parameter with ( 1−αλ ) ∈ ( 0, 1 ). ( 1−αλ ) −α∂l ∂wi – interpretation : decay - based forge"}
{"id": "DGM/Slides/AllSlides.pdf#p244#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 244, "snippet": "l1 - regularization • inl1 - regularization, anl1 - penalty is imposed on the loss function. l = ( x, y ) ∈d ( ) 2 + λ · i = 0 | wi | 1 • update has s"}
{"id": "DGM/Slides/AllSlides.pdf#p245#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 245, "snippet": "l1 - o rl2 - regularization? • l1 - regularization leads to sparse parameter learning. – zero values ofwican be dropped. – equivalent to dropping edge"}
{"id": "DGM/Slides/AllSlides.pdf#p246#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 246, "snippet": "connections with noise injection • l2 - regularization with parameterλis equivalent to adding gaussian noise with varianceλto input. – intuition : bad"}
{"id": "DGM/Slides/AllSlides.pdf#p247#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 247, "snippet": "penalizing hidden units • one can also penalize hidden units. • applyingl1 - penalty leads to sparse activations. • more common in unsupervised applic"}
{"id": "DGM/Slides/AllSlides.pdf#p248#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 248, "snippet": "hard and soft weight sharing • fix particular weights to be the same based on domain - speciﬁc insights. – discussed in lecture on backpropagation. • "}
{"id": "DGM/Slides/AllSlides.pdf#p249#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 249, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny dropout neural networks and deep learning, springer, 2018 chapter 4, section 4. "}
{"id": "DGM/Slides/AllSlides.pdf#p250#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 250, "snippet": "feature co - adaptation • the process of training a neural network often leads to a high level of dependence among features. • parts of the network tr"}
{"id": "DGM/Slides/AllSlides.pdf#p251#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 251, "snippet": "one - way adaptation • consider a single - hidden layer neural network. – all edges into and out of half the hidden nodes are ﬁxed to random values. –"}
{"id": "DGM/Slides/AllSlides.pdf#p252#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 252, "snippet": "why is feature co - adaptation bad? • we want features working together only when essential for prediction. – we do not want features adjusting to eac"}
{"id": "DGM/Slides/AllSlides.pdf#p253#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 253, "snippet": "basic dropout training procedure • for each training instance do : – sample each node in the network in each layer ( except output layer ) with probab"}
{"id": "DGM/Slides/AllSlides.pdf#p254#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 254, "snippet": "basic dropout testing procedures • first procedure : – perform repeated sampling ( like training ) and average re - sults. – geometric averaging for p"}
{"id": "DGM/Slides/AllSlides.pdf#p255#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 255, "snippet": "why does dropout help? • by dropping nodes, we are forcing the network to learn with - out the presence of some inputs ( in each layer ). • will resis"}
{"id": "DGM/Slides/AllSlides.pdf#p256#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 256, "snippet": "the regularization perspective • one can view the dropping of a node as the same process as adding masking noise. – noise is added to both input and h"}
{"id": "DGM/Slides/AllSlides.pdf#p257#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 257, "snippet": "practical aspects of dropout • typical dropout rate ( i. e., probability of exclusion ) is some - where between 20 % to 50 %. • better to use a larger"}
{"id": "DGM/Slides/AllSlides.pdf#p258#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 258, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny unsupervised pretraining neural networks and deep learning, springer, 2018 chapt"}
{"id": "DGM/Slides/AllSlides.pdf#p259#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 259, "snippet": "importance of initialization • bad initializations can lead to unstable convergence. • typical approach is to initialize to a gaussian with variance 1"}
{"id": "DGM/Slides/AllSlides.pdf#p260#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 260, "snippet": "types of pretraining • unsupervised pretraining : use training data without labels for initialization. – improves convergence behavior. – regularizati"}
{"id": "DGM/Slides/AllSlides.pdf#p261#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 261, "snippet": "types of base applications input layer hidden layer output layer xi 4 xi 3 xi 2 xi 1 xi 5 output of this layer provides reduced representation x4 x3 x"}
{"id": "DGM/Slides/AllSlides.pdf#p262#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 262, "snippet": "layer - wise pretraining a deep autoencoder input layer x4 x3 x2 x1 x5 output layer xi 4 xi 3 xi 2 xi 1 xi 5 y1 y2 y3 first - level reduction hidden l"}
{"id": "DGM/Slides/AllSlides.pdf#p263#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 263, "snippet": "pretraining a supervised learner • for a supervised learner withkhidden layers : – remove output layer and create an autoencoder with ( 2k− 1 ) hidden"}
{"id": "DGM/Slides/AllSlides.pdf#p264#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 264, "snippet": "some observations • for unsupervised pretraining, other methods may be used. • historically, restricted boltzmann machines were used before autoencode"}
{"id": "DGM/Slides/AllSlides.pdf#p265#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 265, "snippet": "why does pretraining work? • pretraining already brings the activations of the neural net - work to the manifold of the data distribution. • features "}
{"id": "DGM/Slides/AllSlides.pdf#p266#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 266, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny regularization in unsupervised applications [ denoising, contractive, variationa"}
{"id": "DGM/Slides/AllSlides.pdf#p267#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 267, "snippet": "supervised vs unsupervised applications • there is always greater tendency to overﬁt in supervised ap - plications. – in supervised applications, we a"}
{"id": "DGM/Slides/AllSlides.pdf#p268#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 268, "snippet": "sparse feature learning • use a larger number of hidden units than input units. • addl1 - penalties to the hidden layer. – backpropagation picks up th"}
{"id": "DGM/Slides/AllSlides.pdf#p269#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 269, "snippet": "denoising autoencoder • add noise to the input representation. – gaussian noise for real - valued data and masking noise for binary data. • output rem"}
{"id": "DGM/Slides/AllSlides.pdf#p270#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 270, "snippet": "illustration of denoising autoencoder true manifoldnoisy points projected on true manifold true manifoldnoisy points denoising denoising blurry images"}
{"id": "DGM/Slides/AllSlides.pdf#p271#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 271, "snippet": "gradient - based penalization : contractive autoencoders • we do not want the hidden representation to change very signiﬁcantly with smallrandomchange"}
{"id": "DGM/Slides/AllSlides.pdf#p272#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 272, "snippet": "loss function • the loss function adds up the reconstruction error and uses penalties on the gradients of the hidden layer. l = i = 1 ( ) 2 ( 3 ) • re"}
{"id": "DGM/Slides/AllSlides.pdf#p273#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 273, "snippet": "contractive autoencoder vs denoising autoencoder denoising autoencoder learns to discriminate between noise directions and manifold directions hidden "}
{"id": "DGM/Slides/AllSlides.pdf#p274#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 274, "snippet": "variational autoencoder • all the autoencoders discussed so far create a deterministic hidden representation. • the variational autoencoder creates as"}
{"id": "DGM/Slides/AllSlides.pdf#p275#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 275, "snippet": "regularization of hidden distribution • the hidden distribution is pushed towards gaussian with zero mean and unit variance inkdimensionsover the full"}
{"id": "DGM/Slides/AllSlides.pdf#p276#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 276, "snippet": "stochastic architecture with deterministic inputs encoder network decoder network stddev vector hidden vector mean vector sampled inputreconstruction "}
{"id": "DGM/Slides/AllSlides.pdf#p277#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 277, "snippet": "conversion to deterministic architecture with stochastic inputs encoder network stddev vectormean vector decoder network hidden vector gaussian sample"}
{"id": "DGM/Slides/AllSlides.pdf#p278#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 278, "snippet": "objective function • reconstruction loss same as other models : l = i = 1 ( ) 2 ( 5 ) • regularizer is kl - divergence between unit gaussian and con -"}
{"id": "DGM/Slides/AllSlides.pdf#p279#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 279, "snippet": "connections • a variational autoencoder will regularize because stochastic ( noisy ) hidden representation needs to reconstruct. – one can interpret t"}
{"id": "DGM/Slides/AllSlides.pdf#p280#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 280, "snippet": "comparisons • in denoising autoencoder, noise resistance is shared by en - coder and decoder. – often use both in denoising applications. • in contrac"}
{"id": "DGM/Slides/AllSlides.pdf#p281#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 281, "snippet": "variational autoencoder is useful as generative model decoder network gaussian samples n ( 0, i ) generated image • throw away encoder and feed sample"}
{"id": "DGM/Slides/AllSlides.pdf#p282#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 282, "snippet": "of the variational regularization * * * * * * * * * * * oo o o o o o oo o o o o o o * * * * + + + + + + + + + + + + +............ + + + + + + +. 2 - d"}
{"id": "DGM/Slides/AllSlides.pdf#p283#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 283, "snippet": "applications of variational autoencoder • variational autoencoders have similar applications as gener - ative adversarial networks ( gans ). – can als"}
{"id": "DGM/Slides/AllSlides.pdf#p284#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 284, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny radial basis function networks neural networks and deep learning, springer, 2018"}
{"id": "DGM/Slides/AllSlides.pdf#p285#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 285, "snippet": "radial basis function networks • radial basis function ( rbf ) networks represent a fundamen - tally paradigm in neural networks. – not deep learners⇒"}
{"id": "DGM/Slides/AllSlides.pdf#p286#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 286, "snippet": "when to use rbf networks • deep networks work best when the data has rich structure ( e. g., images ). – property of hierarchical and supervised featu"}
{"id": "DGM/Slides/AllSlides.pdf#p287#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 287, "snippet": "rbf network input layer hidden layer ( rbf activation ) output layer y x3 x2 x1 + 1 bias neuron ( hidden layer ) • single ( unsupervised ) hidden laye"}
{"id": "DGM/Slides/AllSlides.pdf#p288#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 288, "snippet": "workings of the rbf network • each ofmhidden units has its own prototype vector μiand bandwidthσi. – common to set eachσi = σ. • for input vector x, a"}
{"id": "DGM/Slides/AllSlides.pdf#p289#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 289, "snippet": "how do rbf networks classify nonlinearly separable classes? • work on cover ’ s principle of separability of patterns. • transforming low - dimensiona"}
{"id": "DGM/Slides/AllSlides.pdf#p290#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 290, "snippet": "illustration of separation process linearly separable in input space not linearly separable in input space but separable in 4 - dimensional hidden spa"}
{"id": "DGM/Slides/AllSlides.pdf#p291#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 291, "snippet": "training an rbf network • training works in two phases : – learn the prototype vectors μiand bandwidthσin an unsupervised manner. – learn the weights "}
{"id": "DGM/Slides/AllSlides.pdf#p292#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 292, "snippet": "training the hidden layer • only need to ﬁnd the prototype vectors μiand bandwidthσ. – the prototypes can be sampled from data or can be cen - troids "}
{"id": "DGM/Slides/AllSlides.pdf#p293#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 293, "snippet": "kernel methods are special cases of rbf networks • set the prototypes to all data points and : – linear output layer ( squared loss ) for kernel regre"}
{"id": "DGM/Slides/AllSlides.pdf#p294#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 294, "snippet": "are supervised methods any good? • supervised training methods for hidden layer discussed in book. • generally, supervision of hidden layer leads to o"}
{"id": "DGM/Slides/AllSlides.pdf#p295#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 295, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny restricted boltzmann machines neural networks and deep learning, springer, 2018 "}
{"id": "DGM/Slides/AllSlides.pdf#p296#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 296, "snippet": "restricted boltzmann machines • most of the neural architectures map inputs to outputs. – ideal for supervised models. – autoencoders can be used for "}
{"id": "DGM/Slides/AllSlides.pdf#p297#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 297, "snippet": "key from conventional neural networks • no input to output mapping • states arediscrete samplesof probability distributions with interdependencies amo"}
{"id": "DGM/Slides/AllSlides.pdf#p298#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 298, "snippet": "historical signiﬁcance • most of the practical applications in neural networks use su - pervised learning. • rbms can still be used for unsupervised p"}
{"id": "DGM/Slides/AllSlides.pdf#p299#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 299, "snippet": "deﬁning a restricted boltzmann machine hidden states v1v2v3v4 h1h2 h3 visible states • bipartitegraph ofbinaryhidden states and visible states con - n"}
{"id": "DGM/Slides/AllSlides.pdf#p300#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 300, "snippet": "an interpretable boltzmann machine parents see hidden states [ trucks ] conessundaepopsiclecup ben ’ s truck jerry ’ s truck tom ’ s truck child only "}
{"id": "DGM/Slides/AllSlides.pdf#p301#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 301, "snippet": "what kind of model does a restricted boltzmann machine build? • probability distributions of thebinaryhidden and visible states depend on one another."}
{"id": "DGM/Slides/AllSlides.pdf#p302#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 302, "snippet": "notations • we assume that thebinaryhidden units areh1... hmand the visible units arev1... vd. • the bias associated with the visible nodevibe denoted"}
{"id": "DGM/Slides/AllSlides.pdf#p303#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 303, "snippet": "probabilistic relationships • want to learn weightswijso that samples of the training data are most “ consistent ” with the following relationships : "}
{"id": "DGM/Slides/AllSlides.pdf#p304#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 304, "snippet": "how data is generated from a boltzmann machine • data is generated by usinggibb ’ s sampling. • randomly initialize visible states and then sample hid"}
{"id": "DGM/Slides/AllSlides.pdf#p305#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 305, "snippet": "intuition for weights • consider weights like positive values ofwij imply that states will be “ on ” together. • we already have samples showing which"}
{"id": "DGM/Slides/AllSlides.pdf#p306#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 306, "snippet": "overview of contrastive divergence • positive phase : drawbinstances of hidden states based on visible states ﬁxed to each of a mini - batch ofbtraini"}
{"id": "DGM/Slides/AllSlides.pdf#p307#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 307, "snippet": "remarks on contrastive divergence • strictly speaking, the negative phase needs a very large num - ber of iterations to reachthermal equilibriumin neg"}
{"id": "DGM/Slides/AllSlides.pdf#p308#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 308, "snippet": "utility of unsupervised learning • one can use an rbm to initialize an autoencoder for binary data ( later slides ). • treat the sigmoid - based sampl"}
{"id": "DGM/Slides/AllSlides.pdf#p309#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 309, "snippet": "equivalence of directed and undirected models hidden states visible states hidden states visible states equivalence wwwt • replace undirected edges wi"}
{"id": "DGM/Slides/AllSlides.pdf#p310#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 310, "snippet": "using a trained rbm to initialize a conventional autoencoder hidden states visible states wwthidden states ( reduced features ) visible states ( fixed"}
{"id": "DGM/Slides/AllSlides.pdf#p311#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 311, "snippet": "why use an rbm to initialize a conventional neural network? • in the early years, conventional neural networks did not train well ( especially with in"}
{"id": "DGM/Slides/AllSlides.pdf#p312#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 312, "snippet": "stacked rbm rbm 3 rbm 2 rbm 1 copy copy stacked representation w1 w2 w3 the parameter matrices w1, w2, and w3 are learned by successively training rbm"}
{"id": "DGM/Slides/AllSlides.pdf#p313#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 313, "snippet": "stacked rbm to conventional neural network w1 w2 w3 w1 t w2 t w3 t fix to input reconstruction ( target = input ) code encoder decoder fine - tune ( b"}
{"id": "DGM/Slides/AllSlides.pdf#p314#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 314, "snippet": "applications • pretraining can be used for supervised and unsupervised ap - plications – collaborative ﬁltering : was a component of netﬂix prize cont"}
{"id": "DGM/Slides/AllSlides.pdf#p315#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 315, "snippet": "collaborative filtering 010101110 e. t. ( rating = 4 ) 010101011 shrek ( rating = 5 ) hidden units h1 h2 011101010 e. t. ( rating = 2 ) 010101011 nixo"}
{"id": "DGM/Slides/AllSlides.pdf#p316#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 316, "snippet": "topic models binary hidden states binary hidden states multinomial visible states lexicon size d is typically larger than document size number of soft"}
{"id": "DGM/Slides/AllSlides.pdf#p317#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 317, "snippet": "classiﬁcation • can be used for unsupervised pretraining for classiﬁcation – goal of rbm is only to learn features in unsupervised way – class label d"}
{"id": "DGM/Slides/AllSlides.pdf#p318#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 318, "snippet": "classiﬁcation architecture multinomial visible states ( classes ) binary hidden states binary visible states ( features ) wu"}
{"id": "DGM/Slides/AllSlides.pdf#p319#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 319, "snippet": "comments • rbms represent a special case of probabilistic graphical mod - els. • provides an alternative to the autoencoder. • can be extended to non "}
{"id": "DGM/Slides/AllSlides.pdf#p320#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 320, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny recurrent neural networks neural networks and deep learning, springer, 2018 chap"}
{"id": "DGM/Slides/AllSlides.pdf#p321#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 321, "snippet": "the challenges of processing sequences • conventional neural networks have a ﬁxed number of ( pos - sibly independent ) input dimensions and outputs. "}
{"id": "DGM/Slides/AllSlides.pdf#p322#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 322, "snippet": "problems with conventionalarchitecture [ sentiment analysis ] one - hot encoded inputs hidden layer output layer y x4 x3 x2 x1 x5 analytics is hardly "}
{"id": "DGM/Slides/AllSlides.pdf#p323#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 323, "snippet": "problems with conventional architecture one - hot encoded inputs hidden layer output layer y x4 x3 x2 x1 x5 analytics must be???? fun missing • small "}
{"id": "DGM/Slides/AllSlides.pdf#p324#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 324, "snippet": "desiderata for architecture processing sequences • theith element of the sequence should be fed into the neural network after the network has had a ch"}
{"id": "DGM/Slides/AllSlides.pdf#p325#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 325, "snippet": "a time - layered recurrent network [ predicting next word ] whh x1 h1 y1 wxh why x2 h2 y2 wxh why x3 h3 y3 wxh why x4 h4 y4 wxh why whhwhh thecat chas"}
{"id": "DGM/Slides/AllSlides.pdf#p326#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 326, "snippet": "variations no missing inputs or outputs [ example : forecasting, language modeling ] missing inputs [ example : image captioning ] missing outputs ] e"}
{"id": "DGM/Slides/AllSlides.pdf#p327#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 327, "snippet": "recurrent network : basic computations whh x1 h1 y1 wxh why x2 h2 y2 wxh why x3 h3 y3 wxh why x4 h4 y4 wxh why whhwhh thecat chasedthe catchasedthemou"}
{"id": "DGM/Slides/AllSlides.pdf#p328#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 328, "snippet": "flexibility for variable - length inputs • we deﬁne the function for htin terms oftinputs. • we have h1 = f ( h0, x1 ) a n d h2 = f ( f ( h0, x1 ), x2"}
{"id": "DGM/Slides/AllSlides.pdf#p329#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 329, "snippet": "language modeling example : predicting the next word whh wxh why wxh why wxh why wxh why whhwhh thecat chasedthe catchasedthemouse 1 0 0 0 0 1 0 0 0 0"}
{"id": "DGM/Slides/AllSlides.pdf#p330#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 330, "snippet": "multilayer recurrent networks y1y2y3y4 x1x2x3x4 thecat chasedthe catchasedthemouse input words target words h ( k ) t = t a n hw ( k ) h ( k−1 ) t h ("}
{"id": "DGM/Slides/AllSlides.pdf#p331#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 331, "snippet": "training a recurrent network • main from traditional backpropagation is the issue of shared parameters : – pretend that the weights are not shared and"}
{"id": "DGM/Slides/AllSlides.pdf#p332#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 332, "snippet": "truncated backpropagation through time • the number of layers in a recurrent network depends on the the length of the sequence. • causes problems in m"}
{"id": "DGM/Slides/AllSlides.pdf#p333#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 333, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny applications of recurrent networks neural networks and deep learning, springer, "}
{"id": "DGM/Slides/AllSlides.pdf#p334#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 334, "snippet": "recurrent neural network applications are architecture sensitive! no missing inputs or outputs [ example : forecasting, language modeling ] missing in"}
{"id": "DGM/Slides/AllSlides.pdf#p335#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 335, "snippet": "missing inputs • missing inputs represent a bigger challenge than missing out - puts. – missing inputs often occur at inference when output is sequenc"}
{"id": "DGM/Slides/AllSlides.pdf#p336#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 336, "snippet": "observations • most of the applications use advanced variants like lstms and bidirectional recurrent networks. – advanced variants covered in later le"}
{"id": "DGM/Slides/AllSlides.pdf#p337#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 337, "snippet": "language modeling : predicting the next word whh wxh why wxh why wxh why wxh why whhwhh thecat chasedthe catchasedthemouse 1 0 0 0 0 1 0 0 0 0 1 0 1 0"}
{"id": "DGM/Slides/AllSlides.pdf#p338#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 338, "snippet": "generating a language sample • predicting next word is straightforward ( all inputs available ). • predicting a language sample runs into the problem "}
{"id": "DGM/Slides/AllSlides.pdf#p339#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 339, "snippet": "tiny shakespeare character - level rnn : karpathy, johnson, fei - fei • executed code fromhttps : / / github. com / karpathy / char - rnn • after 5 it"}
{"id": "DGM/Slides/AllSlides.pdf#p340#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 340, "snippet": "tiny shakespeare character - level rnn : karpathy, johnson, fei - fei • after 50 iterations : king richard ii : though they good extremit if you damed"}
{"id": "DGM/Slides/AllSlides.pdf#p341#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 341, "snippet": "what good is language modeling? • generating a language sample might seem like an exercise in futility. – samples are syntactically correct but not se"}
{"id": "DGM/Slides/AllSlides.pdf#p342#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 342, "snippet": "image captioning whh x1 h1 y1 wxh why x2 h2 y2 wxh why x3 h3 y3 wxh why x4 h4 y4 wxh why whhwhh < start > cosmicwinterwonderland cosmicwinterwonderlan"}
{"id": "DGM/Slides/AllSlides.pdf#p343#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 343, "snippet": "machine translation and sequence - to - sequence learning idon ’ tunderstandspanish y1y2y3y4 < eos > noentiendoespanol no < eos > entiendoespanol rnn1"}
{"id": "DGM/Slides/AllSlides.pdf#p344#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 344, "snippet": "sentence - level classiﬁcation whh x1 h1 wxh x2 h2 wxh x3 h3 wxh whhwhh ilovethis x4 h4 wxh ipod x5 h5 y wxh why < eos > whh class label positive sent"}
{"id": "DGM/Slides/AllSlides.pdf#p345#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 345, "snippet": "token - level classiﬁcation whh x1 h1 wxh x2 h2 wfh x3 h3 wfh x4 h4 wxh whhwhh williamjeffersonclintonlives whh x5 h5 wfh x6 h6 wfh x7 h7 wfh whh whh "}
{"id": "DGM/Slides/AllSlides.pdf#p346#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 346, "snippet": "temporal recommender systems feedforward network ( static item embedding ) feedforward network ( static user embedding ) recurrent network ( dynamic u"}
{"id": "DGM/Slides/AllSlides.pdf#p347#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 347, "snippet": "protein structure prediction • the elements of the sequence are the symbols representing one of the 20 amino acids. • the 20 possible amino acids are "}
{"id": "DGM/Slides/AllSlides.pdf#p348#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 348, "snippet": "speech and handwriting recognition • speech and handwriting recognition are sequential applica - tions. – frame representation of audios is transcribe"}
{"id": "DGM/Slides/AllSlides.pdf#p349#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 349, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny lstms and grus neural networks and deep learning, springer, 2018 chapters 7. 5 a"}
{"id": "DGM/Slides/AllSlides.pdf#p350#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 350, "snippet": "vanishing and exploding gradient problems w1 w2wm - 1 h1h2hm - 1 • neural network with one node per layer. • backpropagated partial derivative get mul"}
{"id": "DGM/Slides/AllSlides.pdf#p351#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 351, "snippet": "generalization to multi - node layers • vectorwise back - propagation is done by using the jacobian j. gt = jt gt + 1 ( 1 ) • the ( i, j ) th entry of"}
{"id": "DGM/Slides/AllSlides.pdf#p352#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 352, "snippet": "other issues with recurrent networks • hard to retain the information in a hidden state with succes - sive matrix multiplications. – hidden states of "}
{"id": "DGM/Slides/AllSlides.pdf#p353#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 353, "snippet": "recap : multilayer recurrent networks y1y2y3y4 x1x2x3x4 thecat chasedthe catchasedthemouse input words target words h ( k ) t = t a n hw ( k ) h ( k−1"}
{"id": "DGM/Slides/AllSlides.pdf#p354#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 354, "snippet": "long - term vs short term memory • a recurrent neural network only carries forward a hidden state h ( k ) tacross time layers. • an lstm carries forwa"}
{"id": "DGM/Slides/AllSlides.pdf#p355#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 355, "snippet": "setting up intermediate variables and gates • assume that hidden state and cell state arep - dimensional vectors. • the matrixw ( k ) is of size 4p×2p"}
{"id": "DGM/Slides/AllSlides.pdf#p356#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 356, "snippet": "updating cell states and hidden states • selectively forget and / or add to long - term memory c ( k ) t = c ( k ) t−1 reset? + c increment? ( 3 ) – l"}
{"id": "DGM/Slides/AllSlides.pdf#p357#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 357, "snippet": "intuition for lstm • examine the lstm with single dimension : ct = ct−1∗f + i∗c ( 5 ) • partial derivative ofctwith respect toct−1isf⇒multiply gradien"}
{"id": "DGM/Slides/AllSlides.pdf#p358#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 358, "snippet": "gated recurrent unit • simpliﬁcation of lstm but not a special case. – does not use explicit cell states. – controls updates carefully."}
{"id": "DGM/Slides/AllSlides.pdf#p359#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 359, "snippet": "gru updates • use two matricesw ( k ) andv ( k ) of sizes 2p×2pandp×2p, respectively. – in the ﬁrst layer, the matrices are of sizes 2p× ( p + d ) a n"}
{"id": "DGM/Slides/AllSlides.pdf#p360#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 360, "snippet": "explanation of updates • the reset gate rdecides how much of the hidden state to carry over from the previous time - stamp for a matrix - based transf"}
{"id": "DGM/Slides/AllSlides.pdf#p361#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 361, "snippet": "intuition for gru • consider a 1 - dimensional and single - layer gru update : ht = z · ht−1 + ( 1−z ) · tanh [ v1 · xt + v2 · r · ht−1 ] ( 8 ) • one "}
{"id": "DGM/Slides/AllSlides.pdf#p362#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 362, "snippet": "c o m p a r i s o n so fl s t ma n dg r u • k. et al. lstm : a search space odyssey. ieee tnnls, 2016. – many variants of lstm equations. • j. chung e"}
{"id": "DGM/Slides/AllSlides.pdf#p363#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 363, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny convolutional neural networks neural networks and deep learning, springer, 2018 "}
{"id": "DGM/Slides/AllSlides.pdf#p364#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 364, "snippet": "convolutional neural networks • like recurrent neural networks, convolutional neural networks aredomain - awareneural networks. – the structure of the"}
{"id": "DGM/Slides/AllSlides.pdf#p365#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 365, "snippet": "history • motivated by hubel and wiesel ’ s understanding of the cat ’ s visual cortex. – particular shapes in the visual ﬁeld excite neurons⇒ sparse "}
{"id": "DGM/Slides/AllSlides.pdf#p366#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 366, "snippet": "basic structure of a convolutional neural network • most layers have length, width, and depth. – the length and width are almost always the same. – th"}
{"id": "DGM/Slides/AllSlides.pdf#p367#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 367, "snippet": "filter for convolution operation • let the input volume of layerqhave dimensionslq×bq×dq. • the operation uses aﬁlterof sizefq×fq×dq. – the ﬁlter ’ s "}
{"id": "DGM/Slides/AllSlides.pdf#p368#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 368, "snippet": "convolution operation • spatially align the top - left corner of ﬁlter with each of ( lq− fq + 1 ) × ( bq−fq + 1 ) spatial positions. – corresponds to"}
{"id": "DGM/Slides/AllSlides.pdf#p369#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 369, "snippet": "convolution operation : pictorial illustration of dimensions depth defined by number of different filters ( 5 ) 5 3 3 32 32 28 28 5 5 input filter out"}
{"id": "DGM/Slides/AllSlides.pdf#p370#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 370, "snippet": "convolution operation : numerical example with depth 1 convolve 1 0 10 10 00 2 634 474 702 5 8 8 0 64 1 3703 5 254 1 06 4 30 0 45 0 040 34 5 5 1 0 0 2"}
{"id": "DGM/Slides/AllSlides.pdf#p371#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 371, "snippet": "understanding convolution • sparse connectivity because we are creating a feature from a region in the input volume of the size of the ﬁlter. – trying"}
{"id": "DGM/Slides/AllSlides.pdf#p372#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 372, "snippet": "of convolution • each feature in a hidden layer captures some properties of a region of input image. • a convolution in theqth layer increases therece"}
{"id": "DGM/Slides/AllSlides.pdf#p373#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 373, "snippet": "need for padding • the convolution operation reduces the size of the ( q + 1 ) th layer in comparison with the size of theqth layer. – this type of re"}
{"id": "DGM/Slides/AllSlides.pdf#p374#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 374, "snippet": "application of padding with 2 zeros 634 474 702 5 8 8 0 64 1 37 03 5 254 1 06 4 30 0 45 0 040 34 5 5 1 0 0 2 7 2 4 3634 474 702 5 8 8 0 64 1 37 03 5 2"}
{"id": "DGM/Slides/AllSlides.pdf#p375#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 375, "snippet": "types of padding • no padding : when no padding is used around the borders of the image⇒reduces the size of spatial footprint by ( fq−1 ). • half padd"}
{"id": "DGM/Slides/AllSlides.pdf#p376#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 376, "snippet": "strided convolution • when a stride ofsqis used in theqth layer, the convolution is performed at the locations 1, sq + 1, 2sq + 1, and so on along bot"}
{"id": "DGM/Slides/AllSlides.pdf#p377#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 377, "snippet": "max pooling • the pooling operation works on small grid regions of size pq×pqin each layer, and produces another layerwith the same depth. • for each "}
{"id": "DGM/Slides/AllSlides.pdf#p378#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 378, "snippet": "pooling example 634 474 702 5 8 8 0 64 1 3703 5 254 1 06 4 30 0 45 0 040 34 5 5 1 0 0 2 7 2 4 3 88 7 input 7 output 7 8 8 8 7 7 8 8 8 5 5 5 6 6 5 5 5 "}
{"id": "DGM/Slides/AllSlides.pdf#p379#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 379, "snippet": "relu • use of relu is a straightforward one - to - one operation. • the number of feature maps and spatial footprint size is retained. • often stuck a"}
{"id": "DGM/Slides/AllSlides.pdf#p380#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 380, "snippet": "fully connected layers : stuck at the end • each feature in the ﬁnal spatial layer is connected to each hidden state in the ﬁrst fully connected layer"}
{"id": "DGM/Slides/AllSlides.pdf#p381#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 381, "snippet": "interleaving between layers • the convolution, pooling, and relu layers are typically in - terleaved in order to increase expressive power. • the relu"}
{"id": "DGM/Slides/AllSlides.pdf#p382#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 382, "snippet": "example : lenet - 5 : full notation input : grayscale feature map of pixels 32 32 5 5 28 28 2 6 2 6 14 14 5 5 10 2 2 16 10 16 5 5 c1 s2c3 s4 120 8410 "}
{"id": "DGM/Slides/AllSlides.pdf#p383#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 383, "snippet": "example : lenet - 5 : shorthand notation input : grayscale feature map of pixels 32 32 5 5 28 28 5 6 5 c1 10 16 10 c3 120 8410 c5f6o ss ss subsampling"}
{"id": "DGM/Slides/AllSlides.pdf#p384#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 384, "snippet": "feature engineering image horizontal edges detected1 0 11 00 - 1 - 1 - 1 filter - 1 - 1 10 10 10 - 1 filter vertical edges detected next layer filter "}
{"id": "DGM/Slides/AllSlides.pdf#p385#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 385, "snippet": "hierarchical feature engineering • successive layers put together primitive features to create more complex features. • complex features represent reg"}
{"id": "DGM/Slides/AllSlides.pdf#p386#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 386, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny backpropagation in convolutional neural networks and its visualization applicati"}
{"id": "DGM/Slides/AllSlides.pdf#p387#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 387, "snippet": "backpropagation in convolutional neural networks • three operations of convolutions, max - pooling, and relu. • the relu backpropagation is the same a"}
{"id": "DGM/Slides/AllSlides.pdf#p388#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 388, "snippet": "backpropagating through convolutions • traditional backpropagation is transposed matrix multiplica - tion. • backpropagation through convolutions is t"}
{"id": "DGM/Slides/AllSlides.pdf#p389#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 389, "snippet": "backpropagation with an inverted filter [ single channel ] c f ab de ghi filter during convolution g d ih fe cba filter during backpropagation • multi"}
{"id": "DGM/Slides/AllSlides.pdf#p390#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 390, "snippet": "convolution as a matrix multiplication • convolution can be presented as a matrix multiplication. – useful during forward and backward propagation. – "}
{"id": "DGM/Slides/AllSlides.pdf#p391#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 391, "snippet": "convolution as a matrix multiplication flatten to 9 - dimensional vector 4 7 13 96 50 2 filter input a cd b convert to 4x9 sparse matrix c a b 0 c d 0"}
{"id": "DGM/Slides/AllSlides.pdf#p392#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 392, "snippet": "gradient - based visualization • can backpropagate all the way back to the input layer. • imagine the outputois the probability of a class label like "}
{"id": "DGM/Slides/AllSlides.pdf#p393#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 393, "snippet": "gradient - based visualization • examples of portions of speciﬁc images activated by particu - lar class labels. ( simonyan, vedaldi, and zisserman )"}
{"id": "DGM/Slides/AllSlides.pdf#p394#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 394, "snippet": "getting cleaner visualizations • the idea of “ deconvnet ” is sometimes used for cleaner visu - alizations. • main is in terms of how relu ’ s are tre"}
{"id": "DGM/Slides/AllSlides.pdf#p395#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 395, "snippet": "guided backpropagation • a variation of backpropagation, referred to asguided back - propagationis more useful for visualization. – guided backpropaga"}
{"id": "DGM/Slides/AllSlides.pdf#p396#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 396, "snippet": "illustration of “ deconvnet ” and guided backpropagation 2 4 1 - 1 - 2 3 - 213 traditional backpropagation forward pass relu 2 4 10 03 013 - 1 2 - 32 "}
{"id": "DGM/Slides/AllSlides.pdf#p397#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 397, "snippet": "derivatives of features with respect to input pixels deconv guided backpropagationcorresponding image crops deconv guided backpropagationcorresponding"}
{"id": "DGM/Slides/AllSlides.pdf#p398#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 398, "snippet": "creating a fantasy image that matches a label • the value ofomight be the unnormalized score for “ banana. ” • we would like to learn the input image "}
{"id": "DGM/Slides/AllSlides.pdf#p399#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 399, "snippet": "examples cup dalmatian goose"}
{"id": "DGM/Slides/AllSlides.pdf#p400#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 400, "snippet": "generalization to autoencoders • ideas have been generalized to convolutional autoencoders. • deconvolution operation similar to backpropagation. • on"}
{"id": "DGM/Slides/AllSlides.pdf#p401#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 401, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny case studies of convolutional neural networks neural networks and deep learning,"}
{"id": "DGM/Slides/AllSlides.pdf#p402#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 402, "snippet": "alexnet • winner of the 2012 ilsvrc contest – brought attention to the area of deep learning • first use of relu • use dropout with probability 0. 5 •"}
{"id": "DGM/Slides/AllSlides.pdf#p403#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 403, "snippet": "alexnet architecture 224 224 3 11 11 55 55 5 5 96 256 27 27 3 3 13 3 3 384 13 3 3 384 13 13 256 13 13 40964096 1000 input c1 c2c3 c4c5 fc6 fc7 fc8 mp "}
{"id": "DGM/Slides/AllSlides.pdf#p404#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 404, "snippet": "other comments on alexnet • popularized the notion of fc7 features • the features in the penultimate layer are often extracted and used for various ap"}
{"id": "DGM/Slides/AllSlides.pdf#p405#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 405, "snippet": "zfnet alexnet zfnet volume : 224×224×3 224×224×3 operations : conv 11×11 ( stride 4 ) conv 7×7 ( stride 2 ), mp volume : 55×55×96 55×55×96 operations "}
{"id": "DGM/Slides/AllSlides.pdf#p406#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 406, "snippet": "vgg • one of the top entries in 2014 ( but not winner ). • notable for its design principle of reduced ﬁlter size and in - creased depth • all ﬁlters "}
{"id": "DGM/Slides/AllSlides.pdf#p407#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 407, "snippet": "principle of reduced filter size • a single 7×7 ﬁlter will have 49 parameters over one channel. • three 3×3 ﬁlters will have a receptive ﬁeld of size "}
{"id": "DGM/Slides/AllSlides.pdf#p408#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 408, "snippet": "vgg conﬁgurations name : a a - lrn b c d e # l a y e r s 11 11 13 16 16 19 c3d64 c3d64 c3d64 c3d64 c3d64 c3d64 lrn c3d64 c3d64 c3d64 c3d64 m m m m m m"}
{"id": "DGM/Slides/AllSlides.pdf#p409#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 409, "snippet": "vgg design choices and performance • max - pooling had the responsibility of reducing spatial foot - print. • number of ﬁlters often increased by 2 af"}
{"id": "DGM/Slides/AllSlides.pdf#p410#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 410, "snippet": "googlenet • introduced the principle ofinception architecture. • the initial part of the architecture is much like a traditional convolutional network"}
{"id": "DGM/Slides/AllSlides.pdf#p411#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 411, "snippet": "inception module : motivation • key information in the images is available at levels of detail. – large ﬁlter can capture can information in a bigger "}
{"id": "DGM/Slides/AllSlides.pdf#p412#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 412, "snippet": "basic inception module filter concatenation 1 x 1 convolutions previous layer 3 x 3 convolutions 5x 5 convolutions 3x 3 max - pooling • main problem i"}
{"id": "DGM/Slides/AllSlides.pdf#p413#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 413, "snippet": "computationally inception module 3 x 3 convolutions filter concatenation 5x 5 convolutions 1 x 1 convolutions 3 x 3 max - pooling 1 x 1 convolutions 1"}
{"id": "DGM/Slides/AllSlides.pdf#p414#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 414, "snippet": "design principles of output layer • it is common to use fully connected layers near the output. • googlenetuses average pooling across the whole spati"}
{"id": "DGM/Slides/AllSlides.pdf#p415#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 415, "snippet": "googlenet architecture"}
{"id": "DGM/Slides/AllSlides.pdf#p416#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 416, "snippet": "other details on googlenet • winner of ilsvrc 2014 • reached top - 5 error rate of 6. 7 % • contained 22 layers • several advanced variants with impro"}
{"id": "DGM/Slides/AllSlides.pdf#p417#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 417, "snippet": "resnet motivation • increasing depth has advantages but also makes the network harder to train. • even the error on the training data is high! – poor "}
{"id": "DGM/Slides/AllSlides.pdf#p418#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 418, "snippet": "resnet • resnet brought the depth of neural networks into the hun - dreds. • based on the principle of iterative feature engineering rather than hiera"}
{"id": "DGM/Slides/AllSlides.pdf#p419#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 419, "snippet": "skip connections in resnet weight layer weight layer relu + f ( x ) x relu f ( x ) + x identity x 7x7 conv, 64, / 2 3x3 conv, 64 pool, / 2 3x3 conv, 6"}
{"id": "DGM/Slides/AllSlides.pdf#p420#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 420, "snippet": "details of resnet • a3×3 ﬁlter is used at a stride / padding of 1⇒adopted fromvggand maintains dimensionality. • some layers use strided convolutions "}
{"id": "DGM/Slides/AllSlides.pdf#p421#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 421, "snippet": "importance of depth name year number of layers top - 5 error - before 2012 ≤5 > 25 % alexnet 2012 8 15. 4 % zfnet / clarifai 2013 8 / > 8 14. 8 % / 11"}
{"id": "DGM/Slides/AllSlides.pdf#p422#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 422, "snippet": "pretrained models • many of the models discussed in previous slides are available as pre - trained models withimagenetdata. • one can use the features"}
{"id": "DGM/Slides/AllSlides.pdf#p423#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 423, "snippet": "object localization • need to classify image together with bounding box ( four numbers )"}
{"id": "DGM/Slides/AllSlides.pdf#p424#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 424, "snippet": "object localization convolution layers ( weights fixed for both classification and regression ) softmax fully connected classification head class prob"}
{"id": "DGM/Slides/AllSlides.pdf#p425#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 425, "snippet": "other applications • object detection : multiple objects • text and sequence processing applications – 1 - dimensional convolutions • video and spatio"}
{"id": "DGM/Slides/AllSlides.pdf#p426#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 426, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny basic principles of reinforcement learning [ motivating deep learning ] neural n"}
{"id": "DGM/Slides/AllSlides.pdf#p427#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 427, "snippet": "the complexity of human intelligence is quite simple! • herbert simon ’ s ant hypothesis : “ h u m a nb e i n g s, v i e w e da sb e h a v i n gs y s "}
{"id": "DGM/Slides/AllSlides.pdf#p428#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 428, "snippet": "when to use reinforcement learning? • systems that are simple to judge but hard to specify. • easy to use trial - and - error to generate data. – vide"}
{"id": "DGM/Slides/AllSlides.pdf#p429#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 429, "snippet": "why don ’ t we have general forms of artiﬁcial intelligence yet? • reinforcement learning requires large amounts of data ( gen - erated by trial and e"}
{"id": "DGM/Slides/AllSlides.pdf#p430#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 430, "snippet": "simplest reinforcement learning setting : multi - armed bandits • imagine a gambler in a casino faced with 2 slot machines. • each trial costs the gam"}
{"id": "DGM/Slides/AllSlides.pdf#p431#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 431, "snippet": "observations • playing both slot machines alternately helps the gambler learn about their ( over time ). – however, it is wastefulexploration! – gambl"}
{"id": "DGM/Slides/AllSlides.pdf#p432#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 432, "snippet": "na¨ıve algorithm • exploration : play each slot machine for a ﬁxed number of trials. • exploitation : play the winner forever. – might require a large"}
{"id": "DGM/Slides/AllSlides.pdf#p433#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 433, "snippet": "- greedy strategy • probabilistically merge exploration and exploitation. • play a random machine with, and play the ma - chine with highest current w"}
{"id": "DGM/Slides/AllSlides.pdf#p434#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 434, "snippet": "upper bounding : the optimistic gambler! • upper - bounding represents optimism towards unseen ma - chines⇒encourages exploration. • empirically estim"}
{"id": "DGM/Slides/AllSlides.pdf#p435#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 435, "snippet": "multi - armed bandits versus classical reinforcement learning • multi - armed bandits is the simplest form of reinforcement learning. • the model is s"}
{"id": "DGM/Slides/AllSlides.pdf#p436#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 436, "snippet": "markov decision process ( mdp ) : examples from four settings • agent : mouse, chess player, gambler, robot • environment : maze, chess rules, slot ma"}
{"id": "DGM/Slides/AllSlides.pdf#p437#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 437, "snippet": "the basic framework of reinforcement learning agentenvironment reward, state transition action at rtstto st + 1 1. agent ( mouse ) takes an action at "}
{"id": "DGM/Slides/AllSlides.pdf#p438#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 438, "snippet": "examples of markov decision process • game of tic - tac - toe, chess, or go : the state is the position of the board at any point, and the actions cor"}
{"id": "DGM/Slides/AllSlides.pdf#p439#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 439, "snippet": "role of traditional reinforcement learning x o o x playing x here assures victory with optimal play x o playing x here assures victory with optimal pl"}
{"id": "DGM/Slides/AllSlides.pdf#p440#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 440, "snippet": "reinforcement learning for tic - tac - toe • main from multi - armed bandits is that we need to learn the long - term rewards for each action in eachs"}
{"id": "DGM/Slides/AllSlides.pdf#p441#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 441, "snippet": "- greedy to tic - tac - toe • maintain table of values of state - action pairs ( initialize to small random values ). – in multi - armed bandits, we o"}
{"id": "DGM/Slides/AllSlides.pdf#p442#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 442, "snippet": "at the end of training x o o xplay x x oplay x x o o x play x x o play x value = + 0. 9 value = + 0. 8value = + 0. 1value = - 0. 1 • typical examples "}
{"id": "DGM/Slides/AllSlides.pdf#p443#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 443, "snippet": "where does deep learning fit in? • the tic - tac - toe approach is a gloriﬁed “ learning by rote ” algorithm. • works only for toy settings with few s"}
{"id": "DGM/Slides/AllSlides.pdf#p444#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 444, "snippet": "- greedy algorithm with deep learning for chess [ primitive : don ’ t try it! ] • convolutional neural network takes board position as input and produ"}
{"id": "DGM/Slides/AllSlides.pdf#p445#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 445, "snippet": "reinforcement learning in chess and go • the reinforcement learning systems, alphagoandalpha zero, have been designed for chess, go, and shogi. • comb"}
{"id": "DGM/Slides/AllSlides.pdf#p446#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 446, "snippet": "examples of two positions from alpha zero games vs stockﬁsh • generalize to unseen states in training. • deep learner can recognize subtle positional "}
{"id": "DGM/Slides/AllSlides.pdf#p447#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 447, "snippet": "other challenges • chess and tic - tac - toe areepisodic, with a maximum length to the game ( 9 for tic - tac - toe and≈6000 for chess ). • - greedy a"}
{"id": "DGM/Slides/AllSlides.pdf#p448#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 448, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny value function learning and q - learning neural networks and deep learning, spri"}
{"id": "DGM/Slides/AllSlides.pdf#p449#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 449, "snippet": "challenges with long and inﬁnite markov decision processes • previous lecture discusses how value functions can be learned for shorter episodes. – upd"}
{"id": "DGM/Slides/AllSlides.pdf#p450#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 450, "snippet": "an inﬁnite markov decision process • sequence below is of inﬁnite length ( continuous process ) s0a0r0s1a1r1... statrt... • the cumulative rewardrtat "}
{"id": "DGM/Slides/AllSlides.pdf#p451#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 451, "snippet": "recap of - greedy for tic - tac - toe • maintain table of average values of state - action pairs ( ini - tialize to small random values ). • - greedy "}
{"id": "DGM/Slides/AllSlides.pdf#p452#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 452, "snippet": "the bootstrapping intuition • consider a markov decision process in which we are predicting values ( e. g., long - term rewards ) at each time - stamp"}
{"id": "DGM/Slides/AllSlides.pdf#p453#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 453, "snippet": "example of chess • why is the minimax evaluation of a chess program at 10 - ply stronger than that using the 1 - ply board evaluation? – because evalu"}
{"id": "DGM/Slides/AllSlides.pdf#p454#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 454, "snippet": "q - learning • instead of minimax over a tree, we can use one - step looka - head • letq ( st, at ) be a table containing optimal values of state - ac"}
{"id": "DGM/Slides/AllSlides.pdf#p455#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 455, "snippet": "why does this work? • most of the updates we initially make are not meaningful in tic - tac - toe. – we started with random values. • however, the upd"}
{"id": "DGM/Slides/AllSlides.pdf#p456#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 456, "snippet": "sarsa : - greedy evaluation • letq ( st, at ) be the value of actionatin statestwhen fol - lowing - greedy policy. • an improved estimateofq ( st, at "}
{"id": "DGM/Slides/AllSlides.pdf#p457#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 457, "snippet": "on - policy vs - policy learning • sarsa : on - policy learning is useful when learning and infer - ence cannot be separated. – a robot who continuous"}
{"id": "DGM/Slides/AllSlides.pdf#p458#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 458, "snippet": "using deep learning convolutional neural network q ( st, a ) for a = “ up ” observed state ( previousfour screens of pixels ) q ( st, a ) for a = “ do"}
{"id": "DGM/Slides/AllSlides.pdf#p459#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 459, "snippet": "speciﬁc details of convolutional network 84 84 4 8 8 input 32 22 22 4 4 12 3 3 64 12 64 12 12 c1c2c3 512 4t o 1 8 ( game specific ) fc o • same archit"}
{"id": "DGM/Slides/AllSlides.pdf#p460#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 460, "snippet": "neural network updates for q - learning • the neural network outputsf ( xt, w, at ). • we must wait to observe state xt + 1a n dt h e ns e tu pa “ gro"}
{"id": "DGM/Slides/AllSlides.pdf#p461#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 461, "snippet": "neural network updates for sarsa • the neural network outputsf ( xt, w, at ). • we must wait to observe state xt + 1, s i m u l a t eat + 1with - gree"}
{"id": "DGM/Slides/AllSlides.pdf#p462#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 462, "snippet": "value function learning convolutional neural network observed state ( previous four screens of pixels ) v ( st ) • instead of outputting values of sta"}
{"id": "DGM/Slides/AllSlides.pdf#p463#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 463, "snippet": "temporal learningtd ( 0 ) • value network producesg ( xt, w ) and bootstrapped ground truth = rt + γg ( xt + 1, w ) • same as sarsa : observe next sta"}
{"id": "DGM/Slides/AllSlides.pdf#p464#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 464, "snippet": "bootstrapping over multiple steps • temporal bootstraps only over one time - step. – a strategically wrong move will not show up immediately. – can lo"}
{"id": "DGM/Slides/AllSlides.pdf#p465#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 465, "snippet": "fixed window vs smooth decay : temporal learningtd ( λ ) • refer to one - step temporal learning astd ( 0 ) • fixed windown : w + αδt k = t−n + 1γt−k "}
{"id": "DGM/Slides/AllSlides.pdf#p466#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 466, "snippet": "monte carlo vs temporal • not true that greater lookahead always helps! – the value ofλintd ( λ ) regulates the trade - between bias and variance. – u"}
{"id": "DGM/Slides/AllSlides.pdf#p467#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 467, "snippet": "monte carlo vs temporal : chess example • imagine a monte carlo rollout of chess game between two agents alice and bob. – alice and bob each made two "}
{"id": "DGM/Slides/AllSlides.pdf#p468#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 468, "snippet": "implications for other methods • policy gradients often use monte carlo rollouts. – deciding theadvantageof an action is often in games without contin"}
{"id": "DGM/Slides/AllSlides.pdf#p469#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 469, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny policy gradients neural networks and deep learning, springer, 2018 chapter 8. 5"}
{"id": "DGM/Slides/AllSlides.pdf#p470#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 470, "snippet": "from value - based methods • value - based methods like q - learning attempt to predict the value of an action with aparameterized value function. – o"}
{"id": "DGM/Slides/AllSlides.pdf#p471#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 471, "snippet": "policy network vs q - network for atari game convolutional neural network q ( st, a ) for a = “ up ” observed state ( previousfour screens of pixels )"}
{"id": "DGM/Slides/AllSlides.pdf#p472#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 472, "snippet": "overview of approach • we want to update network to maximizeexpectedfuture rewards⇒we need to collect samples of long - term rewards for each simulate"}
{"id": "DGM/Slides/AllSlides.pdf#p473#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 473, "snippet": "example : generating training data • training chess agent alice ( using pool of human opponents ) with reward in { + 1, 0, −1 }. • consider a monte ca"}
{"id": "DGM/Slides/AllSlides.pdf#p474#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 474, "snippet": "nature of training data • we have board positions together with output actionsamples and long - term rewards of eachsampledaction. – we do not have gr"}
{"id": "DGM/Slides/AllSlides.pdf#p475#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 475, "snippet": "log probability trick • letqp ( st, a ) be the long - term reward of actionaand policy p. • the log probability trick of reinforce relates gradient of"}
{"id": "DGM/Slides/AllSlides.pdf#p476#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 476, "snippet": "baseline adjustments • baseline adjustments change the reward into an “ advantage ” with the use of state - speciﬁc adjustments. – subtract some quant"}
{"id": "DGM/Slides/AllSlides.pdf#p477#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 477, "snippet": "why does a state - speciﬁc baseline adjustment make sense? • imagine a self - play chess game in which both sides make mistakes but one side wins. – w"}
{"id": "DGM/Slides/AllSlides.pdf#p478#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 478, "snippet": "problems with monte carlo policy gradients • full monte carlo simulation is best for episodic processes. • actor - critic methods allow online updatin"}
{"id": "DGM/Slides/AllSlides.pdf#p479#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 479, "snippet": "actor - critic method • two separate neural networks : – actor : policy network with parameters θ that decides actions. – c r i t i c : value network "}
{"id": "DGM/Slides/AllSlides.pdf#p480#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 480, "snippet": "actor and critic convolutional neural network observed state ( previousfour screens of pixels ) probability of “ up ” softmax probability of “ down ” "}
{"id": "DGM/Slides/AllSlides.pdf#p481#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 481, "snippet": "steps in actor - critic methods • sample the actionat + 1at statest + 1using the policy net - work. • use q - network to compute temporal errorδtatst "}
{"id": "DGM/Slides/AllSlides.pdf#p482#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 482, "snippet": "advantages and disadvantages of policy gradients • advantages : – work in continuous action spaces. – can be used with stochastic policies. – stable c"}
{"id": "DGM/Slides/AllSlides.pdf#p483#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 483, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny attention mechanisms neural networks and deep learning, springer, 2018 chapter 1"}
{"id": "DGM/Slides/AllSlides.pdf#p484#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 484, "snippet": "the biological motivation • human beings rarely use all the available sensory inputs in order to accomplish speciﬁc tasks. – problem of ﬁnding an addr"}
{"id": "DGM/Slides/AllSlides.pdf#p485#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 485, "snippet": "the notion of attention in the retina fovea macula retina ( not drawn to scale ) maximum density of receptors least density of receptors • only a smal"}
{"id": "DGM/Slides/AllSlides.pdf#p486#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 486, "snippet": "how does the process work? • need to systematically focus on small parts of the image to ﬁnd what one is looking for. • biological organisms draw quic"}
{"id": "DGM/Slides/AllSlides.pdf#p487#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 487, "snippet": "recurrent models of visual attention • use a simple neural network in which only the resolution of speciﬁc portions of the image centered at a particu"}
{"id": "DGM/Slides/AllSlides.pdf#p488#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 488, "snippet": "components of neural architecture • glimpse sensor : creates a retina - like representation ρ ( xt, lt−1 ) o ft h ei m a g e xtb a s e do nl o c a t i"}
{"id": "DGM/Slides/AllSlides.pdf#p489#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 489, "snippet": "neural architecture glimpse sensor glimpse network lt - 1 lt - 1 gt ρ ( xt, lt - 1 ) ρ ( xt, lt - 1 ) ht - 1 lt - 1 lt at gt ht glimpse network hidden"}
{"id": "DGM/Slides/AllSlides.pdf#p490#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 490, "snippet": "reinforcement learning • action corresponds to choosing the class label at each time - stamp. • the reward at time - stamptis 1 if the classiﬁcation i"}
{"id": "DGM/Slides/AllSlides.pdf#p491#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 491, "snippet": "image captioning ( xu et al. ) • modiﬁed version of classiﬁcation framework. • instead of using glimpse sensor outputting locationlt, w eu s e lprepro"}
{"id": "DGM/Slides/AllSlides.pdf#p492#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 492, "snippet": "image captioning architecture! \" #!! $ • k. xuet al. show, attend, and tell : neural image caption generation with visual attention. international con"}
{"id": "DGM/Slides/AllSlides.pdf#p493#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 493, "snippet": "hard attention versus soft attention • hard attention selects speciﬁc locations. – uses reinforcement learning because of hard selection of locations."}
{"id": "DGM/Slides/AllSlides.pdf#p494#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 494, "snippet": "examples of attention locations • k. xuet al. show, attend, and tell : neural image caption generation with visual attention. international conference"}
{"id": "DGM/Slides/AllSlides.pdf#p495#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 495, "snippet": "application to machine translation • a basic machine translation model hooks up two recurrent neural networks. – typically, an advanced variant like l"}
{"id": "DGM/Slides/AllSlides.pdf#p496#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 496, "snippet": "the basic machine translation model idon ’ tunderstandspanish y1y2y3y4 < eos > noentiendoespanol no < eos > entiendoespanol rnn1 rnn2 rnn1 learns repr"}
{"id": "DGM/Slides/AllSlides.pdf#p497#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 497, "snippet": "what does attention do? • the hidden statesh ( 2 ) tare transformed to enhanced states h ( 2 ) twith some additional processing from anattention layer"}
{"id": "DGM/Slides/AllSlides.pdf#p498#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 498, "snippet": "context vector and attention layer • context vector is deﬁned as follows : ct = j = 1exp ( h ( 1 ) j · h ( 2 ) t ) h ( 1 ) j j = 1exp ( h ( 1 ) j · h "}
{"id": "DGM/Slides/AllSlides.pdf#p499#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 499, "snippet": "the attention - centric machine translation model idon ’ tunderstandspanish < eos > noentiendoespanol ) ) ) ) ) ) ) ) ) ) ) ) ) y1y2y3y4 no < eos > en"}
{"id": "DGM/Slides/AllSlides.pdf#p500#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 500, "snippet": "what have we done? • the hidden states will be more weighted towards speciﬁc words in the source sentence. • the context vector helps focus the predic"}
{"id": "DGM/Slides/AllSlides.pdf#p501#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 501, "snippet": "reﬁnements • the previous model uses simple dot products for similarity. • c a na l s ou s ep a r a m e t e r i z e dv a r i a n t s : score ( t, s ) "}
{"id": "DGM/Slides/AllSlides.pdf#p502#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 502, "snippet": "observations • the reﬁned variants do not necessarily help too much for soft attention models. • more details of hard attention models are available i"}
{"id": "DGM/Slides/AllSlides.pdf#p503#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 503, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny generative adversarial networks neural networks and deep learning, springer, 201"}
{"id": "DGM/Slides/AllSlides.pdf#p504#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 504, "snippet": "generative adversarial network • generative adversarial network creates an unsupervised gen - erative model of the data. – alternative to variational "}
{"id": "DGM/Slides/AllSlides.pdf#p505#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 505, "snippet": "adversarial training • we have ageneratorand adiscriminator. • the discriminator has access to training samples and is a classiﬁer designed to disting"}
{"id": "DGM/Slides/AllSlides.pdf#p506#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 506, "snippet": "generator and discriminator • rm : s e to fmrandomly sampled examples from the real data set. • sm : s e to fmsynthetically generated samples. • synth"}
{"id": "DGM/Slides/AllSlides.pdf#p507#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 507, "snippet": "neural architecture for gan sample noise from prior distribution ( e. g., gaussian ) to create m samples synthetic samples code decoder as generator n"}
{"id": "DGM/Slides/AllSlides.pdf#p508#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 508, "snippet": "discriminator objective function • d ( x ) : discriminator output probability that sample xis real. • themaximizationobjective functionjdfor the discr"}
{"id": "DGM/Slides/AllSlides.pdf#p509#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 509, "snippet": "generator objective function • the generator createsmsynthetic samples, sm, and the goal is to fool discriminator. • the objective function is tominim"}
{"id": "DGM/Slides/AllSlides.pdf#p510#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 510, "snippet": "minimax formulation • note thatj ( d ) = j ( g ) + term indpendent of generator parameters • so minimizingjgwith respect to generator parameters is th"}
{"id": "DGM/Slides/AllSlides.pdf#p511#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 511, "snippet": "how to solve? • alternately perform updates with respect to generator and discriminator parameters. – updates for generator parameters are gradient de"}
{"id": "DGM/Slides/AllSlides.pdf#p512#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 512, "snippet": "adjustments during early optimization iterations • maximize log [ d ( x ) ] for each x∈sminstead of minimizing log [ 1−d ( x ) ]. • this alternative o"}
{"id": "DGM/Slides/AllSlides.pdf#p513#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 513, "snippet": "example for image generation [ radford, metz, chintala ]"}
{"id": "DGM/Slides/AllSlides.pdf#p514#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 514, "snippet": "generated bedrooms [ radford, metz, chintala ]"}
{"id": "DGM/Slides/AllSlides.pdf#p515#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 515, "snippet": "changing the synthetic noise sample [ radford, metz, chintala ]"}
{"id": "DGM/Slides/AllSlides.pdf#p516#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 516, "snippet": "vector arithmetic on synthetic noise samples [ radford, metz, chintala ]"}
{"id": "DGM/Slides/AllSlides.pdf#p517#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 517, "snippet": "conditional generative adversarial networks ( cgan ) synthetic samples fusion decoder as generator neural network with single probabilistic output ( e"}
{"id": "DGM/Slides/AllSlides.pdf#p518#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 518, "snippet": "image - to - image translation with cgan [ isola, zhu, zhou, efros ] labels to facadebw to color aerial to map labels to street scene edges to photo i"}
{"id": "DGM/Slides/AllSlides.pdf#p519#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 519, "snippet": "text - to - image translation with cgan : scott reed et al. 7klviorzhukdvvpdoourxqgylrohw shwdovzlwkdgdunsxusohfhqwhu 7klviorzhukdvvpdoourxqgylrohw sh"}
{"id": "DGM/Slides/AllSlides.pdf#p520#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 520, "snippet": "text - to - image translation with cgan : scott reed et al. wklvvpdooelugkdvdslqn euhdvwdqgfurzqdqgeodfn sulpdulhvdqgvhfrqgdulhv wkhiorzhukdvshwdovwkd"}
{"id": "DGM/Slides/AllSlides.pdf#p521#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 521, "snippet": "comments on cgan • capabilities are similar to conditional variational autoencoder – special case is captioning ( conditioning on image and tar - get "}
{"id": "DGM/Slides/AllSlides.pdf#p522#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 522, "snippet": "comparison with variational autoencoder • only a decoder ( i. e., generator ) is learned, and an encoder is not learned in the training process of the"}
{"id": "DGM/Slides/AllSlides.pdf#p523#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 523, "snippet": "charu c. aggarwal ibm t j watson research center yorktown heights, ny kohonen self - organizing maps neural networks and deep learning, springer, 2018"}
{"id": "DGM/Slides/AllSlides.pdf#p524#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 524, "snippet": "introduction and motivation • the kohonen self - organizing map belongs to the class of competitive learning algorithms. – competitive learning algori"}
{"id": "DGM/Slides/AllSlides.pdf#p525#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 525, "snippet": "competitive learning • the neurons compete for the right to respond to a subset of the input data. • the activation of an output neuron increases with"}
{"id": "DGM/Slides/AllSlides.pdf#p526#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 526, "snippet": "notations • let xbe an input vector inddimensions. • let wibe the weight vector associated with theith neuron in the same number of dimensions. • numb"}
{"id": "DGM/Slides/AllSlides.pdf#p527#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 527, "snippet": "iterative steps for each input point • the euclidean distance | | wi− x | | is computed for eachi ( activation value is higher for smaller distance )."}
{"id": "DGM/Slides/AllSlides.pdf#p528#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 528, "snippet": "comparison with prototype - based clustering • the basic idea in competitive learning is to view the weight vectors as prototypes ( like the centroids"}
{"id": "DGM/Slides/AllSlides.pdf#p529#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 529, "snippet": "physically arranging the clusters • pure competitive learning does not impose any relationships among clusters. – clusters often have related content."}
{"id": "DGM/Slides/AllSlides.pdf#p530#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 530, "snippet": "illustrative example of visualization music arts literature drama artsmusic drama literature ( a ) rectangular lattice ( b ) hexagonal lattice • all c"}
{"id": "DGM/Slides/AllSlides.pdf#p531#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 531, "snippet": "kohonen self - organizing map • the kohonen self - organizing map is a variation on the com - petitive learning paradigm in which a 2 - dimensional la"}
{"id": "DGM/Slides/AllSlides.pdf#p532#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 532, "snippet": "types of lattices wk wiwj k ij wk k ij wj wi ( a ) rectangular ( b ) hexagonal • rectangular lattice will lead to rectangular regions and hexagonal la"}
{"id": "DGM/Slides/AllSlides.pdf#p533#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 533, "snippet": "modiﬁcations to basic competitive learning • the weights in the winner neuron are updated in a manner similar to the vanilla competitive learning algo"}
{"id": "DGM/Slides/AllSlides.pdf#p534#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 534, "snippet": "the kohonen som algorithm • ldist ( i, j ) represents thelattice distancebetween neuronsi andj. damp ( i, j ) = exp ( −ldist ( i, j ) 2 2σ2 ) ( 6 ) • "}
{"id": "DGM/Slides/AllSlides.pdf#p535#c1", "source": "DGM/Slides/AllSlides.pdf", "page": 535, "snippet": "illustrative example of visualization music arts literature drama artsmusic drama literature ( a ) rectangular lattice ( b ) hexagonal lattice • color"}
{"id": "DGM/Slides/lec02_rnn.pdf#p1#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 1, "snippet": "introduction to rnns! arun mallya! best viewed with computer modern fonts installed!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p2#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 2, "snippet": "outline! • why recurrent neural networks ( rnns )?! • the vanilla rnn unit! • the rnn forward pass! • backpropagation refresher! • the rnn backward pa"}
{"id": "DGM/Slides/lec02_rnn.pdf#p3#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 3, "snippet": "motivation! • not all problems can be converted into one with ﬁxed - length inputs and outputs!! • problems such as speech recognition or time - serie"}
{"id": "DGM/Slides/lec02_rnn.pdf#p4#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 4, "snippet": "recurrent neural networks ( rnns )! • recurrent neural networks take the previous output or hidden states as inputs.! the composite input at time t ha"}
{"id": "DGM/Slides/lec02_rnn.pdf#p5#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 5, "snippet": "sample feed - forward network! 5 h1! y1! x1! t = 1!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p6#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 6, "snippet": "sample rnn! 6 h1! y1! x1! t = 1! h2! y2! x2! h3! y3! x3! t = 2! t = 3!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p7#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 7, "snippet": "sample rnn! 7 h1! y1! x1! t = 1! h2! y2! x2! h3! y3! x3! t = 2! t = 3! h0!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p8#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 8, "snippet": "the vanilla rnn cell! 8 ht! xt!!! ht - 1!! ht = w!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p9#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 9, "snippet": "the vanilla rnn forward! 9 h1! x1 h0!! c1! y1! h2! x2 h1!! c2! y2! h3! x3 h2!! c3! y3! ht = = f ( ht ) ct = loss ( yt, gtt )"}
{"id": "DGM/Slides/lec02_rnn.pdf#p10#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 10, "snippet": "the vanilla rnn forward! 10 h1! x1 h0!! c1! y1! h2! x2 h1!! c2! y2! h3! x3 h2!! c3! y3! ht = = f ( ht ) ct = loss ( yt, gtt ) indicates shared weights"}
{"id": "DGM/Slides/lec02_rnn.pdf#p11#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 11, "snippet": "recurrent neural networks ( rnns )! • note that the weights are shared over time! • essentially, copies of the rnn cell are made over time ( unrolling"}
{"id": "DGM/Slides/lec02_rnn.pdf#p12#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 12, "snippet": "sentiment classiﬁcation! • classify a! restaurant review from yelp! or! movie review from imdb or! …! as positive or negative!! • inputs : multiple wo"}
{"id": "DGM/Slides/lec02_rnn.pdf#p13#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 13, "snippet": "rnn! the! h1! sentiment classiﬁcation!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p14#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 14, "snippet": "rnn! the! rnn! food! h1! h2! sentiment classiﬁcation!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p15#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 15, "snippet": "rnn! the! rnn! food! h1! h2! rnn! good! hn - 1! hn! sentiment classiﬁcation!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p16#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 16, "snippet": "rnn! the! rnn! food! h1! h2! rnn! good! hn - 1! hn! linear classiﬁer! sentiment classiﬁcation!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p17#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 17, "snippet": "rnn! the! rnn! food! h1! h2! rnn! good! hn - 1! hn! linear classiﬁer! sentiment classiﬁcation! ignore! ignore! h1! h2!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p18#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 18, "snippet": "rnn! the! rnn! food! h1! h2! rnn! good! hn - 1! h = sum ( … )! h1! h2! hn! sentiment classiﬁcation! http : / / deeplearning. net / tutorial / lstm. ht"}
{"id": "DGM/Slides/lec02_rnn.pdf#p19#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 19, "snippet": "rnn! the! rnn! food! h1! h2! rnn! good! hn - 1! h = sum ( … )! h1! h2! hn! linear classiﬁer! sentiment classiﬁcation! http : / / deeplearning. net / t"}
{"id": "DGM/Slides/lec02_rnn.pdf#p20#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 20, "snippet": "image captioning! • given an image, produce a sentence describing its contents!! • inputs : image feature ( from a cnn )! • outputs : multiple words ("}
{"id": "DGM/Slides/lec02_rnn.pdf#p21#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 21, "snippet": "rnn! image captioning! cnn!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p22#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 22, "snippet": "rnn! image captioning! cnn! rnn! h2! h1! the! h2! linear classiﬁer!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p23#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 23, "snippet": "rnn! image captioning! cnn! rnn! rnn! h2! h3! h1! the! dog! h2! h3! linear classiﬁer! linear classiﬁer!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p24#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 24, "snippet": "rnn outputs : image captions! show and tell : a neural image caption generator, cvpr 15!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p25#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 25, "snippet": "rnn outputs : language modeling! http : / / karpathy. github. io / 2015 / 05 / 21 / rnn - /! viola :! why, salisbury must ﬁnd his ﬂesh and thought! th"}
{"id": "DGM/Slides/lec02_rnn.pdf#p26#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 26, "snippet": "input – output scenarios! single - single! single - multiple! multiple - single! multiple - multiple! feed - forward network! image captioning! sentim"}
{"id": "DGM/Slides/lec02_rnn.pdf#p27#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 27, "snippet": "input – output scenarios! note : we might deliberately choose to frame our problem as a! particular input - output scenario for ease of training or! b"}
{"id": "DGM/Slides/lec02_rnn.pdf#p28#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 28, "snippet": "the vanilla rnn forward! 28 h1! x1 h0!! c1! y1! h2! x2 h1!! c2! y2! h3! x3 h2!! c3! y3! ht = = f ( ht ) ct = loss ( yt, gtt ) “ unfold ” network throu"}
{"id": "DGM/Slides/lec02_rnn.pdf#p29#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 29, "snippet": "backpropagation refresher! f ( x ; w )! x! y! c! sgd updatew←w−η∂c∂w∂c∂w = = f ( x ; w ) c = loss ( y, ygt )"}
{"id": "DGM/Slides/lec02_rnn.pdf#p30#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 30, "snippet": "multiple layers! f1 ( x ; w1 )! x! y1! c! sgd updatew2←w2−η∂c∂w2w1←w1−η∂c∂w1f2 ( y1 ; w2 )! y2! y1 = f1 ( x ; w1 ) y2 = f2 ( y1 ; w2 ) c = loss ( y2, "}
{"id": "DGM/Slides/lec02_rnn.pdf#p31#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 31, "snippet": "chain rule for gradient computation! f1 ( x ; w1 )! x! y1! c! ∂c∂w1 = ( y1 ; w2 )! y2! find ∂c∂w1, ∂c∂w2∂c∂w2 = application of the chain rule! y1 = f1"}
{"id": "DGM/Slides/lec02_rnn.pdf#p32#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 32, "snippet": "chain rule for gradient computation! does output change due to −how does output change due to = = f ( x ; w )! x! y! we are interested in computing :!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p33#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 33, "snippet": "chain rule for gradient computation! does output change due to −how does output change due to = = we are interested in computing :!, to the layer are "}
{"id": "DGM/Slides/lec02_rnn.pdf#p34#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 34, "snippet": "extension to computational graphs! f ( x ; w )! f1 ( y ; w1 )! f2 ( y ; w2 )! f ( x ; w )! x! y! x! y! y! y2! y1!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p35#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 35, "snippet": "extension to computational graphs! f ( x ; w )! ( y ; w1 )! ( y ; w2 )! ( x ; w )!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p36#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 36, "snippet": "extension to computational graphs! f ( x ; w )! ( y ; w1 )! ( y ; w2 )! ( x ; w )! accumulation! σ"}
{"id": "DGM/Slides/lec02_rnn.pdf#p37#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 37, "snippet": "backpropagation through time ( bptt )! • one of the methods used to train rnns! • the unfolded network ( used during forward pass ) is treated as one "}
{"id": "DGM/Slides/lec02_rnn.pdf#p38#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 38, "snippet": "the unfolded vanilla rnn! 38 h1! x1!! c1! y1! h2! c2! y2! h3! c3! y3! h0!! h1!! h2!! x2!! x3!! • treat the unfolded network as one big feed - forward "}
{"id": "DGM/Slides/lec02_rnn.pdf#p39#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 39, "snippet": "the unfolded vanilla rnn forward! 39 h1! x1 h0!! c1! y1! h2! x2 h1!! c2! y2! h3! x3 h2!! c3! y3!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p40#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 40, "snippet": "40 h1! x1 h0!! c1! y1! h2! x2 h1!! c2! y2! h3! x3 h2!! c3! y3! the unfolded vanilla rnn backward!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p41#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 41, "snippet": "the vanilla rnn backward! 41 h1! x1 h0!! c1! y1! h2! x2 h1!! c2! y2! h3! x3 h2!! c3! y3! ht = = f ( ht ) ct = loss ( yt, gtt ) ∂ct∂h1 = =!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p42#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 42, "snippet": "issues with the vanilla rnns! • in the same way a product of k real numbers can shrink to zero or explode to inﬁnity, so can a product of matrices! • "}
{"id": "DGM/Slides/lec02_rnn.pdf#p43#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 43, "snippet": "the identity relationship! • recall! ht = ht−1 + f ( xt ) ∂ct∂h1 = =! • suppose that instead of a matrix multiplication, we had an identity relationsh"}
{"id": "DGM/Slides/lec02_rnn.pdf#p44#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 44, "snippet": "the identity relationship! • recall! ht = ht−1 + f ( xt ) ∂ct∂h1 = =! • suppose that instead of a matrix multiplication, we had an identity relationsh"}
{"id": "DGM/Slides/lec02_rnn.pdf#p45#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 45, "snippet": "disclaimer! • the explanations in the previous few slides are handwavy! • for rigorous proofs and derivations, please refer to! on the of training rec"}
{"id": "DGM/Slides/lec02_rnn.pdf#p46#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 46, "snippet": "long short - term memory ( lstm ) 1! 46 • the lstm uses this idea of “ constant error flow ” for rnns to create a “ constant error carousel ” ( cec ) "}
{"id": "DGM/Slides/lec02_rnn.pdf#p47#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 47, "snippet": "the lstm idea! cell! ht! 47 xt!!! ht - 1!! ct = ct−1 + ct! ht = tanhct w! * dashed line indicates time - lag!!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p48#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 48, "snippet": "the original lstm cell! it! ot! input gate! output gate! cell! ht! 48 xt ht - 1!! xt ht - 1!! ct = ct−1 + ct! ht = ot⊗tanhctit = + for ot! xt!!! ht - "}
{"id": "DGM/Slides/lec02_rnn.pdf#p49#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 49, "snippet": "the popular lstm cell! it! ot! ft! input gate! output gate! forget gate! ht! 49 xt ht - 1!! cell! ct! ct = ft⊗ct−1 + xt ht - 1!! xt ht - 1!! xt!!! ht "}
{"id": "DGM/Slides/lec02_rnn.pdf#p50#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 50, "snippet": "lstm – forward / backward! 50 go to : illustrated lstm forward and backward pass!"}
{"id": "DGM/Slides/lec02_rnn.pdf#p51#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 51, "snippet": "summary! 51 • rnns allow for processing of variable length inputs and outputs by maintaining state information across time steps! • various input - ou"}
{"id": "DGM/Slides/lec02_rnn.pdf#p52#c1", "source": "DGM/Slides/lec02_rnn.pdf", "page": 52, "snippet": "other useful resources / references! 52 • http : / / cs231n. stanford. edu / slides / winter1516 _ lecture10. pdf! • http : / / www. cs. toronto. edu "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p1#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 1, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 20221 lecture 10 : recurrent neural networks"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p2#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 2, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 20222 administrative - project ta matchups out, see ed for the link"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p3#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 3, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 20223 administrative - a2 is due next monday may 2nd, 11 : 59pm"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p4#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 4, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 20224 administrative - discussion section tomorrow 2 : 30 - 3 : 30pt object detection & rnn"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p5#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 5, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 20225 last time : detection and segmentation classification semantic segmentation object de"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p6#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 6, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 1. one time set up : activation functions, preprocessing, weight initialization, regul"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p7#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 7, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 20227 today : recurrent neural networks"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p8#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 8, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 20228 vanilla neural networks “ vanilla ” neural network"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p9#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 9, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 20229 recurrent neural networks : process sequences e. g. image captioning image - > sequen"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p10#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 10, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202210 recurrent neural networks : process sequences e. g. action prediction sequence of vi"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p11#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 11, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202211 recurrent neural networks : process sequences e. g. video captioning sequence of vid"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p12#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 12, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202212 recurrent neural networks : process sequences e. g. video classification on frame le"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p13#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 13, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202213 sequential processing of non - sequence data ba, mnih, and kavukcuoglu, “ multiple o"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p14#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 14, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202214 sequential processing of non - sequence data gregor et al, “ draw : a recurrent neur"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p15#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 15, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202215 recurrent neural network x rnn y"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p16#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 16, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202216 recurrent neural network x rnn y key idea : rnns have an “ internal state ” that is "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p17#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 17, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202217 unrolled rnn x1 rnn y1 x2 rnn y2 x3 rnn y3... xt rnn yt"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p18#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 18, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202218 rnn hidden state update x rnn y we can process a sequence of vectors x by applying a"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p19#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 19, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 19 rnn output generation x rnn y we can process a sequence of vectors x by applying a "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p20#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 20, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202220 recurrent neural network x1 rnn y1 x2 rnn y2 x3 rnn y3... xt rnn yt h1 h2 h3h0"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p21#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 21, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202221 recurrent neural network x rnn y we can process a sequence of vectors x by applying "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p22#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 22, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202222 ( vanilla ) recurrent neural network x rnn y the state consists of a single “ hidden"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p23#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 23, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202223 h0 fw h1 x1 rnn : computational graph"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p24#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 24, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202224 h0 fw h1 fw h2 x2x1 rnn : computational graph"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p25#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 25, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202225 h0 fw h1 fw h2 fw h3 x3 … x2x1 rnn : computational graph ht"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p26#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 26, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202226 h0 fw h1 fw h2 fw h3 x3 … x2x1w rnn : computational graph re - use the same weight m"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p27#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 27, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202227 h0 fw h1 fw h2 fw h3 x3 yt … x2x1w rnn : computational graph : many to many ht y3y2y"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p28#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 28, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202228 h0 fw h1 fw h2 fw h3 x3 yt … x2x1w rnn : computational graph : many to many ht y3y2y"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p29#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 29, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202229 h0 fw h1 fw h2 fw h3 x3 yt … x2x1w rnn : computational graph : many to many ht y3y2y"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p30#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 30, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202230 h0 fw h1 fw h2 fw h3 x3 y … x2x1w rnn : computational graph : many to one ht"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p31#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 31, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202231 h0 fw h1 fw h2 fw h3 x3 y … x2x1w rnn : computational graph : many to one ht"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p32#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 32, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202232 h0 fw h1 fw h2 fw h3 yt … xw rnn : computational graph : one to many ht y3y2y1"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p33#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 33, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202233 h0 fw h1 fw h2 fw h3 yt … xw rnn : computational graph : one to many ht y3y2y1???"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p34#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 34, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202234 h0 fw h1 fw h2 fw h3 yt … xw rnn : computational graph : one to many ht y3y2y1 0 0 0"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p35#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 35, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 yt - 1 35 h0 fw h1 fw h2 fw h3 yt … xw rnn : computational graph : one to many ht y3y2"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p36#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 36, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202236 sequence to sequence : many - to - one + one - to - many h0 fw h1 fw h2 fw h3 x3 … x"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p37#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 37, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202237 sequence to sequence : many - to - one + one - to - many y1 y2 … many to one : encod"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p38#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 38, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202238 example : character - level language model vocabulary : [ h, e, l, o ] example train"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p39#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 39, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202239 example : character - level language model vocabulary : [ h, e, l, o ] example train"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p40#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 40, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202240 example : character - level language model vocabulary : [ h, e, l, o ] example train"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p41#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 41, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202241 example : character - level language model sampling vocabulary : [ h, e, l, o ] at t"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p42#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 42, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202242. 03. 84. 00. 13. 25. 20. 05. 50. 11. 17. 68. 03. 11. 02. 08. 79 softmax “ e ” “ l ” "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p43#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 43, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202243. 03. 84. 00. 13. 25. 20. 50. 05. 11. 17. 68. 03. 11. 02. 08. 79 softmax “ e ” “ l ” "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p44#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 44, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202244. 03. 84. 00. 13. 25. 20. 50. 05. 11. 17. 68. 03. 11. 02. 08. 79 softmax “ e ” “ l ” "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p45#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 45, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 [ w11 w12 w13 w14 ] [ 1 ] [ w11 ] [ w21 w22 w23 w14 ] [ 0 ] = [ w21 ] [ w31 w32 w33 w1"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p46#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 46, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202246 backpropagation through time loss forward through entire sequence to compute loss, t"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p47#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 47, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202247 truncated backpropagation through time loss run forward and backward through chunks "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p48#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 48, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202248 truncated backpropagation through time loss carry hidden states forward in time fore"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p49#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 49, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202249 truncated backpropagation through time loss"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p50#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 50, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202250 min - char - rnn. py gist : 112 lines of python ( https : / / gist. github. com / ka"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p51#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 51, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202251 x rnn y"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p52#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 52, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202252 train more train more train more at first :"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p53#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 53, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202253"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p54#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 54, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202254 the stacks project : open source algebraic geometry textbook latex source http : / /"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p55#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 55, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202255"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p56#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 56, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202256"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p57#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 57, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202257"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p58#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 58, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202258 generated c code"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p59#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 59, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202259"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p60#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 60, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202260"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p61#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 61, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 61 https : / / openai. com / blog / openai - codex /"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p62#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 62, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 openai gpt - 2 generated text 62 input : in a shocking finding, scientist discovered a"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p63#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 63, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202263 searching for interpretable cells"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p64#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 64, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202264 searching for interpretable cells karpathy, johnson, and fei - fei : visualizing and"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p65#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 65, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202265 searching for interpretable cells karpathy, johnson, and fei - fei : visualizing and"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p66#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 66, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202266 searching for interpretable cells line length tracking cell karpathy, johnson, and f"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p67#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 67, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202267 searching for interpretable cells if statement cell karpathy, johnson, and fei - fei"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p68#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 68, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202268 searching for interpretable cells karpathy, johnson, and fei - fei : visualizing and"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p69#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 69, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202269 searching for interpretable cells code depth cell karpathy, johnson, and fei - fei :"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p70#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 70, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 rnn tradeoffs rnn advantages : - can process any length input - computation for step t"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p71#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 71, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202271 explain images with multimodal recurrent neural networks, mao et al. deep visual - s"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p72#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 72, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202272 convolutional neural network recurrent neural network"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p73#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 73, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 test image this image is cc0 public domain 73"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p74#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 74, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 test image 74"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p75#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 75, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 test image x 75"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p76#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 76, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 test image x0 < start > 76"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p77#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 77, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 h0 y0 test image before : h = tanh ( wxh * x + whh * h ) now : h = tanh ( wxh * x + wh"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p78#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 78, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 h0 y0 test image straw sample! x0 < start > 78"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p79#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 79, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 h0 y0 test image straw h1 y1 x0 < start > 79"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p80#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 80, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 h0 y0 test image straw h1 y1 hat sample! x0 < start > 80"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p81#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 81, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 h0 y0 test image straw h1 y1 hat h2 y2 x0 < start > 81"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p82#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 82, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 h0 y0 test image straw h1 y1 hat h2 y2 sample < end > token = > finish. x0 < start > 8"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p83#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 83, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202283 a cat sitting on a suitcase on the floor a cat is sitting on a tree branch a dog is "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p84#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 84, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202284 image captioning : failure cases a woman is holding a cat in her hand a woman standi"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p85#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 85, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202285 visual question answering ( vqa ) agrawal et al, “ vqa : visual question answering ”"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p86#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 86, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202286 agrawal et al, “ visual 7w : grounded question answering in images ”, cvpr 2015 figu"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p87#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 87, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202287 das et al, “ visual dialog ”, cvpr 2017 figures from das et al, copyright ieee 2017."}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p88#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 88, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 agent encodes instructions in language and uses an rnn to generate a series of movemen"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p89#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 89, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 jabri et al. “ revisiting visual question answering baselines ” eccv 2016 89 visual qu"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p90#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 90, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202290 time depth multilayer rnns"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p91#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 91, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202291 long short term memory ( lstm ) hochreiter and schmidhuber, “ long short term memory"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p92#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 92, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202292 ht - 1 xt w stack tanh ht vanilla rnn gradient flow bengio et al, “ learning long - "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p93#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 93, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202293 ht - 1 xt w stack tanh ht vanilla rnn gradient flow backpropagation from ht to ht - "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p94#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 94, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202294 ht - 1 xt w stack tanh ht vanilla rnn gradient flow backpropagation from ht to ht - "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p95#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 95, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202295 vanilla rnn gradient flow h0 h1 h2 h3 h4 x1 x2 x3 x4 bengio et al, “ learning long -"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p96#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 96, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202296 vanilla rnn gradient flow gradients over multiple time steps : h0 h1 h2 h3 h4 x1 x2 "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p97#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 97, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202297 vanilla rnn gradient flow gradients over multiple time steps : h0 h1 h2 h3 h4 x1 x2 "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p98#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 98, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202298 vanilla rnn gradient flow gradients over multiple time steps : h0 h1 h2 h3 h4 x1 x2 "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p99#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 99, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 202299 vanilla rnn gradient flow gradients over multiple time steps : h0 h1 h2 h3 h4 x1 x2 "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p100#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 100, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022100 vanilla rnn gradient flow gradients over multiple time steps : h0 h1 h2 h3 h4 x1 x2"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p101#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 101, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022101 vanilla rnn gradient flow gradients over multiple time steps : h0 h1 h2 h3 h4 x1 x2"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p102#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 102, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022102 vanilla rnn gradient flow gradients over multiple time steps : h0 h1 h2 h3 h4 x1 x2"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p103#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 103, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022103 vanilla rnn gradient flow gradients over multiple time steps : h0 h1 h2 h3 h4 x1 x2"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p104#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 104, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022104 long short term memory ( lstm ) hochreiter and schmidhuber, “ long short term memor"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p105#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 105, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022105 long short term memory ( lstm ) hochreiter and schmidhuber, “ long short term memor"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p106#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 106, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022106 long short term memory ( lstm ) [ hochreiter et al., 1997 ] x h vector from before "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p107#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 107, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022107 long short term memory ( lstm ) [ hochreiter et al., 1997 ] x h vector from before "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p108#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 108, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 i : input gate, whether to write to cell f : forget gate, whether to erase cell o : ou"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p109#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 109, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022109 long short term memory ( lstm ) [ hochreiter et al., 1997 ] x h vector from before "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p110#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 110, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022110 long short term memory ( lstm ) [ hochreiter et al., 1997 ] x h vector from before "}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p111#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 111, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 ☉ 111 ct - 1 ht - 1 xt f i g o w ☉ + ct tanh ☉ ht long short term memory ( lstm ) [ ho"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p112#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 112, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 ☉ 112 ct - 1 ht - 1 xt f i g o w ☉ + ct tanh ☉ ht long short term memory ( lstm ) : gr"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p113#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 113, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022113 long short term memory ( lstm ) : gradient flow [ hochreiter et al., 1997 ] c0 c1 c"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p114#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 114, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 do lstms solve the vanishing gradient problem? the lstm architecture makes it easier f"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p115#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 115, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022115 long short term memory ( lstm ) : gradient flow [ hochreiter et al., 1997 ] c0 c1 c"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p116#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 116, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022116 long short term memory ( lstm ) : gradient flow [ hochreiter et al., 1997 ] c0 c1 c"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p117#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 117, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022117 other rnn variants [ lstm : a search space odyssey, greff et al., 2015 ] [ an empir"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p118#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 118, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022 lstm cell 118 neural architecture search for rnn architectures zoph et le, “ neural ar"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p119#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 119, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022119 summary - rnns allow a lot of flexibility in architecture design - vanilla rnns are"}
{"id": "DGM/Slides/RNN_stanford slides.pdf#p120#c1", "source": "DGM/Slides/RNN_stanford slides.pdf", "page": 120, "snippet": "fei - fei li, jiajun wu, ruohan gao lecture 10 - april 28, 2022120 next time : attention and transformers"}
